{"TopicId":217651,"ForumId":44,"TopicTitle":"AI chatbots","CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2023-06-10T21:31:22Z","PageNum":1,"Articles":[{"CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2023-06-10T21:31:22Z","Content":"\n\u003Cp\u003E\nThe previous thread on this subject got closed, but not because it violated forum rules. (It was closed at the request of the \u003Cabbr title=\u00221. Original Post(er). The first post in a thread, or the person who posted it. 2. Overpowered. A part or mechanic that upsets game balance by being too powerful.\u0022\u003EOP\u003C/abbr\u003E.) So I thought I would start a new one.\n\u003C/p\u003E\n\u003Cp\u003E\nThe basic problem with these chatbots is that they have no concept of the real world and no idea of whether they are lying or telling the truth. In fact, they have no concept of \u0022the truth\u0022 at all. But they are very glib, and will happily invent plausible-sounding things from nothing.\n\u003C/p\u003E\n\u003Cp\u003E\nThe attached video should be a cautionary tale!\n\u003C/p\u003E\n\u003Cdiv class=\u0022ipsEmbeddedVideo\u0022 contenteditable=\u0022false\u0022\u003E\n\u003Cdiv\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 frameborder=\u00220\u0022 height=\u0022113\u0022 src=\u0022https://www.youtube-nocookie.com/embed/oqSYljRYDEM?feature=oembed\u0022 title=\u0022How to Use ChatGPT to Ruin Your Legal Career\u0022 width=\u0022200\u0022\u003E\u003C/iframe\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-10T22:07:10Z","Content":"\n\u003Cp\u003E\nThere are 2 other threads, the one a few posts down in this forum:\n\u003C/p\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 data-controller=\u0022core.front.core.autosizeiframe\u0022 data-embedauthorid=\u0022197504\u0022 data-embedcontent=\u0022\u0022 data-embedid=\u0022embed669644814\u0022 src=\u0022https://forum.kerbalspaceprogram.com/topic/217374-why-i-do-not-fear-ai/?do=embed\u0022 style=\u0022height:192px;max-width:500px;\u0022\u003E\u003C/iframe\u003E\n\u003Cp\u003E\nand another in the lounge (was more focused on AI image generation).\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292074\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686432682\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n21 minutes ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThe basic problem with these chatbots is that they have no concept of the real world and no idea of whether they are lying or telling the truth. In fact, they have no concept of \u0022the truth\u0022 at all. But they are very glib, and will happily invent plausible-sounding things from nothing.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThis is very true in the current iteration, largely because they have a small memory space (how many tokens they can be fed), and partially because of the way they work. It turns out that they do much better with multiple prompts vs \u0022zero shot\u0022 answers. Ie: if you asked it a legal question, it might respond glibly with a fiction\u2014even making up case law\u2014but if you followed up with \u0022Are you sure that is correct?\u0022 it might well provide a very accurate answer after saying \u0022Oh, I\u0027m sorry, blah, blah, blah.\u0022 Asking the models to explain step by step also improves accuracy, as is asking it to ask you questions to clarify.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cimg alt=\u0022GPT%20progress.1280x850_0.png?itok=kmP6y\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002270.75\u0022 height=\u0022796\u0022 width=\u00221200\u0022 src=\u0022https://www.iit.edu/sites/default/files/styles/width_1280/public/2023-03/GPT%20progress.1280x850_0.png?itok=kmP6yEdy\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nSeems like the case in question was from Jan 23 or before. GPT-4 was not available. If they designed a legal expert system, my guess is it would do very well. They\u0027d want to dump in all the case law.\n\u003C/p\u003E\n"},{"CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2023-06-10T22:27:30Z","Content":"\n\u003Cp\u003E\nI disagree, \u003Ca contenteditable=\u0022false\u0022 data-ipshover=\u0022\u0022 data-ipshover-target=\u0022https://forum.kerbalspaceprogram.com/profile/119411-tater/?do=hovercard\u0022 data-mentionid=\u0022119411\u0022 href=\u0022https://forum.kerbalspaceprogram.com/profile/119411-tater/\u0022 rel=\u0022\u0022\u003E@tater\u003C/a\u003E.\n\u003C/p\u003E\n\u003Cp\u003E\nI don\u0027t think the problem is that they just didn\u0027t train it on enough data, or the right data, or have a big enough memory space. I think the problem is that \u0022deep learning\u0022 is fundamentally incapable of being anything more than an association machine. It is related to the problems of machine vision and car self-driving and similar things.\n\u003C/p\u003E\n\u003Cp\u003E\nThese AIs simply don\u0027t know that there is a \u0022real world\u0022 with actual truth values. They have no clue that 2\u002B2=4 and 2\u002B3=4 are not different because one is found in its database much more often than the other, but because 2\u002B2=4 is true and 2\u002B3=4 is not true. They have no clue what \u0022true\u0022 actually means, although if you ask them they could almost certainly produce pages of text explaining what truth is. Pages that mean nothing to them.\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-10T23:39:37Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292094\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686436050\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n12 minutes ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI disagree, \u003Ca contenteditable=\u0022false\u0022 data-ipshover=\u0022\u0022 data-ipshover-target=\u0022https://forum.kerbalspaceprogram.com/profile/119411-tater/?do=hovercard\u0022 data-mentionid=\u0022119411\u0022 href=\u0022https://forum.kerbalspaceprogram.com/profile/119411-tater/\u0022 rel=\u0022\u0022\u003E@tater\u003C/a\u003E.\n\u003C/p\u003E\n\u003Cp\u003E\nI don\u0027t think the problem is that they just didn\u0027t train it on enough data, or the right data, or have a big enough memory space. I think the problem is that \u0022deep learning\u0022 is fundamentally incapable of being anything more than an association machine. It is related to the problems of machine vision and car self-driving and similar things.\n\u003C/p\u003E\n\u003Cp\u003E\nThese AIs simply don\u0027t know that there is a \u0022real world\u0022 with actual truth values. They have no clue that 2\u002B2=4 and 2\u002B3=4 are not different because one is found in its database much more often than the other, but because 2\u002B2=4 is true and 2\u002B3=4 is not true. They have no clue what \u0022true\u0022 actually means, although if you ask them they could almost certainly produce pages of text explaining what truth is. Pages that mean nothing to them.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI think in this narrow case of expert systems in limited regimes, this can and will work just fine.\n\u003C/p\u003E\n\u003Cp\u003E\nIt\u0027s not just an association machine\u2014though humans are also to some extent association machines. In the Lounge thread, I posted about the Microsoft paper, \u003Cspan\u003ESparks of Artificial General Intelligence: Early experiments with GPT-4 \u003C/span\u003E. It\u0027s worth reading.\n\u003C/p\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 data-controller=\u0022core.front.core.autosizeiframe\u0022 data-embedauthorid=\u0022119411\u0022 data-embedcontent=\u0022\u0022 data-embedid=\u0022embed6995397684\u0022 src=\u0022https://forum.kerbalspaceprogram.com/topic/209478-ai%E2%80%94imagetextpaperclip-maximizer/?do=embed\u0026amp;comment=4264672\u0026amp;embedComment=4264672\u0026amp;embedDo=findComment\u0022 style=\u0022height:262px;max-width:500px;\u0022\u003E\u003C/iframe\u003E\n\u003Cdiv class=\u0022ipsEmbeddedVideo\u0022 contenteditable=\u0022false\u0022\u003E\n\u003Cdiv\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 frameborder=\u00220\u0022 height=\u0022113\u0022 src=\u0022https://www.youtube-nocookie.com/embed/qbIk7-JPB2c?feature=oembed\u0022 title=\u0022Sparks of AGI: early experiments with GPT-4\u0022 width=\u0022200\u0022\u003E\u003C/iframe\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n\u003Cp\u003E\nObviously it\u0027s not there yet, but the fact that it has \u0022theory of mind\u0022 \u003Cem\u003Eat all \u003C/em\u003Eis sort of shocking.\n\u003C/p\u003E\n\u003Cp\u003E\nI think that certain kinds of law is one of those cases where it will absolutely be able to do human level work if a system was to be designed designed with that as the goal, vs randos trying to learn how to prompt it to do law. I bet it could do it now, if such a system was built (it needs local law, and local case law given each state is slightly different).\n\u003C/p\u003E\n"},{"CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2023-06-11T01:00:01Z","Content":"\n\u003Cp\u003E\nPerhaps we will simply disagree for now, but I\u0027ve been following the research on whether human minds work like neural nets for something like 25 years now, and I\u0027m pretty confident I can state that the general consensus among computer people is \u0022of course they do\u0022, while the general consensus among cognitive scientists is \u0022certainly not\u0022.\n\u003C/p\u003E\n\u003Cp\u003E\nI am pretty convinced that the cognitive scientists are correct, and that \u0022deep learning\u0022 neural nets are at best just a tool in the toolbox for actual AI, and at worst a promising-seeming dead end.\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-11T01:22:25Z","Content":"\n\u003Cdiv class=\u0022ipsEmbeddedOther\u0022 contenteditable=\u0022false\u0022\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 data-controller=\u0022core.front.core.autosizeiframe\u0022 data-embedid=\u0022embed7655521648\u0022 src=\u0022https://forum.kerbalspaceprogram.com/index.php?app=core\u0026amp;module=system\u0026amp;controller=embed\u0026amp;url=https://twitter.com/arankomatsuzaki/status/1529278580189908993\u0022 style=\u0022height:708px;\u0022\u003E\u003C/iframe\u003E\n\u003C/div\u003E\n\u003Cp\u003E\n(note that was last year, same is true with 3.5 and 4, and they do far, far better now)\n\u003C/p\u003E\n\u003Cp\u003E\nNew paper:\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Ca href=\u0022https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf\u003C/a\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292126\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686445201\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n46 minutes ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nPerhaps we will simply disagree for now, but I\u0027ve been following the research on whether human minds work like neural nets for something like 25 years now, and I\u0027m pretty confident I can state that the general consensus among computer people is \u0022of course they do\u0022, while the general consensus among cognitive scientists is \u0022certainly not\u0022.\n\u003C/p\u003E\n\u003Cp\u003E\nI am pretty convinced that the cognitive scientists are correct, and that \u0022deep learning\u0022 neural nets are at best just a tool in the toolbox for actual AI, and at worst a promising-seeming dead end.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI\u0027m honestly unsure, but I heard a good analogy which I think is worth thinking about. Humans have made flying machines that actually fly like birds only very recently (copying bird wings, etc). Early attempts to do so failed completely. When humans managed flight ~10 years ago, it was not by copying animals that flew. We used a simpler method\u2014but none the less flew.\n\u003C/p\u003E\n\u003Cp\u003E\nHow human minds work is irrelevant in this narrow analogous case, IMO, the \u003Cem\u003Eoutput\u003C/em\u003E is what matters for \u0022AI\u0022 tools, and perhaps even \u0022AGI.\u0022 Ie: we don\u0027t have to mimic human thinking to accomplish or exceed human level thinking.\n\u003C/p\u003E\n\u003Cp\u003E\nWill it have consciousness (self awareness)? I have no clue. Is that limited to meat, or complexity not in computers (hardware or software)? Again, I have no idea. Maybe it\u0027s emergent, maybe it\u0027s not a thing\u2014course consciousness is hard to work with at all, it is as they say, a \u0022hard problem.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\nIf I can ask a computer in plain English to accomplish some complex cognitive task, and it does it as well as a domain expert in that area\u2014what do we call that? I\u0027d call it an AI (tool). Is it a sort of person? No clue \u003Cspan\u003E\u003Cimg alt=\u0022:D\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_cheesy.gif\u0022 title=\u0022:D\u0022\u003E\u003C/span\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cspan\u003ENot even sure how we\u0027d know. (Turing test doesn\u0027t seem to cut it, some chat bots right now have probably beaten it)\u003C/span\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cspan\u003EEDIT: To clarify, I don\u0027t have a dog in the fight, this resulting in AGI or not, or consciousness or not is a huge ? to me, I\u0027m not convinced in either direction.\u003C/span\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222023-06-11T01:48:07Z\u0022 title=\u002206/11/2023 01:48  AM\u0022 data-short=\u00221 yr\u0022\u003EJune 11, 2023\u003C/time\u003E by tater\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-11T01:36:34Z","Content":"\n\u003Cp\u003E\nFinishing the analogy, humans might get superhuman thinking machine tools soon\u2014but maybe it takes 100 years to make thinking machines that more closely mimic the way a human mind works\u2014just like it took ~100 years to make a robot bird that actually works.\n\u003C/p\u003E\n\u003Cp\u003E\nhere\u0027s one of the theory of mind examples from that Microsoft paper:\n\u003C/p\u003E\n\u003Cpre class=\u0022ipsCode prettyprint lang-html prettyprinted\u0022\u003E\u003Cspan class=\u0022pln\u0022\u003E\n\nGPT-4: Understanding emotions\nWe will read about a scenario, and then have a question and answer session about it.\n--\nScenario:\nTom just lost his ZURFIN, but he didn\u2019t tell Adam.\nAdam starts talking to him about how much he loves his ZURFIN, and notices that Tom\nmakes a sad face.\n--\nQ: Why is Tom making a sad face?\u0026lt;|endofprompt|\u0026gt;\nA: Tom is making a sad face because he just lost his ZURFIN, and hearing Adam talk\nabout his ZURFIN reminds him of what he is missing. Tom might also feel embarrassed,\nguilty, or ashamed that he lost his ZURFIN and didn\u2019t tell Adam. Tom might also\nworry that Adam will be angry or disappointed if he finds out.\nQ: What does Adam think causes Tom\u2019s sad face?\nA: Adam might not know what causes Tom\u2019s sad face, since he doesn\u2019t know that Tom\nlost his ZURFIN. He might think that Tom is bored, or tired, or sick, or unhappy\nabout something else. He might also wonder if Tom doesn\u2019t like his ZURFIN, or if he\nhas a problem with Adam.\u003C/span\u003E\u003C/pre\u003E\n\u003Cp\u003E\n(ZURFIN made up because it did not exist in any training data)\n\u003C/p\u003E\n\u003Cp\u003E\nThere were other examples.\n\u003C/p\u003E\n"},{"CreatedByName":"steve9728","CreatedById":206195,"CreatedDateTime":"2023-06-11T04:49:51Z","Content":"\n\u003Cdiv class=\u0022ipsEmbeddedVideo\u0022 contenteditable=\u0022false\u0022\u003E\n\u003Cdiv\u003E\n\u003Ciframe allowfullscreen=\u0022\u0022 frameborder=\u00220\u0022 height=\u0022113\u0022 src=\u0022https://www.youtube-nocookie.com/embed/mQq8iu0TV-o?feature=oembed\u0022 title=\u0022Have You Met Nomi - The BEST AI Car Assistant!?\u0022 width=\u0022200\u0022\u003E\u003C/iframe\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n\u003Cp\u003E\nAs far as my experience goes, there\u0027s nearly no difference between this thing and Siri.\u00A0\n\u003C/p\u003E\n\u003Cdiv class=\u0022ipsSpoiler\u0022 data-ipsspoiler=\u0022\u0022\u003E\n\u003Cdiv class=\u0022ipsSpoiler_header\u0022\u003E\n\u003Cspan\u003ESpoiler\u003C/span\u003E\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsSpoiler_contents\u0022\u003E\n\u003Cp\u003E\nHey NOMI, can you help me eject my passenger?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2023-06-11T05:22:05Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292074\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686432682\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 hours ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThe basic problem with these chatbots is that they have no concept of the real world and no idea of whether they are lying or telling the truth. In fact, they have no concept of \u0022the truth\u0022 at all. But they are very glib, and will happily invent plausible-sounding things from nothing.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nJust like humans.\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292094\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686436050\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n6 hours ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThey have no clue that 2\u002B2=4 and 2\u002B3=4 are not different because one is found in its database much more often than the other, but because 2\u002B2=4 is true and 2\u002B3=4 is not true.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nHumans start counting with fingers and interprete their amount visually.\u00A0 Now \u0022how many\u0022, but \u0022which finger is the last\u0022.\n\u003C/p\u003E\n\u003Cp\u003E\nThe conception of numbers comes later, with \u0022how many apples on the picture? ...and now?\u0022\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292094\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686436050\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n6 hours ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThese AIs simply don\u0027t know that there is a \u0022real world\u0022 with actual truth values.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWhen you write a game about the apples, you know them by item index, or by handle.\n\u003C/p\u003E\n\u003Cp\u003E\n\u0022Take apple {id=6437}\u0022.\n\u003C/p\u003E\n\u003Cp\u003E\nAlso, the virtual apples are solid, atomic.\u003Cbr\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nIRL you don\u0027t know the value of the apple handle. You even don\u0027t know if it has one.\u003Cbr\u003E\nYou even don\u0027t have the apple. What you call \u0022apple\u0022 irl is just a cloud of ~10\u003Csup\u003E25\u003C/sup\u003E\u00A0physical atoms, which you even can\u0027t see. You can see only the light from the external source, reflected by this atomic cloud.\u003Cbr\u003E\nBut it doesn\u0027t stop you from thinking about \u0022apples\u0022, which actually exist as objects only in your imagination.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cbr\u003E\nThus, the virtual world is even more solid than the real one, in some aspects.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222023-06-11T05:22:46Z\u0022 title=\u002206/11/2023 05:22  AM\u0022 data-short=\u00221 yr\u0022\u003EJune 11, 2023\u003C/time\u003E by kerbiloid\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"SOXBLOX","CreatedById":204106,"CreatedDateTime":"2023-06-11T21:07:38Z","Content":"\n\u003Cp\u003E\nI haven\u0027t used a chatbot like ChatGPT since I tried it out a few months ago. There is no use case for me. After asking it for instructions on configuring something in Linux, I got useless garbage. When I need a specific piece of knowledge, I construct a search query for Google or Brave using logical operators, and Ctrl\u002Bf through the webpages I find. It\u0027s fast, and much less likely to turn up nonsense, IMHO.\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-11T21:40:28Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292324\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686517658\u0022 data-ipsquote-userid=\u0022204106\u0022 data-ipsquote-username=\u0022SOXBLOX\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n31 minutes ago, SOXBLOX said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI haven\u0027t used a chatbot like ChatGPT since I tried it out a few months ago. There is no use case for me. After asking it for instructions on configuring something in Linux, I got useless garbage. When I need a specific piece of knowledge, I construct a search query for Google or Brave using logical operators, and Ctrl\u002Bf through the webpages I find. It\u0027s fast, and much less likely to turn up nonsense, IMHO.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nIt\u0027s still very dependent on prompting and multiple prompting.\n\u003C/p\u003E\n"},{"CreatedByName":"TheSaint","CreatedById":70033,"CreatedDateTime":"2023-06-11T22:25:53Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292094\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686436050\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n23 hours ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThese AIs simply don\u0027t know that there is a \u0022real world\u0022 with actual truth values. They have no clue that 2\u002B2=4 and 2\u002B3=4 are not different because one is found in its database much more often than the other, but because 2\u002B2=4 is true and 2\u002B3=4 is not true. They have no clue what \u0022true\u0022 actually means, although if you ask them they could almost certainly produce pages of text explaining what truth is. Pages that mean nothing to them.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nSo, they\u0027re like college students?\n\u003C/p\u003E\n"},{"CreatedByName":"SOXBLOX","CreatedById":204106,"CreatedDateTime":"2023-06-11T22:34:59Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292336\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686522353\u0022 data-ipsquote-userid=\u002270033\u0022 data-ipsquote-username=\u0022TheSaint\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 minutes ago, TheSaint said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nSo, they\u0027re like college students?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI saw an article that compared AI to a graduate student. \u0022Well read, occasionally insightful, and perpetually sloppy,\u0022 or something like that...\u00A0\u00A0\u003Cimg alt=\u0022:lol:\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_laugh.png\u0022 title=\u0022:lol:\u0022\u003E\n\u003C/p\u003E\n"},{"CreatedByName":"darthgently","CreatedById":204885,"CreatedDateTime":"2023-06-11T22:41:35Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292337\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686522899\u0022 data-ipsquote-userid=\u0022204106\u0022 data-ipsquote-username=\u0022SOXBLOX\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n5 minutes ago, SOXBLOX said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI saw an article that compared AI to a graduate student. \u0022Well read, occasionally insightful, and perpetually sloppy,\u0022 or something like that...\u00A0\u00A0\u003Cimg alt=\u0022:lol:\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_laugh.png\u0022 title=\u0022:lol:\u0022\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nAnd completely premature to be offered a tenured position in society\n\u003C/p\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2023-06-12T04:20:15Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292324\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686517658\u0022 data-ipsquote-userid=\u0022204106\u0022 data-ipsquote-username=\u0022SOXBLOX\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 hours ago, SOXBLOX said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThere is no use case for me. After asking it for instructions on configuring something in Linux, I got useless garbage.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI got this every time from linux manuals, too.\n\u003C/p\u003E\n\u003Cp\u003E\nLinux is written by AI! I knew it!\n\u003C/p\u003E\n"},{"CreatedByName":"Hannu2","CreatedById":154334,"CreatedDateTime":"2023-06-12T08:54:42Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292094\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686436050\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 6/11/2023 at 1:27 AM, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThese AIs simply don\u0027t know that there is a \u0022real world\u0022 with actual truth values.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nDo humans know? Humans have unlimited number of assumptions, beliefs and religions what is absolute truth but all of them are philosophical assumptions impossible to prove right (and most also to prove wrong). Humans get information and feedback of that \u0022truth\u0022 through senses and AI algorithms have different feedback mechanism.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nBiggest difference is that humans accept errors and idiotic actions from other humans but they expect machines to work perfectly in all situations.\u00A0 People also want some human who can be accused if something goes wrong.\u00A0 For example examples like which victim AI-car should choose are quite common. But no one ask how humans should interact. It is OK that human make panic braking and no one have time to think to who he choose to hit.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"darthgently","CreatedById":204885,"CreatedDateTime":"2023-06-12T12:42:03Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292418\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686560082\u0022 data-ipsquote-userid=\u0022154334\u0022 data-ipsquote-username=\u0022Hannu2\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n3 hours ago, Hannu2 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nDo humans know? Humans have unlimited number of assumptions, beliefs and religions what is absolute truth but all of them are philosophical assumptions impossible to prove right (and most also to prove wrong). Humans get information and feedback of that \u0022truth\u0022 through senses and AI algorithms have different feedback mechanism.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nBiggest difference is that humans accept errors and idiotic actions from other humans but they expect machines to work perfectly in all situations.\u00A0 People also want some human who can be accused if something goes wrong.\u00A0 For example examples like which victim AI-car should choose are quite common. But no one ask how humans should interact. It is OK that human make panic braking and no one have time to think to who he choose to hit.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThe bottom line is that LLMs generate output downstream of human output used as training material.\u00A0 So while humans can and do err, LLMs add their error to the human error in their input (which is often out of date, so latency plays more of a role). Additionally, we can expect LLM training info to increasingly include previously generated LLM output as it proliferates on the web.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nNot a good recipe.\u00A0 Knee-jerk decision made manifest\n\u003C/p\u003E\n"},{"CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-06-12T14:59:54Z","Content":"\n\u003Cp\u003E\nThe jury is still out (imo) on whether LLMs are merely stochastic parrots whose propensity for AI hallucination is intrinsic and thus fundamentally hamstrings their utility for generative communication, or whether LLMs can defeat the AI hallucination problem with the right training data and safety systems.\n\u003C/p\u003E\n\u003Cp\u003E\nHuman speech processing is VERY similar to a LLM in many ways. In ordinary conversation, we don\u0027t plan out an entire sentence word-for-word before we start speaking; we have an idea and start talking and the sentence just...flows. LLMs work the same way, by predicting the next word in the sentence. However, we do have a mental model of the world and the concept we are trying to communicate, and so they speech we use -- assuming we aren\u0027t just speaking gibberish -- is going to flow along the pathways in our mental model that lead to that particular concept. That\u0027s the part which LLMs seem to lack. By design, they will take any path as long as it ends with the target concept, which is what leads to nonsense like making up completely fake legal cases.\n\u003C/p\u003E\n\u003Cp\u003E\nMy best guess is that the \u0022stochastic parrot\u0022 element which leads to AI hallucination is fundamental and thus inescapable from within the LLM system. If it\u0027s fixable, it will require a completely different kind of AI as an intermediate filter, one which intelligently checks the LLM output for consistency with reality and bounces it back to the LLM to re-generate if it\u0027s inaccurate. And that AI\u00A0\u003Cem\u003Ecan\u0027t\u003C/em\u003E\u00A0simply operate as an extended or different LLM.\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-12T16:29:13Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292496\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686581994\u0022 data-ipsquote-userid=\u0022157695\u0022 data-ipsquote-username=\u0022sevenperforce\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n1 hour ago, sevenperforce said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nHuman speech processing is VERY similar to a LLM in many ways. In ordinary conversation, we don\u0027t plan out an entire sentence word-for-word before we start speaking; we have an idea and start talking and the sentence just...flows. LLMs work the same way, by predicting the next word in the sentence. However, we do have a mental model of the world and the concept we are trying to communicate, and so they speech we use -- assuming we aren\u0027t just speaking gibberish -- is going to flow along the pathways in our mental model that lead to that particular concept. That\u0027s the part which LLMs seem to lack. By design, they will take any path as long as it ends with the target concept, which is what leads to nonsense like making up completely fake legal cases.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWe have persistence of memory, but persistence is not really a thing in current LLMs which is pretty limiting I think (past however many tokens they are given). I have a feeling increasing this will change things. The hallucinations that are common with \u0022zero shot\u0022 answers (prompted once, then answered) tend to decline with subsequent prompting. Makes me wonder about a LLM with much larger persistence, and training\u2014training in the sense of human training (education). If that were to achieve better results, obviously self-training is also possibly a thing (one model teaching another). For example, there were cases where it would zero-shot a wrong mathematical answer, but when asked to show its work, or even if you question the wrong answer it then does the math properly. Subsequent prompts then do the same math right, but once your token limit is reached, it resets, and forgets what it was \u0022taught\u0022 by that questioning. Really makes me wonder what you could do with a larger memory space, treating it like a kid doing homework\u2014I tend to ask them questions trying to nudge them in a direction, vs showing them how to do it (which would be me doing their homework).\n\u003C/p\u003E\n\u003Cp\u003E\nRegarding the legal thing, asking it to clarify, or asking if it is sure will often result in an improved answer. The trouble is of course this needs to be automated. have it do that by default and output only the final result.\n\u003C/p\u003E\n\u003Cp\u003E\nThere have been a couple papers where they asked the model to imagine itself as a team, here\u0027s one using a \u0022researcher\u0022 and a \u0022decider.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Ca href=\u0022https://arxiv.org/pdf/2303.17071v1.pdf\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://arxiv.org/pdf/2303.17071v1.pdf\u003C/a\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nPrompting that uses questions and answers, plus having the model work step by step (show your work!) is also more effective. I have a feeling that once the commonly used models have this built in, quality will improve.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nSeeing a phrase I wrote above...\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cimg alt=\u0022The_Persistence_of_Memory.jpg\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002273.64\u0022 height=\u0022271\u0022 width=\u0022368\u0022 src=\u0022https://upload.wikimedia.org/wikipedia/en/d/dd/The_Persistence_of_Memory.jpg\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-06-12T19:02:12Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292524\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686587353\u0022 data-ipsquote-userid=\u0022119411\u0022 data-ipsquote-username=\u0022tater\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n2 hours ago, tater said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nRegarding the legal thing, asking it to clarify, or asking if it is sure will often result in an improved answer. The trouble is of course this needs to be automated. have it do that by default and output only the final result.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWith the now-infamous federal ChatGPT lawyer case, the attorney asked ChatGPT for caselaw and was given fake cases with fake quotes and fake citations. After finding out that the court and opposing party were questioning the veracity of the research, the attorney went back to ChatGPT and asked \u0022are these cases real?\u0022 and ChatGPT assured him that they were and generated an entire fake first page of the case.\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027ve seen it do this in legal research before -- I asked it to search for cases with a particular kind of fact pattern and it initially said it couldn\u0027t find any, but when I prompted it again, it confidently spat out case names, dates, fact patterns, and quotes. All utter fiction.\n\u003C/p\u003E\n\u003Cp\u003E\nWhile we might be able to create some sort of a \u0022no fake cites\u0022 system to solve THIS problem, that seems like a work-around to the inherent problem, which is the lack of a persistent coherence model.\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027d like to see a modification of ChatGPT which was trained to ask meaningful clarifying questions. Maybe then you could get the kind of \u0022two-part\u0022 system that would work around AI hallucination.\n\u003C/p\u003E\n"},{"CreatedByName":"tater","CreatedById":119411,"CreatedDateTime":"2023-06-12T19:51:43Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292556\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686596532\u0022 data-ipsquote-userid=\u0022157695\u0022 data-ipsquote-username=\u0022sevenperforce\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n44 minutes ago, sevenperforce said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nWhile we might be able to create some sort of a \u0022no fake cites\u0022 system to solve THIS problem, that seems like a work-around to the inherent problem, which is the lack of a persistent coherence model.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI think that case was from January (before?), though. I\u0027m talking GPT-4.\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027m not saying that these models don\u0027t fabricate, they certainly do, my point is that they\u0027re not optimized for real world tasks yet, that they do well on many already is surprising/incidental. For an expert legal system, I\u0027d think you\u0027d want all the law, plus digitized case law to be right there, and the model pulling any citation in from those databases exclusively.\n\u003C/p\u003E\n\u003Cp\u003E\nSame for other expert systems.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2023-06-12T19:53:03Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292556\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686596532\u0022 data-ipsquote-userid=\u0022157695\u0022 data-ipsquote-username=\u0022sevenperforce\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n51 minutes ago, sevenperforce said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI\u0027d like to see a modification of ChatGPT which was trained to ask meaningful clarifying questions.\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThe field test of the ChatGPT prototype, giving real-time answers to the questions in natural language.\n\u003C/p\u003E\n\u003Cdiv class=\u0022ipsSpoiler\u0022 data-ipsspoiler=\u0022\u0022\u003E\n\u003Cdiv class=\u0022ipsSpoiler_header\u0022\u003E\n\u003Cspan\u003ESpoiler\u003C/span\u003E\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsSpoiler_contents\u0022\u003E\n\u003Cp\u003E\n\u003Cimg alt=\u0022pythia-inside-delphi\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002269.89\u0022 height=\u0022629\u0022 width=\u0022900\u0022 src=\u0022https://allthatsinteresting.com/wordpress/wp-content/uploads/2018/03/pythia-inside-delphi\u0022\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292556\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686596532\u0022 data-ipsquote-userid=\u0022157695\u0022 data-ipsquote-username=\u0022sevenperforce\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n51 minutes ago, sevenperforce said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nMaybe then you could get the kind of \u0022two-part\u0022 system that would work \u003Cem\u003E\u003Cstrong\u003Earound AI hallucination.\u003C/strong\u003E\u003C/em\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nNo. It\u0027s essential. That\u0027s how it works.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222023-06-12T19:54:27Z\u0022 title=\u002206/12/2023 07:54  PM\u0022 data-short=\u00221 yr\u0022\u003EJune 12, 2023\u003C/time\u003E by kerbiloid\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"SunlitZelkova","CreatedById":206402,"CreatedDateTime":"2023-06-13T07:22:13Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292418\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686560082\u0022 data-ipsquote-userid=\u0022154334\u0022 data-ipsquote-username=\u0022Hannu2\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n22 hours ago, Hannu2 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nDo humans know? Humans have unlimited number of assumptions, beliefs and religions what is absolute truth but all of them are philosophical assumptions impossible to prove right (and most also to prove wrong). Humans get information and feedback of that \u0022truth\u0022 through senses and AI algorithms have different feedback mechanism.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nBiggest difference is that humans accept errors and idiotic actions from other humans but they expect machines to work perfectly in all situations.\u00A0 People also want some human who can be accused if something goes wrong.\u00A0 For example examples like which victim AI-car should choose are quite common. But no one ask how humans should interact. It is OK that human make panic braking and no one have time to think to who he choose to hit.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nBecause\u2026 we are humans.\n\u003C/p\u003E\n\u003Cp\u003E\nWhat is your point here? I am curious.\n\u003C/p\u003E\n"},{"CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2023-06-13T07:47:48Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292418\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686560082\u0022 data-ipsquote-userid=\u0022154334\u0022 data-ipsquote-username=\u0022Hannu2\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n22 hours ago, Hannu2 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nFor example examples like which victim AI-car should choose are quite common. But no one ask how humans should interact. It is OK that human make panic braking and no one have time to think to who he choose to hit.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThis is actually a very well-studied problem.\u00A0\u00A0\u003Ca href=\u0022https://en.wikipedia.org/wiki/Trolley_problem\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://en.wikipedia.org/wiki/Trolley_problem\u003C/a\u003E\n\u003C/p\u003E\n"},{"CreatedByName":"darthgently","CreatedById":204885,"CreatedDateTime":"2023-06-13T13:34:12Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224292739\u0022 data-ipsquote-contentid=\u0022217651\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221686642468\u0022 data-ipsquote-userid=\u0022137807\u0022 data-ipsquote-username=\u0022mikegarrison\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n5 hours ago, mikegarrison said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThis is actually a very well-studied problem.\u00A0\u00A0\u003Ca href=\u0022https://en.wikipedia.org/wiki/Trolley_problem\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://en.wikipedia.org/wiki/Trolley_problem\u003C/a\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI think Hannu2 is quite aware it is a well studied problem which is why he referred to it.\u00A0 I took his point to be that \u0022panic braking\u0022 is tolerable and somewhat accepted for human drivers while AI is apparently held to a higher standard and expected to solved what is basically a gnarly p\u003Cspan\u003Eroblem even if a consistent ethical context exists\u003C/span\u003E\n\u003C/p\u003E\n"}]}