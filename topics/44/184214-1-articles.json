{"TopicId":184214,"ForumId":44,"TopicTitle":"How much physical space would an exabyte of data require?","CreatedByName":"daniel l.","CreatedById":57763,"CreatedDateTime":"2019-05-04T08:26:21Z","PageNum":1,"Articles":[{"CreatedByName":"daniel l.","CreatedById":57763,"CreatedDateTime":"2019-05-04T08:26:21Z","Content":"\n\u003Cp\u003E\nI\u0027ve recently been thinking about how humans would survive on long-term space voyages. And while it\u0027s definitely a fact that we can survive even without books, which I consider the bare minimum requirement for a human being to not be completely bored, it would still be best for travelers to have as much content at their disposal as possible. The limitations of the speed of light, of course, make easy access to the existing internet impossible once out of Earth\u0027s general area by a half light-minute or so, so it would be necessary to bring along a diverse and near-inexhaustible amount of information for purposes of entertainment and education.\n\u003C/p\u003E\n\u003Cp\u003E\nSo what if a spaceship were to have, stored in its data archives, every movie ever made and every episode of every series ever made (excluding ones that are lost or illegal, of course) in the highest possible resolutions? Every work of written literature, and the sum total of all human scientific knowledge as well as, thrown in for good measure, a few photorealistic virtual-reality games and environments for the crew to enjoy using either a conventional VR headset or some form of real life holodeck.\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027d imagine that such a massive data archive would require hundreds of petabytes at least, so I\u0027m thinking that an Exabyte of capacity would be enough for it all, or at least something close to that estimation.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nSo my question is: how much physical space would such an archive require? How compact could such a thing be made with present day tech, or with future tech based on a logical prediction. I\u0027d hope that, with effort, it could be made small enough to fit about a craft like SpaceX\u0027s Starship, or at least a larger successor craft.\n\u003C/p\u003E\n"},{"CreatedByName":"Flibble","CreatedById":196830,"CreatedDateTime":"2019-05-04T08:36:00Z","Content":"\n\u003Cp\u003E\nA few cubic metres.\n\u003C/p\u003E\n\u003Cp\u003E\nA large flash drive is 4TB, and has a volume of approximately 1.2 x 10^-5 m^3. 262,000 of them is a petabyte, which is just over 3 m^3.\n\u003C/p\u003E\n"},{"CreatedByName":"Elthy","CreatedById":63317,"CreatedDateTime":"2019-05-04T09:16:11Z","Content":"\n\u003Cp\u003E\nEven less if you use MicroSD cards. Currently the biggest ones allow 512GB, while only being 11*15*0,7mm in size (=115,5mm^3). You need 2 milllion of them, which results in 0,231m^3.\n\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-05-04T20:28:35Z","Content":"\n\u003Cp\u003E\ni wouldnt want to build an exobyte storage cluster on microsd for obvious reasons.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nm.2 sticks are up to about 2tb. and i know ive seen quad m.2 to pcie-16 adapter cards. so thats 8 tb per board. i found server boards with 7 16x slots to give you 56 tb per server. of course the server eats all the space you save with the m.2 and on top of that\u00A0for large nas clusters you probably want mechanical drives, ssds just aren\u0027t cut out for server loads. 14 tb drives are available. you could get a 48 bay nas server for about $20k\u00A0 that gets you up to 672tb. about 2000 of those and there\u0027s your exabyte. congratulations, you have invented the datacenter.\u00A0 you might be able to cram 3 of those per rack so you would need 666 racks because math worships\u00A0satan. thats not counting routers and switches and everything else that setup would need. so it would be about the size of a fairly large room if you want practical storage. if speed is not an issue you could go with a tape system with a robotic tape recovery. i think 10 tb tapes are the norm these days\u00A0and you would only need 100000 of them.\u00A0\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-05-04T20:39:06Z\u0022 title=\u002205/04/2019 08:39  PM\u0022 data-short=\u00225 yr\u0022\u003EMay 4, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Gargamel","CreatedById":64714,"CreatedDateTime":"2019-05-05T05:26:35Z","Content":"\n\u003Cp\u003E\nBy the time we need that much storage space for such a voyage, it will have become \u0022trivial\u0022 to create such a storage space.\u00A0\u00A0\u00A0 The real issue will be digitizing everything and getting it there....\n\u003C/p\u003E\n"},{"CreatedByName":"Cunjo Carl","CreatedById":162109,"CreatedDateTime":"2019-05-06T01:09:58Z","Content":"\n\u003Cp\u003E\nSamsung\u0027s modern flash memory (V-NAND) is somewhere in the 2-4 Terabits per square inch ball park (Oh no, imperial units, run away!), and will probably be made on thinned wafers stacked to 200-300um thick, so if we could do the packaging to stack them on the die level we\u0027d be looking at about 300mm^3/TB (byte). For a real device we could probably happily triple that taking cooling, datalines and such into account. Let\u0027s call it 1cm^3/TB. So if sufficiently motivated, I\u0027m getting about the same values as everyone else that we should be able to cram the whole Exabyte into about a 1m cube. It would cost a bloody fortune both for development and manufacturing, and the datarates out would not be particularly impressive, but it could be done!\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nIf we want to go near future though, single atom storage is starting to peak over the horizon! \u003Ca href=\u0022https://arstechnica.com/science/2016/07/researchers-hit-record-storage-density-by-writing-bits-with-single-atoms/?comments=1\u0026amp;amp;post=31577207\u0022 rel=\u0022external nofollow\u0022\u003EArs Technica article\u003C/a\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nEdit: Got a better source for the stack thickness: \u003Ca href=\u0022https://thememoryguy.com/flash-memory-summit-limitless-layers-of-3d-nand/\u0022 rel=\u0022external nofollow\u0022\u003Ehere\u003C/a\u003E.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-05-06T01:45:31Z\u0022 title=\u002205/06/2019 01:45  AM\u0022 data-short=\u00225 yr\u0022\u003EMay 6, 2019\u003C/time\u003E by Cunjo Carl\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"satnet","CreatedById":147366,"CreatedDateTime":"2019-05-06T04:31:59Z","Content":"\n\u003Cp\u003E\nThis might be a good application for \u003Ca href=\u0022https://en.wikipedia.org/wiki/Holographic_data_storage\u0022 rel=\u0022external nofollow\u0022\u003Eholographic storage\u003C/a\u003E assuming it could be developed beyond proof of concept. For this application we don\u0027t need read-write capability, so a dense write-one-read-many approach would work. Theoretically you can get 1 bit per cube of the laser\u0027s wavelength, which means ~30 TB/cm\u003Csup\u003E3\u003C/sup\u003E with a fluorine excimer laser (157 nm). If we use a blu-ray laser (320 nm) it would have a theoretical maximum of ~3.5 TB/cm\u003Csup\u003E3\u003C/sup\u003E. You\u0027re looking at somewhere between 0.035 m\u003Csup\u003E3\u003C/sup\u003E and 0.3 m\u003Csup\u003E3\u003C/sup\u003E for an exabyte assuming we can get close to the theoretical limit (I would assume something close to 0.1 m\u003Csup\u003E3\u003C/sup\u003E accounting for error-correction, redundancy, etc.). Obviously this wouldn\u0027t beat single atom storage for density, but it could probably handle the occasional cosmic ray.\n\u003C/p\u003E\n\u003Cp\u003E\nYou probably would also want an archive of knowledge and teaching materials, particularly for skills that would be hard to maintain on a generation ship (assuming that is what we\u0027re talking about). For example geology would be a little hard to maintain on a ship.\n\u003C/p\u003E\n"},{"CreatedByName":"Mad Rocket Scientist","CreatedById":139256,"CreatedDateTime":"2019-05-06T05:14:58Z","Content":"\n\u003Cp\u003E\nMagnetic tape is probably a better solution in terms of cost. The latest advances are around 25 GB/in^2.\u00A0\u003Ca href=\u0022https://newatlas.com/sony-ibm-magnetic-tape-density-record/50743/\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://newatlas.com/sony-ibm-magnetic-tape-density-record/50743/\u003C/a\u003E\u00A0Really not a ton more space, maybe 20m^3, but also\u00A0cheaper and\u00A0more resistant to radiation.\n\u003C/p\u003E\n"},{"CreatedByName":"wumpus","CreatedById":133850,"CreatedDateTime":"2019-05-06T13:00:01Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223594021\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557119698\u0022 data-ipsquote-userid=\u0022139256\u0022 data-ipsquote-username=\u0022Mad Rocket Scientist\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 hours ago, Mad Rocket Scientist said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nMagnetic tape is probably a better solution in terms of cost. The latest advances are around 25 GB/in^2.\u00A0\u003Ca href=\u0022https://newatlas.com/sony-ibm-magnetic-tape-density-record/50743/\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://newatlas.com/sony-ibm-magnetic-tape-density-record/50743/\u003C/a\u003E\u00A0Really not a ton more space, maybe 20m^3, but also\u00A0cheaper and\u00A0more resistant to radiation.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nJudging by what the uncool kids over at \u003Ca href=\u0022https://www.reddit.com/r/DataHoarder/\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://www.reddit.com/r/DataHoarder/\u003C/a\u003E have to say, (on a good sale*) retail hard drives make it hard to justify tape.\u00A0 Tape might still be surviving in \u0022enterprise grade disk\u0022 vs. \u0022enterprise grade tape\u0022 (*all* tape these days are for servers.\u00A0 Hard drives appear to be heading for either for servers or the cheapest computers, and I expect the cheapest computers will simply switch to even less flash), but it is getting close.\u00A0 Flash vs. hard drive seems to have a factor of 5 difference in cost, but that is coming down quickly.\u00A0 I wouldn\u0027t be surprised if the first manned flight to Mars wouldn\u0027t have this question: flash will have won outright (or be replaced with something more high tech).\n\u003C/p\u003E\n\u003Cp\u003E\nApparently Google is already storing exabytes of data (I expect their answer would be \u0022x% of a datacenter\u0022 if they were talking.\u00A0 They aren\u0027t.)\n\u003C/p\u003E\n\u003Cp\u003E\n* this involves waiting for a holiday sale at Best Buy and buying the 10TB WD external drives and removing [\u0022shucking\u0022] the hard drive (USA only).\u00A0 Gets to be around $16/TB.\u00A0 LTO-6 tape is on Amazon for less than $10/TB, but good luck finding a cheap price on a LTO-6 drive.\n\u003C/p\u003E\n"},{"CreatedByName":"Lukaszenko","CreatedById":95776,"CreatedDateTime":"2019-05-09T07:59:47Z","Content":"\n\u003Cp\u003E\n\u0022DNA Fountain\u0022 method can store 215 petabytes of data in a single gram of DNA. So, looking at the density of DNA, you could easily fit an exabyte into \u0026lt; 3 cm^3.\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"Codraroll","CreatedById":163815,"CreatedDateTime":"2019-05-09T08:38:38Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223593154\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221556961371\u0022 data-ipsquote-userid=\u002263317\u0022 data-ipsquote-username=\u0022Elthy\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/4/2019 at 11:16 AM, Elthy said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nEven less if you use MicroSD cards. Currently the biggest ones allow 512GB, while only being 11*15*0,7mm in size (=115,5mm^3). You need 2 milllion of them, which results in 0,231m^3.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI can imagine it now, somewhere in the spacecraft there\u0027s a bathtub-sized chest filled to the brim with micro SD cards, each carefully labelled with a small letter code, maybe colour coded too. Strapped to its side, a phone book-sized index detailing what is found on each card. Finding the right card for what you want to watch would be like finding a particular piece of LEGO in a chock-full\u00A0ball pit.\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"Flibble","CreatedById":196830,"CreatedDateTime":"2019-05-09T09:02:49Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596101\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557391118\u0022 data-ipsquote-userid=\u0022163815\u0022 data-ipsquote-username=\u0022Codraroll\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n23 minutes ago, Codraroll said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI can imagine it now, somewhere in the spacecraft there\u0027s a bathtub-sized chest filled to the brim with micro SD cards, each carefully labelled with a small letter code, maybe colour coded too. Strapped to its side, a phone book-sized index detailing what is found on each card. Finding the right card for what you want to watch would be like finding a particular piece of LEGO in a chock-full\u00A0ball pit.\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWell obviously you need a large robotic arm mechanism to automatically select the correct micro SD car for you. \u003Cimg alt=\u0022;)\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_wink.gif\u0022 title=\u0022;)\u0022\u003E\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-05-09T10:11:24Z","Content":"\n\u003Cp\u003E\nsd cards come\u00A0with all the wear leveling machinery taken out and is essentially dumb flash that fails when it fails. its completely unmanaged unlike say an ssd. im not to thrilled about flash\u0027s data retention rates either. or rather the lack of hard data about data retention rates.\u00A0im not sure if we have been using ssd\u0027s long enough to characterize their failure rates accurately when used for archival storage.\u00A0\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-05-09T10:12:09Z\u0022 title=\u002205/09/2019 10:12  AM\u0022 data-short=\u00225 yr\u0022\u003EMay 9, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Listy","CreatedById":168860,"CreatedDateTime":"2019-05-09T15:28:20Z","Content":"\n\u003Cp\u003E\nThis is a question that has a lot of relevance right now - for example when the Square Kilometer Array telescope in Australia \u0026amp; South Africa starts operating it will generate \u0026amp; store about an exabyte of processed data per day. There are more than 250,000 radio telescopes in the array \u0026amp; each one will generate 160 Gbits of raw, unprocessed data per second - which I think works out to be about 400 exabytes of raw data per day in total before processing.\n\u003C/p\u003E\n\u003Cp\u003E\nFor data storage density nature currently has us well beaten - 1 gram of perfectly encoded DNA could theoretically store 455 exabytes of data, if you could keep it in a state that was both stable \u0026amp; somehow readable.\n\u003C/p\u003E\n"},{"CreatedByName":"DAL59","CreatedById":180784,"CreatedDateTime":"2019-05-09T15:55:21Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596094\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557388787\u0022 data-ipsquote-userid=\u002295776\u0022 data-ipsquote-username=\u0022Lukaszenko\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 hours ago, Lukaszenko said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\n\u0022DNA Fountain\u0022 method can store 215 petabytes of data in a single gram of DNA. So, looking at the density of DNA, you could easily fit an exabyte into \u0026lt; 3 cm^3.\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\ndna is not optimal for space applications though\n\u003C/p\u003E\n"},{"CreatedByName":"wumpus","CreatedById":133850,"CreatedDateTime":"2019-05-09T23:25:32Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596217\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557415700\u0022 data-ipsquote-userid=\u0022168860\u0022 data-ipsquote-username=\u0022Listy\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n7 hours ago, Listy said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThis is a question that has a lot of relevance right now - for example when the Square Kilometer Array telescope in Australia \u0026amp; South Africa starts operating it will generate \u0026amp; store about an exabyte of processed data per day. There are more than 250,000 radio telescopes in the array \u0026amp; each one will generate 160 Gbits of raw, unprocessed data per second - which I think works out to be about 400 exabytes of raw data per day in total before processing.\n\u003C/p\u003E\n\u003Cp\u003E\nFor data storage density nature currently has us well beaten - 1 gram of perfectly encoded DNA could theoretically store 455 exabytes of data, if you could keep it in a state that was both stable \u0026amp; somehow readable.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI really doubt that anyone will be interested in the raw data (except those interested in developing such systems, but I suspect they will have to go to the site for such data), the point is to process the array to mimic a single radio telescope.\u00A0 This cuts down the data needed by a factor of a quarter of a million, and the \u0022denoising\u0022 effects by merely averaging those individual telescopes would be significant (and I expect significantly more sophisticated telescopic tricks plus more well known DSP filtering as well).\n\u003C/p\u003E\n\u003Cp\u003E\nBut that still means that you are getting upwards of 160 Gb/s of highly useful data.\u00A0 Better start hoarding LTO tape.\n\u003C/p\u003E\n\u003Cp\u003E\nModern storage devices use a great deal of their bits for error correcting (and detection).\u00A0 CDs and DVDs have 1 bit of ECC for each data bit.\u00A0 I\u0027ve heard that hard drives wouldn\u0027t work at all without nearly all their error correction bits (but don\u0027t expect to find out how many are needed/on the drive).\u00A0 So I\u0027d expect that with enough ECC (and hints of where the data goes.\u00A0 Think of the ancient trick of numbering your Hollerith deck) you could get DNA storage to fit: just that I doubt that anyone expects a CRISPR unit to be cost effective storing data.\n\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-05-10T00:32:21Z","Content":"\n\u003Cp\u003E\nwhat if we take optical media up the spectrum?\u00A0using hf uv and xray.\u00A0 could we get more information density than what we currently have available with hard drives and better?\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-05-10T00:33:46Z\u0022 title=\u002205/10/2019 12:33  AM\u0022 data-short=\u00225 yr\u0022\u003EMay 10, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Lukaszenko","CreatedById":95776,"CreatedDateTime":"2019-05-10T13:59:40Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596231\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557417321\u0022 data-ipsquote-userid=\u0022180784\u0022 data-ipsquote-username=\u0022DAL59\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n22 hours ago, DAL59 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\ndna is not optimal for space applications though\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWhy not?\n\u003C/p\u003E\n\u003Cdiv\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nQuote\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\n\u003Cspan style=\u0022background-color:#ffffff;color:#353c41;font-size:14px;\u0022\u003EFor data storage density nature currently has us well beaten - 1 gram of perfectly encoded DNA could theoretically store 455 exabytes of data, if you could keep it in a state that was both stable \u0026amp; somehow readable.\u003C/span\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nAll sources I checked say that \u0022\u003Cspan style=\u0022background-color:#ffffff;color:#353c41;font-size:14px;\u0022\u003E215 petabytes of data in a single gram of DNA\u0022 is 85% of the theoretical limit. Where did you find 455 exabytes \u003Cspan\u003E\u003Cimg alt=\u0022:o\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/ZNGIcon2.png\u0022 title=\u0022:o\u0022\u003E\u003C/span\u003E?\u003C/span\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-05-10T14:11:25Z\u0022 title=\u002205/10/2019 02:11  PM\u0022 data-short=\u00225 yr\u0022\u003EMay 10, 2019\u003C/time\u003E by Lukaszenko\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"DAL59","CreatedById":180784,"CreatedDateTime":"2019-05-10T14:47:26Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596683\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557496780\u0022 data-ipsquote-userid=\u002295776\u0022 data-ipsquote-username=\u0022Lukaszenko\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n47 minutes ago, Lukaszenko said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nWhy not?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nradiation\n\u003C/p\u003E\n"},{"CreatedByName":"magnemoe","CreatedById":57801,"CreatedDateTime":"2019-05-10T19:38:40Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596453\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557444332\u0022 data-ipsquote-userid=\u0022133850\u0022 data-ipsquote-username=\u0022wumpus\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n19 hours ago, wumpus said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI really doubt that anyone will be interested in the raw data (except those interested in developing such systems, but I suspect they will have to go to the site for such data), the point is to process the array to mimic a single radio telescope.\u00A0 This cuts down the data needed by a factor of a quarter of a million, and the \u0022denoising\u0022 effects by merely averaging those individual telescopes would be significant (and I expect significantly more sophisticated telescopic tricks plus more well known DSP filtering as well).\n\u003C/p\u003E\n\u003Cp\u003E\nBut that still means that you are getting upwards of 160 Gb/s of highly useful data.\u00A0 Better start hoarding LTO tape.\n\u003C/p\u003E\n\u003Cp\u003E\nModern storage devices use a great deal of their bits for error correcting (and detection).\u00A0 CDs and DVDs have 1 bit of ECC for each data bit.\u00A0 I\u0027ve heard that hard drives wouldn\u0027t work at all without nearly all their error correction bits (but don\u0027t expect to find out how many are needed/on the drive).\u00A0 So I\u0027d expect that with enough ECC (and hints of where the data goes.\u00A0 Think of the ancient trick of numbering your Hollerith deck) you could get DNA storage to fit: just that I doubt that anyone expects a CRISPR unit to be cost effective storing data.\u003Cspan\u003E\uFEFF\u003C/span\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nVery often old data is very valuable, in this setting cost is way more important than space.\u00A0\u003Cbr\u003E\nIn this setting you would use tape.\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nIn space probe you go or die as the Israel lander . Manual: t-30s, Jeb: \u0022major computer fail, going in manual using only mechjeb while assuming it to be bugged, Val, you are abort commander, on my call you are in command until back in orbit. \u0022\n\u003C/p\u003E\n"},{"CreatedByName":"NiL","CreatedById":190772,"CreatedDateTime":"2019-05-11T17:24:50Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223593142\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221556958381\u0022 data-ipsquote-userid=\u002257763\u0022 data-ipsquote-username=\u0022daniel l.\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/4/2019 at 11:26 AM, daniel l. said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI\u0027ve recently been thinking about how humans would survive on long-term space voyages. And while it\u0027s definitely a fact that we can survive even without books, which I consider the bare minimum requirement for a human being to not be completely bored, it would still be best for travelers to have as much content at their disposal as possible. The limitations of the speed of light, of course, make easy access to the existing internet impossible once out of Earth\u0027s general area by a half light-minute or so, so it would be necessary to bring along a diverse and near-inexhaustible amount of information for purposes of entertainment and education.\n\u003C/p\u003E\n\u003Cp\u003E\nSo what if a spaceship were to have, stored in its data archives, every movie ever made and every episode of every series ever made (excluding ones that are lost or illegal, of course) in the highest possible resolutions? Every work of written literature, and the sum total of all human scientific knowledge as well as, thrown in for good measure, a few photorealistic virtual-reality games and environments for the crew to enjoy using either a conventional VR headset or some form of real life holodeck.\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027d imagine that such a massive data archive would require hundreds of petabytes at least, so I\u0027m thinking that an Exabyte of capacity would be enough for it all, or at least something close to that estimation.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nSo my question is: how much physical space would such an archive require? How compact could such a thing be made with present day tech, or with future tech based on a logical prediction. I\u0027d hope that, with effort, it could be made small enough to fit about a craft like SpaceX\u0027s Starship, or at least a larger successor craft.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWell, wikipedia says that according to 2007 study there was a 281exabytes of digital information on Earth, soo i think even if 90% of it is just copies you still need way more than 1 exabyte to store it, huh. However,\u00A0Google proceses ~20-30 petabytes per day, AT\u0026amp;AT handles a 20 petabytes per week stream of data, archive.org holds about 50 petabytes, so i think 1-2 exabytes\u0027ll be enough to entertain crew during a year of flight or so, but if you want something like a copy of the whole Internet for lets say an interstellar decade(s)-long trip, portable source of scientific knowlege or so, 50-100 exabytes may be requred (althrough\u00A09-month Mars trip Google cashe may still fit on Starship \u003Cimg alt=\u0022;)\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_wink.gif\u0022 title=\u0022;)\u0022\u003E ). And also the amount of digital data on Earth is growing logariphmicaly fast, so you\u0027d beter hurry - in 5 years you\u0027ll need something about a couple of zettabytes, espicially for \u0022scientific data\u0022\u2122 and VR\u00A0games with 16k textures, lol)\n\u003C/p\u003E\n"},{"CreatedByName":"NiL","CreatedById":190772,"CreatedDateTime":"2019-05-11T17:35:29Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223597344\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557595490\u0022 data-ipsquote-userid=\u0022190772\u0022 data-ipsquote-username=\u0022NiL\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n3 minutes ago, NiL said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nWell, wikipedia says that according to 2007 study there was a 281exabytes of digital information on Earth, soo i think even if 90% of it is just copies you still need way more than 1 exabyte to store it, huh. However,\u00A0Google proceses ~20-30 petabytes per day, AT\u0026amp;AT handles a 20 petabytes per week stream of data, archive.org holds about 50 petabytes, so i think 1-2 exabytes\u0027ll be enough to entertain crew during a year of flight or so, but if you want something like a copy of the whole Internet for lets say an interstellar decade(s)-long trip, portable source of scientific knowlege or so, 50-100 exabytes may be requred (althrough\u00A09-month Mars trip Google cashe may still fit on Starship \u003Cimg alt=\u0022;)\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_wink.gif\u0022 title=\u0022;)\u0022\u003E ). And also the amount of digital data on Earth is growing logariphmicaly fast, so you\u0027d beter hurry - in 5 years you\u0027ll need something about a couple of zettabytes, espicially for \u0022scientific data\u0022\u2122 and VR\u00A0games with 16k textures, lol)\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWait, here [\u00A0\u003Ca href=\u0022https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf\u003C/a\u003E\u00A0] they saying that total amound of digital data on Earth alreay was\u00A033 zettabytes in 2018 (!!!) and predicted to be 175 zB in 2025.\u00A0Oh shi... I think\u00A0an extra SLS block II launch with 200t of\u00A0DNA-drives is needed.\n\u003C/p\u003E\n"},{"CreatedByName":"Lukaszenko","CreatedById":95776,"CreatedDateTime":"2019-05-13T11:52:25Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596709\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557499646\u0022 data-ipsquote-userid=\u0022180784\u0022 data-ipsquote-username=\u0022DAL59\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/10/2019 at 4:47 PM, DAL59 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nradiation\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nIt did cross my mind, but it can\u0027t be that hard to shield 3 cm^3. Besides, since the first sentence of this\u00A0discussion is\u00A0\u0022\u003Cspan style=\u0022background-color:#ffffff;color:#353c41;font-size:14px;\u0022\u003Eabout how humans would survive on long-term space voyage\u0022,\u00A0we\u0027ll be using dna storage in one way or another whether we like it or not.\u003C/span\u003E\n\u003C/p\u003E\n"},{"CreatedByName":"Listy","CreatedById":168860,"CreatedDateTime":"2019-05-14T02:06:03Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596683\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557496780\u0022 data-ipsquote-userid=\u002295776\u0022 data-ipsquote-username=\u0022Lukaszenko\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/10/2019 at 11:29 PM, Lukaszenko said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nWhy not?\n\u003C/p\u003E\n\u003Cdiv\u003E\n\u003Cp\u003E\nAll sources I checked say that \u0022\u003Cspan style=\u0022background-color:#ffffff;color:#353c41;font-size:14px;\u0022\u003E215 petabytes of data in a single gram of DNA\u0022 is 85% of the theoretical limit. Where did you find 455 exabytes \u003Cspan\u003E\u003Cimg alt=\u0022:o\u0022 data-emoticon=\u0022\u0022 src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/ZNGIcon2.png\u0022 title=\u0022:o\u0022\u003E\u003C/span\u003E?\u003C/span\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThe claim seems to date\u00A0to a paper published in Science in 2012:\u00A0\u003Ca href=\u0022https://science.sciencemag.org/content/337/6102/1628\u0022 rel=\u0022external nofollow\u0022\u003ENext-Generation Digital Information Storage in DNA\u003C/a\u003E\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nFrom the methods \u003Cspan\u003E:\u00A0\u003C/span\u003E\u0022Theoretical DNA density was calculated by using 2 bits per nucleotide of single stranded DNA. The molecular weight of DNA we used was based on an average of 330.95 g/mol/nucleotide of anhydrous weight for the sodium salt of an ATGC balanced library. This results in a weight density of 1 bit per 2.75 x 10\u003Csup\u003E-22\u003C/sup\u003E g, and thus 4.5 x 10\u003Csup\u003E-20\u003C/sup\u003E bytes per gram. Of course, practical maximums would be several orders of magnitude less dense depending the types of redundancy, barcoding, and encoding schemes desired.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\nThe 215 petabytes / gram figure is certainly\u00A0much closer to the\u00A0realistic \u0027practical maximum\u0027 the authors mention \u0026amp; it\u0027s still a huge number!\n\u003C/p\u003E\n\u003Cp\u003E\nDNA can be encapsulated in silica\u00A0for long term error free storage (1 week at \u002B70 degrees C \u0026amp; estimated to be\u00A0~2000 years at \u002B10C, or up to 1 million years at -20C). 1 million years is about the age of the oldest fossil remains where DNA fragments have\u00A0been successfully extracted \u0026amp; sequenced as well, so it would seem that on Earth at least for very long term storage\u00A0controlling temperature \u0026amp; the chemistry of the surrounding environment is more important than worrying about radiation damage. In deep space maintaining your DNA data bank at an ultra cold temperature might be\u00A0fairly easy \u0026amp; radiation might become a much bigger concern.\n\u003C/p\u003E\n"},{"CreatedByName":"wumpus","CreatedById":133850,"CreatedDateTime":"2019-05-14T14:11:08Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596474\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557448341\u0022 data-ipsquote-userid=\u002210883\u0022 data-ipsquote-username=\u0022Nuke\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/9/2019 at 8:32 PM, Nuke said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nwhat if we take optical media up the spectrum?\u00A0using hf uv and xray.\u00A0 could we get more information density than what we currently have available with hard drives and better?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nMagnetic media has resolved tighter than optical (blue lasers at least) can resolve.\u00A0 I doubt you will be focusing UV very well (the semi conductor industry has been dragging their legs going from normal optical to uv for years.\u00A0 Expect any company that can\u0027t make the transition in the next year or so to drop out of the race for producing \u0022bleeding edge chips\u0022*) and I\u0027ve never heard of an x-ray lens being big enough to be functional (perhaps mirrors will work).\u00A0 In any even, modern NAND flash approaches optical resolution limits (for cost reasons, modern CPUs push into \u0022really weird optical tricks\u0022) and then stack the transistors 48-96 layers deep.\u00A0 You aren\u0027t getting a tape to do that, unless you go 3D (holographic tape might work, but still won\u0027t touch the density.\u00A0 The holograms would effectively act as a ECC/frequency expanding trick).\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223596709\u0022 data-ipsquote-contentid=\u0022184214\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221557499646\u0022 data-ipsquote-userid=\u0022180784\u0022 data-ipsquote-username=\u0022DAL59\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 5/10/2019 at 10:47 AM, DAL59 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nradiation\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThe entire problem assumed humans would be on board.\u00A0 And remember when I mentioned that optical media used half the bits to correct the other data bits?\u00A0 You would have to do that with DNA as well, but obviously not as much.\u00A0 In other news I\u0027d like my own DNA re-encoded with these tricks so it can \u0022scrub\u0022 out any transcription errors and mutations (but not with today\u0027s technology.\u00A0 It will have to wait).\n\u003C/p\u003E\n"}]}