{"TopicId":117359,"ForumId":44,"TopicTitle":"Von Neumann probes","CreatedByName":"Souper","CreatedById":86003,"CreatedDateTime":"2015-08-01T00:20:45Z","PageNum":5,"Articles":[{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T10:06:24Z","Content":"\u003E \n\u003E It\u0027s not my theory, it\u0027s just mathematics. If you\u0027re not familiar with algorithmic information theory, I suggest you to read a book or two about it. Algorithmic information theory is generally offered as a graduate-level computer science/mathematics class, so the web pages of your favorite university should have book suggestions.\n\nAre you denying the basic premise that your interpretation of the theory is wrong? Since reality disagrees with your interpretation, I see no other possibility. If your theory were a law of physics, we would not be having this discussion. It\u0027s just a theory, and it\u0027s incorrect. The math may be self consistent, but it is not what the universe is using.\n\nWe are talking about Von Neumann machines - in what way is the requirement to send additional information over a laser link a change that makes the machine no longer count as a self replicating machine that can expand until the galaxy\u0027s solid matter is consumed? What exact detail makes it different? It just means the host machine is part of the system.\n\nHow does a machine that is sentient fit into your explanation? If the machine is capable of deriving goals for itself, evaluating possible designs, conducting experiments, and then building a better version of yourself...I think you\u0027re saying that such a device is impossible?\n\n**Edited \u003Ctime datetime=\u00222015-08-07T10:10:23Z\u0022 title=\u002208/07/2015 10:10  AM\u0022 data-short=\u00228 yr\u0022\u003EAugust 7, 2015\u003C/time\u003E by SomeGuy12**"},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-07T10:26:47Z","Content":"\u003E \n\u003E Are you denying the basic premise that your interpretation of the theory is wrong? Since reality disagrees with your interpretation, I see no other possibility. If your theory were a law of physics, we would not be having this discussion. It\u0027s just a theory, and it\u0027s incorrect. The math may be self consistent, but it is not what the universe is using.\n\nIn which way reality disagrees with my interpretation? Provide a detailed example, where reality creates useful information from nothing in a deterministic way.\n\nBesides, do you even know computability theory and algorithmic information? Or what \u0027theory\u0027 means in a mathematical context?\n\n\u003E \n\u003E We are talking about Von Neumann machines - in what way is the requirement to send additional information over a laser link a change that makes the machine no longer count as a self replicating machine that can expand until the galaxy\u0027s solid matter is consumed? What exact detail makes it different? It just means the host machine is part of the system.\n\nWe\u0027re talking about von Neumann machines, which are autonomous self-replicating machines. If the machine has an external source of information, the replication must also replicate that source.\n\n\u003E \n\u003E How does a machine that is sentient fit into your explanation? If the machine is capable of deriving goals for itself, evaluating possible designs, conducting experiments, and then building a better version of yourself...I think you\u0027re saying that such a device is impossible?\n\nSentience is completely irrelevant to this discussion, unless you\u0027re assuming some kind of soul, which is not bounded by logic and the laws of physics. If sentience arises from a physical object, a sentient machine is just like any other machine from an information-theoretic point of view."},{"CreatedByName":"magnemoe","CreatedById":57801,"CreatedDateTime":"2015-08-07T10:55:55Z","Content":"\u003E \n\u003E Are you denying the basic premise that your interpretation of the theory is wrong? Since reality disagrees with your interpretation, I see no other possibility. If your theory were a law of physics, we would not be having this discussion. It\u0027s just a theory, and it\u0027s incorrect. The math may be self consistent, but it is not what the universe is using.\n\u003E We are talking about Von Neumann machines - in what way is the requirement to send additional information over a laser link a change that makes the machine no longer count as a self replicating machine that can expand until the galaxy\u0027s solid matter is consumed? What exact detail makes it different? It just means the host machine is part of the system.\n\u003E \n\u003E How does a machine that is sentient fit into your explanation? If the machine is capable of deriving goals for itself, evaluating possible designs, conducting experiments, and then building a better version of yourself...I think you\u0027re saying that such a device is impossible?\n\nPart of the premise is correct, an ship who can make an copy of itself will need to be more complex than an factory building the ship as it will also have to be able to build the factory.\n\nStill its an upper level, out technological civilization can make all parts it need to operate, an more advanced technology will add extra complexity but still an upper level. \n\nthe VNP does not need to be the size of our industry as the industry produces far more parts than the probe needs, just as important industry is focused on mass production too meet the demand and keep the cost down. This increase the size and complexity a lot, making an simple plastic or metal piece for mass production take an long production line, if you need a one off an 3d printer or crc lathe will do the work. Still you will need loads of stuff, electronic, wires, tubing, drill bits, engines, sensors. each will need an long production line from raw materials. \n\nFew has done any work on this as we could not build an practical VNP now anyway."},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T11:14:16Z","Content":"\u003E \n\u003E In which way reality disagrees with my interpretation? Provide a detailed example, where reality creates useful information from nothing in a deterministic way.\n\nAll life on earth. The \u0022nothing\u0022 is that the information to form the life on earth never existed until random chance and a ratcheting algorithm generated it. The physics of the planet are deterministic...\n\nOur Von Neumann probes can use such a ratcheting algorithm. And you can make them deterministic, where the seed of that algorithm is preset, and the answers are also preset. Or you can make them partly deterministic, where the probability of getting a useful result is far more likely than the chance of garbage.\n\nA simple example : the probe has an inbuilt simulation of physics. As it encounters technical problems, it simulates using an evolutionary algorithm possible solutions to the problem. The computer doing this might not be quite deterministic (big multiprocessing systems are often not), but the answer it creates passes the simulation test for functionality. A series of design variants are generated and tested in the real world, as small simulation errors mean that the best design in the sim may not work exactly as well in the real world. The design variant with the highest overall score is used...\n\nI see no reason the probe couldn\u0027t use such a technique to design a more sophisticated version of itself, limited of course by ultimate theoretical limits of how efficiently you can arrange matter to perform the probe\u0027s mission objectives.\n\nIn all these cases, you are definitely getting something containing more information than you started with. You are definitely lowering entropy locally for your probe. Doesn\u0027t that little equation of yours say this is impossible?"},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-07T12:38:05Z","Content":"\u003E \n\u003E All life on earth. The \u0022nothing\u0022 is that the information to form the life on earth never existed until random chance and a ratcheting algorithm generated it. The physics of the planet are deterministic...\n\nNote that I was using \u0022deterministic\u0022 in the computer science sense. A deterministic machine knows in advance what it\u0027s trying to build, while a nondeterministic machine (such as a living cell) introduces some elements of randomness to the product. A deterministic machine can\u0027t create new information, while a nondeterministic machine can. On the other hand, because the new information arises from randomness, it\u0027s almost always detrimental to the purpose of the machine, and often makes the product non-viable.\n\n\u003E \n\u003E In all these cases, you are definitely getting something containing more information than you started with. You are definitely lowering entropy locally for your probe. Doesn\u0027t that little equation of yours say this is impossible?\n\nWhich definition of entropy you\u0027re using? In algorithmic context, lowering the entropy generally means that you\u0027re losing information, not gaining it."},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T15:54:55Z","Content":"\u003E \n\u003E Note that I was using \u0022deterministic\u0022 in the computer science sense. A deterministic machine knows in advance what it\u0027s trying to build, while a nondeterministic machine (such as a living cell) introduces some elements of randomness to the product. A deterministic machine can\u0027t create new information, while a nondeterministic machine can. On the other hand, because the new information arises from randomness, it\u0027s almost always detrimental to the purpose of the machine, and often makes the product non-viable.\n\u003E Which definition of entropy you\u0027re using? In algorithmic context, lowering the entropy generally means that you\u0027re losing information, not gaining it.\n\nI was as well. You don\u0027t know in advance what you\u0027re trying to build, however, the non deterministic algorithm got tested before you left at the host star, and was tested with a large number of different starting seeds until you found one that resulted in something that met your design constraints. (it isn\u0027t an exact match to your blueprint, it\u0027s basically a form of lossy compression)\n\nUmm, a non deterministic algorithm seeded with a starting number that results in a desired result that will be arrived at every time...what have we done here? \n\nThe entropy definition I\u0027m using is the physics one - the probe\u0027s a physical object. As you gain information stored in your memory banks and more tiny parts as the probe gets larger with more systems, entropy decreases because the *order* of the many atoms crammed into your probe matters, and that order has been forced into a low entropy configuration."},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-07T16:46:52Z","Content":"\u003E \n\u003E You don\u0027t know in advance what you\u0027re trying to build, however, the non deterministic algorithm got tested before you left at the host star, and was tested with a large number of different starting seeds until you found one that resulted in something that met your design constraints. (it isn\u0027t an exact match to your blueprint, it\u0027s basically a form of lossy compression)\n\u003E Umm, a non deterministic algorithm seeded with a starting number that results in a desired result that will be arrived at every time...what have we done here?\n\nA completely deterministic algorithm that can\u0027t create new information. Pseudo-random number generators are just iterated hash functions. The only randomness in their output comes from the seed. If we fix the seed in advance, we get an ordinary deterministic algorithm. Even if we run the algorithm with a truly random seed, the amount of new information it can create is limited by the size of the seed.\n\nEntropy and information content both refer essentially to the shortest description of the object. If we take a non-deterministic algorithm with a hard-coded seed, the algorithm is obviously a description of its output. As the shortest description of the output can\u0027t be longer than a concrete description we already know, the entropy/information content of the output can\u0027t be higher than that of the algorithm.\n\n\u003E \n\u003E The entropy definition I\u0027m using is the physics one - the probe\u0027s a physical object. As you gain information stored in your memory banks and more tiny parts as the probe gets larger with more systems, entropy decreases because the *order* of the many atoms crammed into your probe matters, and that order has been forced into a low entropy configuration.\n\nWe\u0027re talking about information and complexity, so the information-theoretic definition of entropy is the correct one. A few examples:\n\n- If we have a binary sequence consisting of n 1-bits, its information content and entropy are both around log n bits.\n- If we have a random binary sequence of length n, its information content is around log n bits, while its entropy is around n bits.\n- If we have a computer program of size n (bits), its entropy is probably around n/10 bits, while its information content is lower.\n- If we take the same computer program and encode it in the low-order bits of a byte sequence, and then encrypt the sequence, we have a random-looking bit sequence of length 8n. The entropy and the information content of the sequence are still roughly the same as in the previous case."},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T17:33:47Z","Content":"Ok, so we can build self modifying algorithms right this second that start simple and become far more complex than their original source code. Practically all of the neural net-AI algorithms give you this result. There is some simple code that defines the ANN, the rules used to evaluate a given ANN configuration\u0027s score, and the generational breeding.\n\nNow, all the examples are fed a dataset, though, and you would argue that the information is coming from the dataset. The complexity of the input data set is greater than the resulting neural network to solve that data set - an example of a mario AI I saw only had a few hundred neurons, while the source code to Mario itself, plus the structure of the computer chip that runs Mario, is probably much more complex. More complex examples like an AI that recognizes dog breeds is obviously using a far more complex input data set.\n\nAssuming you can build a probe that eats rocks and designs itself new equipment to eat rocks better, maybe the information to do so is coming from the laws of physics of the real world? That is, the probe has a test chamber, and it builds physical rock-eating robots. It tests how well the robots eat rocks, and promotes the ones that have higher scores. That \u0022test chamber\u0022 contains complex pieces of rock, is governed by whatever mechanism calculates the answers to the laws of physics, and the laws themselves create complex and difficult to predict results that are all data input to this hypothetical machine of yours.\n\nI think your arguments are correct for an error-free, closed digital system that has no information inputs of any kind. However, a von neumann probe is not such a system, which is where you went wrong - it\u0027s adding information because it\u0027s using sensors that are injecting additional information, such as a sensor that measures the rock-eating performance of a probe subsystem. You may argue that you want to pretend it\u0027s a spherical cow and doesn\u0027t require input matter and energy, but that would be simplifying it inappropriately.\n\nSorry if I seem so determined by my position when I don\u0027t know the details of the theory you\u0027re using. Most people don\u0027t. Does this theory have any practical applications? As far as I am aware, the absolute state of the art in useful computer science - stuff that is only marginally useful because it is so new - is various forms of artificial neural network, where recent results have gotten much better because of the use of GPUs and more sophisticated rules for the ANN that use tricks stolen from nature.\n\n**Edited \u003Ctime datetime=\u00222015-08-07T17:40:16Z\u0022 title=\u002208/07/2015 05:40  PM\u0022 data-short=\u00228 yr\u0022\u003EAugust 7, 2015\u003C/time\u003E by SomeGuy12**"},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-07T17:52:27Z","Content":"\u003E \n\u003E Please be more specific. What are the exact statements you\u0027re interested in, what is your understanding of the issues, and why do you think so?\n\nNo, I do not want or need to be more specific, I want to put your money where your mouth is. You have repeatedly stated your views in this thread, yet never substantiated them, most specifically your ideas of ever increasing complex products needing ever more complex production facilities. Please state your definitions and then back those up with facts and sources.\n\nIt is a very interesting subject, but going round in circles in to too terribly useful, therefore I ask you to stop distracting and changing the subject.\n\n**Edited \u003Ctime datetime=\u00222015-08-07T19:05:34Z\u0022 title=\u002208/07/2015 07:05  PM\u0022 data-short=\u00228 yr\u0022\u003EAugust 7, 2015\u003C/time\u003E by Camacha**"},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-07T18:42:04Z","Content":"\u003E \n\u003E Ok, so we can build self modifying algorithms right this second that start simple and become far more complex than their original source code. Practically all of the neural net-AI algorithms give you this result. There is some simple code that defines the ANN, the rules used to evaluate a given ANN configuration\u0027s score, and the generational breeding.\n\nSelf-modification doesn\u0027t help, but observing the environment and experimenting with it can extract information from it. With them, we come back to some of the issues I mentioned earlier:\n\n- A single probe might not be smart enough or creative enough to figure out the solutions. To alleviate the problem, we need multiple autonomous entities with enough equipment to do the experiments they choose to do independently. Essentially, we need to launch multiple probes instead of just one.\n- Experimentation in an unknown environment is inherently dangerous. A significant fraction of the probes can be expected to damage or destroy themselves in the process. The obvious solution is to launch even more probes to the same destination.\n\nNow the whole thing is starting to look more like colonization than von Neumann probes.\n\n\u003E \n\u003E Sorry if I seem so determined by my position when I don\u0027t know the details of the theory you\u0027re using. Most people don\u0027t. Does this theory have any practical applications? As far as I am aware, the absolute state of the art in useful computer science - stuff that is only marginally useful because it is so new - is various forms of artificial neural network, where recent results have gotten much better because of the use of GPUs and more sophisticated rules for the ANN that use tricks stolen from nature.\n\nProbability and information are two sides of the same coin. There are many fields and subfields of research studying their fundamental properties, and algorithmic information theory is one of them. Anyone who\u0027s serious about (for example) data compression, error correction, cryptography, or machine learning should be familiar with it.\n\nI haven\u0027t really followed machine learning and the neighboring fields for a decade or so. Back then, people were mostly excited about Bayesian inference and support vector machines."},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-07T19:09:56Z","Content":"\u003E \n\u003E Self-modification doesn\u0027t help, but observing the environment and experimenting with it can extract information from it. With them, we come back to some of the issues I mentioned earlier:\n\u003E \n\u003E \n\u003E - A single probe might not be smart enough or creative enough to figure out the solutions. To alleviate the problem, we need multiple autonomous entities with enough equipment to do the experiments they choose to do independently. Essentially, we need to launch multiple probes instead of just one.\n\u003E - Experimentation in an unknown environment is inherently dangerous. A significant fraction of the probes can be expected to damage or destroy themselves in the process. The obvious solution is to launch even more probes to the same destination.\n\nSelf modification helps, since it can help probes to adapt to problems that previously could not be tackled. Sending more probes is not necessary, since reproduction does exactly that. They might even start forming communities of probes, with each having a specialisation, but that is for the probes to find out.\n\nIt would be human hubris to think we can come up with something that will fit the bill in all cases. Also, could you respond to the request in [my](https://forum.kerbalspaceprogram.com/threads/130333-Von-Neumann-probes?p=2125421\u0026viewfull=1#post2125421) previous [posts](https://forum.kerbalspaceprogram.com/threads/130333-Von-Neumann-probes?p=2123843\u0026viewfull=1#post2123843)?"},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T19:12:24Z","Content":"\u003E \n\u003E Now the whole thing is starting to look more like colonization than von Neumann probes.\n\nAren\u0027t we arguing about the size of the apple now? We could :\n\n1. Package several autonomous systems into a single probe. By sharing subsystems (you need a minimum amount of mass to absorb the gamma rays produced by an antimatter fired starship engine, so a bigger engine is more efficient) you save on total mass. Once it arrives at the destination, the autonomous probes split up...kind of like cancer does when it colonizes the body...so that a single adverse event or failure won\u0027t stop the overall effort.\n\n2. Once we consume an entire star\u0027s worth of retrievable solid matter (presumably any star in our local group will have at least an earth-mass or more worth of solid rocks captured around it) we have an awful lot of resources for launching the next set of \u0022probes\u0022 to a star not yet colonized. Once you are converting earth-masses or more worth of rocks into machinery, you have an awful lot of it. \n\nThis, by the way, means that a realistic probe might in fact have incredibly sophisticated software systems, equivalent to cramming an artificial neural network emulating the mind states of a 100 people or more into it. If you had the equivalent of the minds of 100 people, you would be able to do very intelligent experimentation and engineering. You do realize that this probably would fit into 100 kilograms or less of computing machinery, assuming you had 3 dimensional, nanostructured hardware. (the brain weighs 1.3 kilograms, and obviously you don\u0027t need many of the brain\u0027s systems for this, also, you could make equivalent circuits with a lot less mass because they don\u0027t have to be alive and self repairing nor use proteins...)\n\nAnd the reason it agrees with your theory above is that when this probe encounters an obstacle it lacks the programming to overcome, it can extract information from the environment and use that combined with base programming (knowledge about the laws of physics, prior techniques for similar problems, etc etc etc) to craft a new solution. It\u0027s not a closed system and information is both entering and leaving.\n\nFor that matter, isn\u0027t a human brain, like the one you are using the read this message, a device that destroys information on a colossal scale? The hugely complex state your mind is in right now is only transient, and only a tiny fraction of the information will be stored.\n\n**Edited \u003Ctime datetime=\u00222015-08-07T19:16:32Z\u0022 title=\u002208/07/2015 07:16  PM\u0022 data-short=\u00228 yr\u0022\u003EAugust 7, 2015\u003C/time\u003E by SomeGuy12**"},{"CreatedByName":"AndrewBCrisp","CreatedById":57119,"CreatedDateTime":"2015-08-07T19:34:18Z","Content":"\u003E \n\u003E An VNP with an error would stop working as in unable to repair itself or make other parts. Its very unlikely that it would say start producing more and more mining robots who mine or similar cancer like replication problems. its some ways to make this even more safe, if you compress and encrypt the relevant files any bit fail will result in scrambled data, you then use error correction and multiple copies to avoid corruption. Now design system so it regularly has to read the files from disc. \n\u003E The VNP would not have much to do with planets other than land probes on it for exploring. If you see any sign of an advanced civilization, radio, primarily but also light and it would report back and go into some sort of ambassador role. An pre industrial civilization would be hard to detect but would also not notice anything except perhaps an lander or two. \n\u003E \n\u003E I imagine the VNP would have two purposes, primarily would be exploration they would move outward exploring solar systems, report back in the chain make copies of itself for more exploration. It would not send VNP to systems who have them, might send an backup probe to an system with ship underway but nothing more.\n\u003E \n\u003E After the expansion part it would maintain the VNP communication network and report back. it might also get new orders or plans from the operators. \n\u003E \n\u003E An second stage might be to prepare a planet for colonization.\n\nSafe replication is not my concern, per se. It is important to consider, and you raise a good point that runaway replication can be guarded against, but my argument (and Keith Cooper\u0027s) is that the probe must be *responsible.* I believe that checking for civilizations and establishing contact must be the first thing on a VNP\u0027s \u0022to-do\u0022 list as soon as it enters a target system, rather than (if I read your response correctly), as a side activity to replication / colonization prep. \n\nAn argument can be made that the resources of another star system, if the system has a life-bearing planet, belongs to the present or future civilization on said planet. If the first thing our probe did was to make more copies of itself as soon as it arrives, the native civilization may see that as *stealing*. I cannot imagine they will approve, or be receptive to the probe making contact later on. (As an analogy, imagine I entered your home without knocking or asking permission, and immediately helped myself to the contents of your fridge. You\u0027d be pretty ticked at me, I\u0027d suspect. An alien civilization would behave similarly to a probe mooching off a few asteroids... and may be doubly ticked at the civilization that sent it). \n\nAs for pre-industrial civilizations, or even planets where sentient life has yet to evolve, again courtesy plays a role here. Suppose a VNP arrived in our solar system a million years ago. No human civilization was around, and by your criteria, the VNP would then decide that it can do whatever it wants with the resources of our system, and digs in. Maybe it\u0027s makers are still interested and decide to send some colonists along as well. By the time we reach the present age, when our civilization is slowly getting interested in expanding into space, we\u0027d find that we\u0027ve been crowded out or that all the good resources have been used up. We might be able to settle the inner planets, but everything else is occupied or consumed. The native peoples of North America can tell you what that\u0027s like, to have your future cut off by another civilization. \n\nI believe we need to be more responsible than that, which means that if our future probe is to replicate without asking permission, it should limit its numbers - two or three probes to send to other stars at **most** - and cease replicating altogether once those two or three probes are on their way. So long as the probe can maintain itself, it can afford to be patient, waiting millions of years if need be before someone it can talk to evolves and starts exploring.\n\nEither way, the probe would need to wait until it can contact with a native civilization and engage in a cultural exchange. Then, and *only* then, can it ask for permission from the natives to replicate, and do so *only* if the natives agree.\n\nIncidentally, as probes might continue on in a chain from system to system for millenia, and thus contact with the parent civilization is likely to dry up, the probe will likely need a true AI for its brain - not just to make the judgement call on to replicate or not, but also to interact with the native civilization on its own and to negotiate for permission to replicate so it can expand the network. A possible trade is to offer to carry an AI modeled off the natives, so that the natives\u0027 culture can be carried to the next star, as payment for the asteroids needed to build those two or three VNPs."},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T19:44:52Z","Content":"Andrew, the reason your idea will not work is because a probe setup that doesn\u0027t care about the rules would outbreed/outcompete one that does. Turning whole star systems into more probes, faster probes, or much bigger probes, armed with weapons, is a better strategy than nibbling a few asteroids and waiting. That\u0027s the lowest common denominator and what you would expect to see. \n\nThe fact that we exist at all strongly suggests that alien intelligences capable of building such things are very, very, very far away from us. (not in our galaxy far)"},{"CreatedByName":"AndrewBCrisp","CreatedById":57119,"CreatedDateTime":"2015-08-07T20:00:22Z","Content":"With respect, Someguy12, to suggest that any alien intelligences capable of building VNPs would choose to release what amounts to a technological plague on the galaxy speaks very poorly of our hypothetical aliens\u0027 ethics. Surely they are at least a *little* wiser than that. \n\nBut even if aliens *have* chosen the low road, caring for nothing but themselves, does not mean that we should do likewise."},{"CreatedByName":"KSK","CreatedById":61025,"CreatedDateTime":"2015-08-07T20:32:52Z","Content":"\u003E \n\u003E With respect, Someguy12, to suggest that any alien intelligences capable of building VNPs would choose to release what amounts to a technological plague on the galaxy speaks very poorly of our hypothetical aliens\u0027 ethics. Surely they are at least a *little* wiser than that. \n\u003E But even if aliens *have* chosen the low road, caring for nothing but themselves, does not mean that we should do likewise.\n\nOr hopefully those aliens would take the Ian M Banks approach and decide that turning chunks of the galaxy into swarms of identical machines would be a) too egotistical to be cool and ![B)](//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_cool.png) deeply, deeply *boring*.\n\n- - - Updated - - -\n\n\u003E Incidentally, as probes might continue on in a chain from system to system for millenia, and thus contact with the parent civilization is likely to dry up, the probe will likely need a true AI for its brain - not just to make the judgement call on to replicate or not, but also to interact with the native civilization on its own and to negotiate for permission to replicate so it can expand the network. A possible trade is to offer to carry an AI modeled off the natives, so that the natives\u0027 culture can be carried to the next star, as payment for the asteroids needed to build those two or three VNPs.\n\nNow *that\u0027s* a beautiful idea. Humanity finally makes it out to the asteroid belt, uncovers alien probe - and in exchange for a couple of lumps of rock we get our own ambassador to the stars."},{"CreatedByName":"SomeGuy12","CreatedById":103022,"CreatedDateTime":"2015-08-07T21:13:23Z","Content":"The problem it\u0027s a quickdraw contest/who can be the biggest jerk. Whoever creates the \u0022techno-plague\u0022 first will have the biggest impact on future events in the galaxy. By choosing not to do this, you are choosing a suboptimal method for survival. (you\u0027d attach copies of yourselves to the machinery, so the farther the plague spreads, the farther you personally spread)\n\nSame argument applies in that it\u0027s better to drain a significant fraction of the resources of a star, if needed, building a really really big rocket that can hit a higher fraction of C. Well, if you need to do that - there probably is an efficient and elegant method that gets you to 0.9 C and only requires eating a few asteroids to build the equipment. An antimatter rocket with a big enough mass ratio can do this in theory."},{"CreatedByName":"AndrewBCrisp","CreatedById":57119,"CreatedDateTime":"2015-08-07T21:38:46Z","Content":"Of course, that assumes that \u0022being the biggest jerk\u0022 as you so aptly put it is an optimal survival strategy. It\u0027s tempting, but I suspect that it is not feasible over the long run... or else locusts would have driven everything larger than them to extinction long ago. \n\nEarthly biology shows that species that replicate without limit usually either have high attrition rates (ensuring only a few survive long enough to have offspring) or risk destroying themselves by burning through the available food faster than is sustainable. How this might play out on a galactic scale is a tough question to answer, as we know little or nothing about what might serve as \u0022natural\u0022 checks on VNP population. But I would also suggest that those civilizations that chose **not** to be jerks might soon find it in their interest to start culling jerk-minded VNPs (and their parent civilizations); especially if they\u0027ve been on the receiving end of a jerk-class VNP. \n\nI would hope that most civilizations that survive long enough to design and build a VNP will also see the folly of being jerks to the galaxy. Or, to quote Commander Norton from Clarke\u0027s *Rendezvous with Rama*, \u0022The human race has to live with its conscience. [Whatever the Hermians argue,] survival is not everything.\u0022"},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-07T21:54:52Z","Content":"Somehow I see a connection with stars, where both being large and being small seems to be a downside ![:)](//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif)"},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-08T01:49:00Z","Content":"\u003E \n\u003E Self modification helps, since it can help probes to adapt to problems that previously could not be tackled.\n\nThe context was information/complexity. Self-modification can\u0027t create any new information, so it\u0027s an irrelevant technical detail in that discussion.\n\n\u003E \n\u003E Sending more probes is not necessary, since reproduction does exactly that. They might even start forming communities of probes, with each having a specialisation, but that is for the probes to find out.\n\nSending more probes is necessary, because exploration and experimentation is a risky business.\n\nLook at any item around you. Before we could figure out how to do it right, people probably died because of our early failed attempts. I don\u0027t see any reason why developing new stuff would be less risky for probes.\n\n\u003E \n\u003E Also, could you respond to the request in [my](https://forum.kerbalspaceprogram.com/threads/130333-Von-Neumann-probes?p=2125421\u0026viewfull=1#post2125421) previous [posts](https://forum.kerbalspaceprogram.com/threads/130333-Von-Neumann-probes?p=2123843\u0026viewfull=1#post2123843)?\n\nI believe that the discussion about algorithmic information answers those requests.\n\n\u003E \n\u003E Aren\u0027t we arguing about the size of the apple now?\n\nI think it\u0027s an important distinction.\n\nVon Neumann probes are essentially a brute-force approach to interstellar expansion. A probe arrives at a new star system, looks at the system arrogantly, imposes its will upon the lowly matter (regardless of whether it\u0027s just a dead planet or an advanced civilization), and creates a bunch of identical copies of itself to continue the expansion. Colonization, on the other hand, is more organic. You explore the system, determine what kind of production it can support, and figure out ways to produce something that can continue the expansion to the next star system."},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-08T02:15:19Z","Content":"\u003E \n\u003E The context was information/complexity. Self-modification can\u0027t create any new information, so it\u0027s an irrelevant technical detail in that discussion.\n\nSelf-modification would mostly be used to adapt to local or new circumstances, but you could even truly introduce new information like our DNA does through mutation.\n\n\u003E \n\u003E Sending more probes is necessary, because exploration and experimentation is a risky business.\n\u003E Look at any item around you. Before we could figure out how to do it right, people probably died because of our early failed attempts. I don\u0027t see any reason why developing new stuff would be less risky for probes.\n\nSure, but you only need enough to ensure that ones survives to reproduce. You might even make the first generation or unit extremely conservative, and only go into full exploration mode when enough copies have been made to guarantee a certain level of survival. Maybe the probes could even produce simplified sacrificial units.\n\n\u003E \n\u003E I believe that the discussion about algorithmic information answers those requests.\n\nWe were talking about physical complexity and production, not software complexity ![:)](//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif) If you would be so kind as to answer the questions in relation to physical devices, that would be splendid."},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-08T12:43:09Z","Content":"\u003E \n\u003E Self-modification would mostly be used to adapt to local or new circumstances, but you could even truly introduce new information like our DNA does through mutation.\n\nThe new information comes from random events, not from self-modification itself. Furthermore, the random events typically have a neutral or negative effect on the system, so they\u0027re not a very good source of new information.\n\n\u003E \n\u003E Sure, but you only need enough to ensure that ones survives to reproduce. You might even make the first generation or unit extremely conservative, and only go into full exploration mode when enough copies have been made to guarantee a certain level of survival. Maybe the probes could even produce simplified sacrificial units.\n\n\u0022Extremely conservative\u0022 also means \u0022extremely bad at making new discoveries\u0022. The first probes have to take more risks to be able to do anything useful, while the later ones can benefit from the discoveries of their predecessors and be more conservative.\n\n\u003E \n\u003E We were talking about physical complexity and production, not software complexity ![:)](//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif) If you would be so kind as to answer the questions in relation to physical devices, that would be splendid.\n\nThe fun thing about the core areas of theoretical computer science is that the results are applicable to all kinds of systems, not just computers. The explicit purpose of Turing machines was to develop formalism that could simulate any kind of machine, no matter whether it\u0027s abstract of physical, and then prove that there are things no machine can do. Computers were just an unexpected application of the results.\n\nWe can choose any reasonable standard for describing physical objects, and define the complexity of an object to be length of the shortest description of all of its relevant characteristics. It doesn\u0027t matter which one we choose, because all of them are equivalent, up to an additive constant. After we have chosen the standard, the results from algorithmic information theory are applicable to physical objects described using the standard."},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-08T13:20:14Z","Content":"\u003E \n\u003E The new information comes from random events, not from self-modification itself. Furthermore, the random events typically have a neutral or negative effect on the system, so they\u0027re not a very good source of new information.\n\nThat makes no sense. Why would random events have a neutral or negative effect? You also seem to have ignored the benefits of random modification, see my comparison with DNA.\n\n\u003E \n\u003E \u0022Extremely conservative\u0022 also means \u0022extremely bad at making new discoveries\u0022. The first probes have to take more risks to be able to do anything useful, while the later ones can benefit from the discoveries of their predecessors and be more conservative.\n\nAgain, this makes no sense. There is no reason why the initial probe could not be conservative until it has multiplied. It does not need to discover, it needs to multiply. Multiplying initially is a relatively simple task, with little risk.\n\n\u003E \n\u003E The fun thing about the core areas of theoretical computer science is that the results are applicable to all kinds of systems, not just computers. The explicit purpose of Turing machines was to develop formalism that could simulate any kind of machine, no matter whether it\u0027s abstract of physical, and then prove that there are things no machine can do. Computers were just an unexpected application of the results.\n\u003E We can choose any reasonable standard for describing physical objects, and define the complexity of an object to be length of the shortest description of all of its relevant characteristics. It doesn\u0027t matter which one we choose, because all of them are equivalent, up to an additive constant.\n\nInteresting statements, but lacking any substantiation that your computer science approach is in any shape, way or form applicable to our earlier discussion about physical products and real world probes. You are just trying to shift the problem somewhere else, but not coming any closer to finally proving anything.\n\nI am going to ask you one more time to stop bobbing and weaving and creating smoke screens and start backing things up and quoting sources. The alternative would be for everyone to ignore your statements completely. This thread deserves more than words without meaning."},{"CreatedByName":"Jouni","CreatedById":97346,"CreatedDateTime":"2015-08-08T14:27:44Z","Content":"\u003E \n\u003E That makes no sense. Why would random events have a neutral or negative effect? You also seem to have ignored the benefits of random modification, see my comparison with DNA.\n\nDNA is a perfect example of this. While mutations are the only source of new genetic information, their short-to-medium term effects are mostly harmful and rarely beneficial. Cells have a plenty of mechanisms for preventing mutations, and for repairing the damage if a mutation happens. There is also a lot of redundancy in the genome, which helps to minimize the effects of mutations.\n\nIn a large population, which can afford to lose a plenty of individuals, a low mutation rate can help to increase the genetic variation in the population, increasing its fitness in the long term (after hundreds or thousands of generations). In short-to-medium term, the primary adaptation mechanism is recombining the genetic variation that already exists in the population. Recombination is much less likely to have adverse effects than mutation, because individuals have already lived with all of the genes involved.\n\n\u003E \n\u003E Again, this makes no sense. There is no reason why the initial probe could not be conservative until it has multiplied. It does not need to discover, it needs to multiply. Multiplying initially is a relatively simple task, with little risk.\n\nReplication is not as simple as copying a piece of data. It involves adapting the resource extraction and production activities of the probe to the conditions in the new environment. Many of these activities may involve location-specific risks that are not always apparent in advance. If the probe can\u0027t afford to take risks, it may not have access to all the resources it needs, or it may not be able to process them in the forms they occur in in the current environment.\n\n\u003E \n\u003E Interesting statements, but lacking any substantiation that your computer science approach is in any shape, way or form applicable to our earlier discussion about physical products and real world probes. You are just trying to shift the problem somewhere else, but not coming any closer to finally proving anything.\n\nComputer science is the science of systems, processes, mechanisms, complexity, information, and similar things. It\u0027s a methodological science with a similar role in the 21st century science as statistics had in the 20th century science. Its English name is kind of unfortunate, because computer science is no more about computers than astronomy is about telescopes. In many other languages, people use more appropriate names, which translate as e.g. computing, informatics, datalogy, information processing science, and science of computation.\n\nComputer science is simply the most relevant field of study to this discussion."},{"CreatedByName":"Camacha","CreatedById":59088,"CreatedDateTime":"2015-08-08T14:55:57Z","Content":"\u003E \n\u003E [...]\n\nRandom events are not negative or neutral. A probe encounters an event, learns something, improves itself. The outcome is almost always positive.\n\n\u003E \n\u003E Replication is not as simple as copying a piece of data. It involves adapting the resource extraction and production activities of the probe to the conditions in the new environment. Many of these activities may involve location-specific risks that are not always apparent in advance. If the probe can\u0027t afford to take risks, it may not have access to all the resources it needs, or it may not be able to process them in the forms they occur in in the current environment.\n\nYou are confusing low risk with no risk. The premise was that you need many probes due to the risks, while I said that you probably only need a few because the intent is to replicate anyway. Exploring a full system will certainly have very risky parts, but there is no need to dive in straight away.\n\n\u003E \n\u003E Computer science is the science of systems, processes, mechanisms, complexity, information, and similar things. It\u0027s a methodological science with a similar role in the 21st century science as statistics had in the 20th century science. Its English name is kind of unfortunate, because computer science is no more about computers than astronomy is about telescopes. In many other languages, people use more appropriate names, which translate as e.g. computing, informatics, datalogy, information processing science, and science of computation.\n\u003E Computer science is simply the most relevant field of study to this discussion.\n\nThe most relevant field to physical production is actual physical production, which I do happen to know. You stated that a more complex product would require an every increasingly production facility. I have given examples, digital manufacturing through CNC machines and 3D printing, for instance, why this is not necessarily the case. Flexibility and complexity are not the same. So can we finally conclude that increasingly complex probes do not necessarily require an even more complex production environment? Great! ![:)](//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif)"}]}