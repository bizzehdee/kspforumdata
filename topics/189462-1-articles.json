{"TopicId":189462,"ForumId":44,"TopicTitle":"Old math for new problems.","CreatedByName":"steuben","CreatedById":77368,"CreatedDateTime":"2019-10-31T18:33:56Z","PageNum":1,"Articles":[{"CreatedByName":"steuben","CreatedById":77368,"CreatedDateTime":"2019-10-31T18:33:56Z","Content":"\n\u003Cp\u003E\nI\u0027ve been thinking about the math that KSP does when working with the craft. And I need some help shaping a thought.\n\u003C/p\u003E\n\u003Cp\u003E\nOne of the things taught to me by a _very_ theoretical prof back at school was this: \u0022Any impossible problem can be changed into one easy problem and one difficult one. This axiom scales nicely.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\nAnd something taught to me by an old programmer: \u0022Multiplication costs more than addition, floating point triply so.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\nMy zeroth approximation is that the bulk of the math involved in multiplication/division. So rather than use the conventional number space that we are used to, what if we convert to logarithm base 10? With a trivial amount of work the floating point numbers can be represented as a pair of 9 digit\u00A0\u00A0integers. I had thought about using logarithm base e to remove the trig functions that will end up in the calculations. But, if the idea doesn\u0027t pan using log10 then it most likely won\u0027t using ln.\n\u003C/p\u003E\n\u003Cp\u003E\nMy\u00A0zeroth approximation\u00A0is that by doing it this way processing time could be cut in half.\n\u003C/p\u003E\n\u003Cp\u003E\nLeaving aside my thoughts on processing time improvements\n\u003C/p\u003E\n\u003Cp\u003E\n1. does this pass the basic sanity checks.\n\u003C/p\u003E\n\u003Cp\u003E\n2. can gains be made with this approach or would the overhead of the conversion to logs eat any gains from the faster operations in log-space.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"StrandedonEarth","CreatedById":89439,"CreatedDateTime":"2019-10-31T19:24:11Z","Content":"\n\u003Cp\u003E\nI only sort-of follow that. Returning to slide rules?\u00A0 Or making (micro-electronic) computers use slide rules?\n\u003C/p\u003E\n"},{"CreatedByName":"steuben","CreatedById":77368,"CreatedDateTime":"2019-10-31T20:38:53Z","Content":"\n\u003Cp\u003E\nYou\u0027ve pretty much got it, making digital electronic computers use slip stick style math. Though like that old programmer taught me, \u0022never trust a computer to give you an answer you can\u0027t find yourself.\u0022\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nThough it might\u00A0be closer to adding-machine style than slip-stick.\n\u003C/p\u003E\n"},{"CreatedByName":"Superfluous J","CreatedById":73725,"CreatedDateTime":"2019-10-31T20:45:52Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698020\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572546836\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n2 hours ago, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\n\u0022Any impossible problem can be changed into one easy problem and one difficult one. This axiom scales nicely.\u0022\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nNo offense to your professor but that sounds like something a motivational speaker would say to get you to buy their DVDs. Or a consultant to a customer at my job, implying I\u0027m going to be working all weekend on a problem that actually is impossible and not just an easy problem and a hard one.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-10-31T20:46:15Z\u0022 title=\u002210/31/2019 08:46  PM\u0022 data-short=\u00224 yr\u0022\u003EOctober 31, 2019\u003C/time\u003E by 5thHorseman\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"steuben","CreatedById":77368,"CreatedDateTime":"2019-10-31T22:32:50Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698086\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572554752\u0022 data-ipsquote-userid=\u002273725\u0022 data-ipsquote-username=\u00225thHorseman\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n1 hour ago, 5thHorseman said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nthat sounds like something a motivational speaker would say to get you to buy their DVDs. Or a consultant to a customer\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThat\u0027s because they are fools, and thrive on condensed statements and sound bites.\n\u003C/p\u003E\n\u003Cp\u003E\nThough I did think the same thing when he trotted it out. \u0022You doubt me\u00A0Stu?\u0022\u00A0he said. \u0022Me a grand high wizard, sitting atop my ivory tower.\u0022 He did have a touch of drama in his speech.\u00A0I think it was all that math it does weird things to your brain. \u0022Isolated from the practical necessities of reality.\u00A0 Not as much as you may think. You see that one statement sums up ninety percent of what the art and craft of\u00A0theoretical math, and by extension any kind of problem solving, is about. How can I take this problem, that is impossible, hard, difficult, or what have you, and change it into a problem that I can easily find the answer to? That transformation is the hard part, and where the real proof of skill lies. Not in tables, proofs, axioms,\u00A0and theorems, though they are important. They are merely the tools you need.\u0022\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-11-01T00:42:37Z","Content":"\n\u003Cp\u003E\ni fathom to guess that the old programmer in questione never used a modern cpu or gpu. floating point is actually faster than int math these days, at least on x86-64 and possibly arm. ive used fixed point math on microcontrollers, gone out of my way in avoiding divide instructions, using simple bit shifts to divide by multiples of 2. on avrs hard multiply is available but its one of the 2-cycle instructions. even then if you can shift, shift. much of the slowness in the fpu isnt the fpu itself but the register ops needed to feed it and recover the result, which the move to 64 bit has effectively halved (the x87 instruction set can handle 80-bit float, but even on 64 you need 2 extra reg ops per operation). piplining is also a thing.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-01T01:19:53Z\u0022 title=\u002211/01/2019 01:19  AM\u0022 data-short=\u00224 yr\u0022\u003ENovember 1, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"razark","CreatedById":32993,"CreatedDateTime":"2019-11-01T02:40:06Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698086\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572554752\u0022 data-ipsquote-userid=\u002273725\u0022 data-ipsquote-username=\u00225thHorseman\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n5 hours ago, 5thHorseman said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\n...I\u0027m going to be working all weekend on a problem that actually is impossible and not just an easy problem and a hard one.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nEasy Problem:\u003Cbr\u003E\nIs this impossible? (Yes.)\n\u003C/p\u003E\n\u003Cp\u003E\nHard Problem:\u003Cbr\u003E\nHow do I get out of \u003Cem\u003Ethis field\u003C/em\u003E, and into a consulting position that pays ten times more to tell people what they want to hear, in a way that doesn\u0027t make them realize I have no idea what I\u0027m talking about?\u00A0 (Hell, if you know the answer, \u003Cstrong\u003Eplease\u003C/strong\u003E let me know.)\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-01T02:40:56Z\u0022 title=\u002211/01/2019 02:40  AM\u0022 data-short=\u00224 yr\u0022\u003ENovember 1, 2019\u003C/time\u003E by razark\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Xd the great","CreatedById":188222,"CreatedDateTime":"2019-11-01T07:15:29Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698224\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572576006\u0022 data-ipsquote-userid=\u002232993\u0022 data-ipsquote-username=\u0022razark\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n4 hours ago, razark said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nHard Problem:\u003Cbr\u003E\nHow do I get out of \u003Cem\u003Ethis field\u003C/em\u003E, and into a consulting position that pays ten times more to tell people what they want to hear, in a way that doesn\u0027t make them realize I have no idea what I\u0027m talking about?\u00A0 (Hell, if you know the answer, \u003Cstrong\u003Eplease\u003C/strong\u003E let me know.)\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nLook at my profile.\n\u003C/p\u003E\n"},{"CreatedByName":"mikegarrison","CreatedById":137807,"CreatedDateTime":"2019-11-01T07:44:52Z","Content":"\n\u003Cp\u003E\n\u003Cimg alt=\u00221572537517-20191031.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u0022221.35\u0022 height=\u0022768\u0022 width=\u0022346\u0022 src=\u0022http://www.smbc-comics.com/comics/1572537517-20191031.png\u0022\u003E\u003C/p\u003E\n"},{"CreatedByName":"Guest","CreatedById":-1,"CreatedDateTime":"2019-11-01T13:15:18Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698127\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572561170\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n14 hours ago, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nYou see that one statement sums up ninety percent of what the art and craft of\u00A0theoretical math, \u003Cstrong\u003Eand by extension any kind of problem solving\u003C/strong\u003E, is about.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nAnd this is where he was very, \u003Cem\u003Every\u003C/em\u003E, \u003Cstrong\u003E\u003Cem\u003Every \u003C/em\u003E\u003C/strong\u003Ewrong. Theoretical math is a peculiar field, which does \u003Cem\u003Enot \u003C/em\u003Ework like most others, in that it\u0027s completely detached from physical reality. It\u0027s a lot like philosophy, in that regard. An impossible problem is still an impossible problem, it\u0027s just that in mathematics, there are \u003Cem\u003Eno impossible problems\u00A0\u003C/em\u003E(note, I\u0027m not including things that, provably, can\u0027t be done. \u0022It can\u0027t be done\u0022 or even \u0022it can\u0027t be proven\u0022 are\u00A0perfectly valid answers to a mathematical question, which is another counter-intuitive thing about\u00A0mathematics).\n\u003C/p\u003E\n\u003Cp\u003E\nIf you try that with any other science, you\u0027ll run up against limits of physics and\u00A0technology \u003Cem\u003Every\u003C/em\u003E quickly. Even programming, which seems almighty to many, has to consider architecture limitations.\u00A0Ever wonder why every physical discovery had been predicted by theorists by a long shot? Because it\u0027s far easier to calculate that something like Higgs boson \u003Cem\u003Eshould \u003C/em\u003Eexist than actually detecting it. The former takes only your brain, a good computer, and a good theory to build upon. The latter takes a gigantic particle accelerator and a legion of Earth\u0027s brightest minds, along with some of the most powerful computers,\u00A0to sift through the data it produces. And then there\u0027s biology, in which you could argue that there\u0027s only one problem to solve.\u00A0\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698084\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572554333\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n16 hours ago, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nYou\u0027ve pretty much got it, making digital electronic computers use slip stick style math. Though like that old programmer taught me, \u0022never trust a computer to give you an answer you can\u0027t find yourself.\u0022\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nA very outdated thinking in most practical applications. Many things, especially\u00A0iterative numeric calculations, cannot be checked by hand. These days, quite often you either trust a computer, or are left with nothing. Understanding of how the algorithm works, and its limitations, is sufficient, and in many cases has to be.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-01T13:15:36Z\u0022 title=\u002211/01/2019 01:15  PM\u0022 data-short=\u00224 yr\u0022\u003ENovember 1, 2019\u003C/time\u003E by Guest\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2019-11-01T19:18:54Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698020\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572546836\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/31/2019 at 9:33 PM, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nAny impossible problem can be changed into one easy problem and one difficult one.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThat\u0027s true.\u003Cbr\u003E\nTo google and to find out the correct one from 20 pages of googled links.\u003Cbr\u003E\nMost of problems are already googled by somebody at least once. For others there is stackoverflow.\n\u003C/p\u003E\n\u003Cp\u003E\n***\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698020\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572546836\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/31/2019 at 9:33 PM, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nSo rather than use the conventional number space that we are used to, what if we convert to logarithm base 10?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThen in the end we\u0027ll get a virtual analog computer with limited accuracy like we had several decades earlier, just made of electric current rather than of metal.\u003Cbr\u003E\nBecause the world is made of exponents. Sometimes of harmonics.\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-01T19:20:01Z\u0022 title=\u002211/01/2019 07:20  PM\u0022 data-short=\u00224 yr\u0022\u003ENovember 1, 2019\u003C/time\u003E by kerbiloid\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"magnemoe","CreatedById":57801,"CreatedDateTime":"2019-11-02T04:57:36Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698366\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572614118\u0022 data-ipsquote-userid=\u002217022\u0022 data-ipsquote-username=\u0022Dragon01\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n15 hours ago, Dragon01 said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nA very outdated thinking in most practical applications. Many things, especially\u00A0iterative numeric calculations, cannot be checked by hand. These days, quite often you either trust a computer, or are left with nothing. Understanding of how the algorithm works, and its limitations, is sufficient, and in many cases has to be.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nYes, analoge or mechanical computers are real time so faster than quantum computers. Downside is that they can not really be programmed you have to build them for the purpose. And you don\u0027t want to feed them data they have previously calculated because of inaccuracy. Later is also an problem with digital computers, its was how the butterfly effect was discovered and an major problem making KSP.\n\u003C/p\u003E\n"},{"CreatedByName":"satnet","CreatedById":147366,"CreatedDateTime":"2019-11-02T05:07:34Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698020\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572546836\u0022 data-ipsquote-userid=\u002277368\u0022 data-ipsquote-username=\u0022steuben\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/31/2019 at 1:33 PM, steuben said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nMy zeroth approximation is that the bulk of the math involved in multiplication/division. So rather than use the conventional number space that we are used to, what if we convert to logarithm base 10? With a trivial amount of work the floating point numbers can be represented as a pair of 9 digit\u00A0\u00A0integers. I had thought about using logarithm base e to remove the trig functions that will end up in the calculations. But, if the idea doesn\u0027t pan using log10 then it most likely won\u0027t using ln.\n\u003C/p\u003E\n\u003Cp\u003E\nMy\u00A0zeroth approximation\u00A0is that by doing it this way processing time could be cut in half.\n\u003C/p\u003E\n\u003Cp\u003E\nLeaving aside my thoughts on processing time improvements\n\u003C/p\u003E\n\u003Cp\u003E\n1. does this pass the basic sanity checks.\n\u003C/p\u003E\n\u003Cp\u003E\n2. can gains be made with this approach or would the overhead of the conversion to logs eat any gains from the faster operations in log-space.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nFloating point is already logarithmic, and already represented by a pair of integers (plus a sign bit). The most common representation is \u003Ca href=\u0022https://en.wikipedia.org/wiki/IEEE_754\u0022 rel=\u0022external nofollow\u0022\u003EIEEE 754\u003C/a\u003E which defines several formats, but consists of a sign bit, significand, and an exponent with a pre-defined base. When you use a float data type it is (usually) a 32 bit representation with 1 sign bit, 23 significand bits, and 8 exponent bits with a base of 2 (making the exponent the whole number portion of log\u003Csub\u003E2\u003C/sub\u003E(N) ). Converting this into a base 2 number looks like this N = (-1)\u003Csup\u003E(sign bit)\u003C/sup\u003E x significand x 2\u003Csup\u003E(exponent)\u003C/sup\u003E which is very similar to scientific notation except in base 2 since binary computers naturally operate with a base of 2. IEEE 754 does define a base 10 representations as well, though they are recent and if anyone uses them I\u0027m not aware of it.\n\u003C/p\u003E\n\u003Cp\u003E\nBasically you won\u0027t save any time because this is already what we do. On the other hand you have managed to come up with a perfectly viable approach with a long history of working.\n\u003C/p\u003E\n\u003Cp\u003E\nBase e is not likely to work well. Since it is an irrational number a computer needs to approximate it to some number of digits. Computing the log base 2 of a number can be computed with a fairly simple circuit, log base 10 is more complicated yet reasonable, and log base e is just an awful mess.\n\u003C/p\u003E\n\u003Cp\u003E\nP.S. I\u0027m glossing over some details. There is more to this than is relevant to the discussion.\n\u003C/p\u003E\n"},{"CreatedByName":"IncongruousGoat","CreatedById":157062,"CreatedDateTime":"2019-11-02T07:55:48Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698196\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572568957\u0022 data-ipsquote-userid=\u002210883\u0022 data-ipsquote-username=\u0022Nuke\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/31/2019 at 5:42 PM, Nuke said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nfloating point is actually faster than int math these days, at least on x86-64 and possibly arm.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThis is technically true in some cases, but highly misleading. Floating-point vector operations are generally faster than the equivalent integer vector operations would be due to the presence of specialized instructions and registers (XMM on x64 and SIMD on ARM64) to handle such operations, and this generally translates to floating point having higher performance than integer math in real world scenarios because most floating point math is vector math that can take advantage of these specialized instructions. However, a plain old double-precision floating-point add is going to be slower than an 8-byte integer add on a modern x64 or ARM64 machine.\n\u003C/p\u003E\n"},{"CreatedByName":"Guest","CreatedById":-1,"CreatedDateTime":"2019-11-02T10:06:36Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698674\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572670656\u0022 data-ipsquote-userid=\u002257801\u0022 data-ipsquote-username=\u0022magnemoe\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n4 hours ago, magnemoe said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nYes, analoge or mechanical computers are real time so faster than quantum computers. Downside is that they can not really be programmed you have to build them for the purpose. And you don\u0027t want to feed them data they have previously calculated because of inaccuracy. Later is also an problem with digital computers, its was how the butterfly effect was discovered and an major problem making KSP.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nInaccuracies aren\u0027t the problem with the machine (\u003Cem\u003Eanything\u003C/em\u003E with finite precision will exhibit similar effects), but with the\u00A0model being used. That\u0027s why it\u0027s important, when making predictions about real world,\u00A0to avoid models that are chaotic. Computers are very much capable of arbitrary precision calculations, you can\u0027t really get infinite precision, but it\u0027s enough to ensure that\u00A0inaccuracies come only from measurements themselves. Measurement errors appear to\u00A0inherent in how the world works on quantum level.\u00A0They can be reduced to a point, but they will always be a factor.\n\u003C/p\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2019-11-02T11:25:11Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698674\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572670656\u0022 data-ipsquote-userid=\u002257801\u0022 data-ipsquote-username=\u0022magnemoe\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n6 hours ago, magnemoe said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nanaloge or mechanical computers are real time so faster than quantum computers. Downside is that they can not really be programmed you have to build them for the purpose.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\n\u002B1. We live in one.\u003Cbr\u003E\nAnd noone can get the purpose.\n\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-11-02T14:11:09Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698714\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572681348\u0022 data-ipsquote-userid=\u0022157062\u0022 data-ipsquote-username=\u0022IncongruousGoat\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n6 hours ago, IncongruousGoat said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nThis is technically true in some cases, but highly misleading. Floating-point vector operations are generally faster than the equivalent integer vector operations would be due to the presence of specialized instructions and registers (XMM on x64 and SIMD on ARM64) to handle such operations, and this generally translates to floating point having higher performance than integer math in real world scenarios because most floating point math is vector math that can take advantage of these specialized instructions. However, a plain old double-precision floating-point add is going to be slower than an 8-byte integer add on a modern x64 or ARM64 machine.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nthat 8 byte integer operation is functionally no different than a 64 bit int operation. its simply going to mask and copy when moving the data in and out of the register. there are probably single instructions for that lurking in the bowels of the instruction set. the slowdowns come when you need a bigger data type than will fit in the register and you have to break it down into 2 ops. with doubles its the same deal. actually last time i checked in gcc sizeof(double) == sizeof(float). with reguards to the vector instructions, the correct answer is that its bloody complicated. x86 is especially messy.\n\u003C/p\u003E\n\u003Cp\u003E\nnow if you compare float to fixed point maths, you definitely lose performance with ints. sure you can say do your math in milimeters instead of meters for example, that works fine. i do that a lot on lesser platforms. but int is not a float and you will have limitations if you try to use it as such. you can do large fixed point operations to do what float can do, but the extra shifts and extra space (a multiply needs 2x the space of the data type, and a divide needs 4x, a 64 bit divide (say 32.32) requires a 256 bit intermediary) resulting in additional reg ops. in this case float is faster. using float when an integer will do on the other hand is bad practice because in that case the int math is faster.\u00A0\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-02T14:13:19Z\u0022 title=\u002211/02/2019 02:13  PM\u0022 data-short=\u00224 yr\u0022\u003ENovember 2, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"wumpus","CreatedById":133850,"CreatedDateTime":"2019-11-02T16:18:05Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698196\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572568957\u0022 data-ipsquote-userid=\u002210883\u0022 data-ipsquote-username=\u0022Nuke\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/31/2019 at 8:42 PM, Nuke said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\ni fathom to guess that the old programmer in questione never used a modern cpu or gpu. floating point is actually faster than int math these days, at least on x86-64 and possibly arm. ive used fixed point math on microcontrollers, gone out of my way in avoiding divide instructions, using simple bit shifts to divide by multiples of 2. on avrs hard multiply is available but its one of the 2-cycle instructions. even then if you can shift, shift. much of the slowness in the fpu isnt the fpu itself but the register ops needed to feed it and recover the result, which the move to 64 bit has effectively halved (the x87 instruction set can handle 80-bit float, but even on 64 you need 2 extra reg ops per operation). piplining is also a thing.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nNote that only \u0022floats\u0022 are unbelievably fast on consumer GPUs, doubles typically are greatly reduced (thanks to market segmentation, often much worse than simple transistor savings would imply.\u00A0 And equally often by disabling working double logic where present for the \u0022big boys\u0022).\u00A0 CPUs tend to focus more on doubles, although equally capable of cranking out as many floats as they have register ports (for those bits).\u00A0 I\u0027m not even sure they need to pipeline anymore (although I expect if you dig deep enough, plenty of pipelining has to happen even if they can execute back-to-back instructions).\n\u003C/p\u003E\n\u003Cp\u003E\nFused floating point multiply add is also pretty common.\u00A0 Believe it or not, the biggest stumbling block is if you need to round on the intermediary multiply.\u00A0 IEEE-754 appears to make floating point as hard as possible to actually lay out in transistors (example: you have to compute the full 112 bit multiply and then round to the exact 56 bit result...), but nowadays they seem to not have any problem including the full suite on GPUs.\n\u003C/p\u003E\n\u003Cp\u003E\nDivide is still a pain.\u00A0 Any pipelined or parallel algorithm is going to have nasty limitations such as inverting the thing then multiplying [favored in early vector extensions, I suspect it might be standard practice today] or converting back and forth from fourier transforms (which greatly optimises multiplication of \u0026gt;\u0026gt;100 bit factors and makes division as easy as multiplication).\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698821\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572703869\u0022 data-ipsquote-userid=\u002210883\u0022 data-ipsquote-username=\u0022Nuke\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n1 hour ago, Nuke said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nthat 8 byte integer operation is functionally no different than a 64 bit int operation. its simply going to mask and copy when moving the data in and out of the register. there are probably single instructions for that lurking in the bowels of the instruction set. the slowdowns come when you need a bigger data type than will fit in the register and you have to break it down into 2 ops. with doubles its the same deal. actually last time i checked in gcc sizeof(double) == sizeof(float). with reguards to the vector instructions, the correct answer is that its bloody complicated. x86 is especially messy.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nx86 will have to update each flag for all flagged conditions the same as 8 byte and 64 bit int operations (assuming using an ALU and not the SSE/AVX interface).\u00A0 This means the internal datapaths will be a lot more complicated than naively assumed (although the programmer will never see the difference).\u00A0 I\u0027d assume other architectures have similar things that were assumed trivial whenever they were designed.\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698675\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572671254\u0022 data-ipsquote-userid=\u0022147366\u0022 data-ipsquote-username=\u0022satnet\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n10 hours ago, satnet said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nFloating point is already logarithmic, and already represented by a pair of integers (plus a sign bit). The most common representation is \u003Ca href=\u0022https://en.wikipedia.org/wiki/IEEE_754\u0022 rel=\u0022external nofollow\u0022\u003EIEEE 754\u003C/a\u003E which defines several formats, but consists of a sign bit, significand, and an exponent with a pre-defined base. When you use a float data type it is (usually) a 32 bit representation with 1 sign bit, 23 significand bits, and 8 exponent bits with a base of 2 (making the exponent the whole number portion of log\u003Csub\u003E2\u003C/sub\u003E(N) ). Converting this into a base 2 number looks like this N = (-1)\u003Csup\u003E(sign bit)\u003C/sup\u003E x significand x 2\u003Csup\u003E(exponent)\u003C/sup\u003E which is very similar to scientific notation except in base 2 since binary computers naturally operate with a base of 2. IEEE 754 does define a base 10 representations as well, though they are recent and if anyone uses them I\u0027m not aware of it.\n\u003C/p\u003E\n\u003Cp\u003E\nBasically you won\u0027t save any time because this is already what we do. On the other hand you have managed to come up with a perfectly viable approach with a long history of working.\n\u003C/p\u003E\n\u003Cp\u003E\nBase e is not likely to work well. Since it is an irrational number a computer needs to approximate it to some number of digits. Computing the log base 2 of a number can be computed with a fairly simple circuit, log base 10 is more complicated yet reasonable, and log base e is just an awful mess.\n\u003C/p\u003E\n\u003Cp\u003E\nP.S. I\u0027m glossing over some details. There is more to this than is relevant to the discussion.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI\u0027d assume that any log operation would be based on base 2, not base e.\u00A0 This should allow for much more producable algorithms.\u00A0 This sort of thing would be ideal for things like graphics and audio, mostly because our senses send more-or-less log-based signals to the brain (or at least we interpret such signals logarithmically).\u00A0 Don\u0027t expect any great gains unless you want the output in log form.\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698674\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572670656\u0022 data-ipsquote-userid=\u002257801\u0022 data-ipsquote-username=\u0022magnemoe\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n11 hours ago, magnemoe said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nYes, analoge or mechanical computers are real time so faster than quantum computers. Downside is that they can not really be programmed you have to build them for the purpose. And you don\u0027t want to feed them data they have previously calculated because of inaccuracy. Later is also an problem with digital computers, its was how the butterfly effect was discovered and an major problem making KSP.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nYou can more or less effectively create a nth order differential equation simulator out of opamps and design in ways to change the coefficients on the fly.\u00A0 But they\u0027ll never be \u0022generally programmable\u0022.\u00A0 And the \u0022butterfly effect\u0022 becomes pretty clear long before that: consider why we even have double precision in computer.\u00A0 A \u0022float\u0022 is good for seven decimal places of accuracy.\u00A0 A \u0022double\u0022 is good for ~16.\u00A0 Only the most rigorous physical experiments will get 7 decimals of accuracy, and nobody is getting 16.\u00A0 But it isn\u0027t just the \u0022butterfly effect\u0022: rounding errors (even with \u0022perfect rounding\u0022 griped about above) will eventually corrupt even the most linear and stable algorithms.\u00A0 I remember doing a 32k Fourier transform on audio (16 bits, about 5 decimal digits) using just floats and the audio output quality was worse than 8 bit.\n\u003C/p\u003E\n\u003Cp\u003E\nBuilding a mechanical or analog computer to 70dB (a float\u0027s accuracy) of accuracy (each operation, so expect things to get degraded each operation) sounds possible.\u00A0 Getting to 150dB (double) is likely impossible (and no, the only way to break it down into \u0022hard\u0022 and \u0022simple\u0022 is by going digital).\n\u003C/p\u003E\n"},{"CreatedByName":"IncongruousGoat","CreatedById":157062,"CreatedDateTime":"2019-11-02T18:17:36Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698821\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572703869\u0022 data-ipsquote-userid=\u002210883\u0022 data-ipsquote-username=\u0022Nuke\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n3 hours ago, Nuke said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nthat 8 byte integer operation is functionally no different than a 64 bit int operation.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI think we might have gotten hung up on a nomenclature issue. I didn\u0027t mean 8 8-bit ops - I meant a single 8-byte/64-bit integer add (so something like \u0060add rsi, 10\u0060).\n\u003C/p\u003E\n\u003Cp\u003E\nOh, and, in gcc, sizeof(float) == 4 and sizeof(double) == 8, regardless of whether or not -m32 is set. It\u0027s sizeof(int) that equals sizeof(long) if you\u0027re building a 32-bit binary (you need sizeof(long long) for a 64-bit int type).\n\u003C/p\u003E\n\u003Cp\u003E\nBut yes, floating point math\u0027s speed advantage comes because of specialized complicated instructions to do aggregate operations, mostly over vector types.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"Nuke","CreatedById":10883,"CreatedDateTime":"2019-11-02T22:20:06Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698941\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572718656\u0022 data-ipsquote-userid=\u0022157062\u0022 data-ipsquote-username=\u0022IncongruousGoat\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n4 hours ago, IncongruousGoat said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nI think we might have gotten hung up on a nomenclature issue. I didn\u0027t mean 8 8-bit ops - I meant a single 8-byte/64-bit integer add (so something like \u0060add rsi, 10\u0060).\n\u003C/p\u003E\n\u003Cp\u003E\nOh, and, in gcc, sizeof(float) == 4 and sizeof(double) == 8, regardless of whether or not -m32 is set. It\u0027s sizeof(int) that equals sizeof(long) if you\u0027re building a 32-bit binary (you need sizeof(long long) for a 64-bit int type).\n\u003C/p\u003E\n\u003Cp\u003E\nBut yes, floating point math\u0027s speed advantage comes because of specialized complicated instructions to do aggregate operations, mostly over vector types.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nhey i only know what my compiler (maybe it was visual studio idk) told me. this is why its good practice to print(sizeof((type) your types so you know for sure what you are dealing with. sometimes its not what you expect. sometimes you want a smaller type as a storage format, like to make file size smaller, so it doesnt make much sense to make all the types the same size. but again check your compiler for shenanigans.\u00A0\u00A0\n\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222019-11-02T22:22:46Z\u0022 title=\u002211/02/2019 10:22  PM\u0022 data-short=\u00224 yr\u0022\u003ENovember 2, 2019\u003C/time\u003E by Nuke\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"kerbiloid","CreatedById":129408,"CreatedDateTime":"2019-11-03T06:50:36Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nQuote\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nfloating is faster\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWe need real bits.\n\u003C/p\u003E\n"},{"CreatedByName":"wumpus","CreatedById":133850,"CreatedDateTime":"2019-11-05T19:54:10Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223698941\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572718656\u0022 data-ipsquote-userid=\u0022157062\u0022 data-ipsquote-username=\u0022IncongruousGoat\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 11/2/2019 at 2:17 PM, IncongruousGoat said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nBut yes, floating point math\u0027s speed advantage comes because of specialized complicated instructions to do aggregate operations, mostly over vector types.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThat and transistors are as close to free as possible.\u00A0 In CPUs, the limiting factor is likely clockspeed and the how complicated an instruction is (unless it needs to interact with memory) really isn\u0027t an issue.\u00A0 Once vectors get to 512 bits long or so, power becomes a problem, but otherwise ungodly complicated instructions \u003Cspan\u003E\u0022just happen\u0022 every cycle.\u003C/span\u003E\u00A0 It might make sense to optimize GPU instructions to use something more simple than IEEE-754, but current GPU companies won\u0027t be interested as long as AI developers (and other non-graphics GPU buyers) want something based on floating points (Intel and cell phone GPU manufacturers might be more interested in optimizing graphics operations).\n\u003C/p\u003E\n"},{"CreatedByName":"IncongruousGoat","CreatedById":157062,"CreatedDateTime":"2019-11-06T01:07:39Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00223700520\u0022 data-ipsquote-contentid=\u0022189462\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221572983650\u0022 data-ipsquote-userid=\u0022133850\u0022 data-ipsquote-username=\u0022wumpus\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n4 hours ago, wumpus said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nIn CPUs, the limiting factor is likely clockspeed and the how complicated an instruction is (unless it needs to interact with memory) really isn\u0027t an issue.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThis isn\u0027t really the case. Modern CPUs do a whole lot of complicated stuff under the hood to speed up instruction execution, and the execution time for a given instruction can range anywhere from a fraction of a cycle to dozens of cycles, depending on instruction complexity, instruction operands (independent of complexity), branch predictor accuracy, dependencies, and pipeline contents at time of execution. Some instructions on x86 and x64 will even vary in execution time based on the literal values of the operands \u003Cem\u003Eat runtime\u003C/em\u003E (!).\n\u003C/p\u003E\n\u003Cp\u003E\nIf you want to do some more reading into the nitty-gritty of per-instruction processor performance, I found \u003Ca href=\u0022http://www.agner.org/optimize/instruction_tables.pdf\u0022 rel=\u0022external nofollow\u0022\u003Ethis pdf\u003C/a\u003E, which is fairly comprehensive.\n\u003C/p\u003E\n"}]}