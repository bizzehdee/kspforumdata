{"TopicId":219798,"ForumId":44,"TopicTitle":"Non-comparative sorting algorithm - CurveSort","CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-10-05T19:22:29Z","PageNum":1,"Articles":[{"CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-10-05T19:22:29Z","Content":"\n\u003Cp\u003E\nWay back in my undergrad comp sci I class, we went through the usual steps of learning to code various sorting algorithms: bubble sort, quicksort, mergesort, and so forth. If you aren\u0027t familiar, all these sorting algorithms are comparison sorts, meaning that they sort a list of elements by successive greater-than/less-than comparison of individual elements. Conceptually, they function like a person with a balance scale and a set of unmarked weights, successively comparing individual weights against each other until all values have been sorted. Lots of energy in the computer science and mathematical logic worlds has been expended trying to design efficient sorting algorithms for data.\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022220px-Sorting_quicksort_anim.gif\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002276.36\u0022 height=\u0022168\u0022 width=\u0022220\u0022 src=\u0022https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Sorting_quicksort_anim.gif/220px-Sorting_quicksort_anim.gif\u0022\u003E\u003Cimg alt=\u0022Bubble-sort-example-300px.gif\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002260.00\u0022 height=\u0022180\u0022 width=\u0022300\u0022 src=\u0022https://upload.wikimedia.org/wikipedia/commons/c/c8/Bubble-sort-example-300px.gif\u0022\u003E\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Csup\u003E(Quicksort, left, and Bubble Sort, right)\u003C/sup\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nIt is mathematically proven that the best-case guaranteed runtime for any purely comparison-based sorting algorithm is\u00A0\u003Cstrong\u003E\u003Cem\u003En*log(n)\u003C/em\u003E\u003C/strong\u003E, where the number of comparisons required is equal to some constant (usually disregarded) multiplied by the number of list items, multiplied by the logarithm of the number of list items. Some algorithms can have better runtime on certain datasets but worse average performance, while other algorithms have better average performance but terrible performance (up to \u003Cstrong\u003E\u003Cem\u003En\u003Csup\u003E2\u003C/sup\u003E\u003C/em\u003E\u003C/strong\u003E or worse) on certain datasets. As long as you\u0027re sorting by successive comparisons,\u00A0\u003Cem\u003E\u003Cstrong\u003En*log(n)\u003C/strong\u003E\u00A0\u003C/em\u003Eis the shortest set of comparisons you can be sure will properly sort the set.\u00A0At the very best-case scenario, the number of comparisons you need is a simple factor of \u003Cstrong\u003E\u003Cem\u003En\u003C/em\u003E\u003C/strong\u003E, the number of items in the list.\n\u003C/p\u003E\n\u003Cp\u003E\nDuring my comp sci class, I came up with a half-baked idea for a sorting algorithm that wouldn\u0027t rely on individual comparisons at all, but I was having trouble figuring out how to make it complete recursively and it just stayed in the back of my head for more than a decade. At the time I had the notion of calling it \u0022bucket sort\u0022 on the theory that it would \u0022toss\u0022 the list items into a series of buckets that were arranged by size (an \u003Cstrong\u003E\u003Cem\u003En\u003C/em\u003E\u003C/strong\u003E-complex operation) and then sort those buckets individually. I had the vague sense that this approach could work well for many real-world datasets, especially ones with a normal distribution or a weighted distribution.\n\u003C/p\u003E\n\u003Cp\u003E\nThis idea came back to me last night, and I couldn\u0027t sleep so I worked it out in my head and I think I figured out how to actually make it work, not necessarily with buckets, but by defining a two-dimensional curve and then placing the list elements along that curve. Both of these are\u00A0\u003Cem\u003E\u003Cstrong\u003En\u003C/strong\u003E-\u003C/em\u003Ecomplex operations, so it would have a best-case runtime of \u003Cstrong\u003E\u003Cem\u003E2n\u003C/em\u003E\u003C/strong\u003E\u00A0(which in computer science is taken as simply equal to \u003Cem\u003E\u003Cstrong\u003En\u003C/strong\u003E\u003C/em\u003E)\u00A0for data with linear, exponential, or normal distributions. You could also conceptualize it as a \u0022histogram sort\u0022 because it essentially creates a new histogram of the dataset with each successive pass and uses that histogram to refine the curve being used to sort the data. There are still comparisons, of course, but you never do piecewise comparisons between individual list items.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cstrong\u003ELinear Case\u003C/strong\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nAs an example, I\u0027ll start by doing a purely linear-curve version of the algorithm. Consider the following set of ten random list items:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cem\u003ES = {14,39,11,41,30,37,19,18,6,27}\u003C/em\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nWe can depict this set graphically:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022Random-10.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u0022277.42\u0022 height=\u0022258\u0022 width=\u002293\u0022 src=\u0022https://i.postimg.cc/wjcY37GQ/Random-10.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nOnce sorted, we know that these list items (arranged graphically) will form some two-dimensional monotonic curve.\u00A0 Let\u0027s pretend for a moment that the curve they form when sorted is defined by your standard \u003Cstrong\u003E\u003Cem\u003Ey = mx \u002B b\u003C/em\u003E\u003C/strong\u003E\u00A0slope-intercept equation, where \u003Cem\u003E\u003Cstrong\u003Ey\u003C/strong\u003E\u00A0\u003C/em\u003Eis the value of each list item and\u00A0\u003Cstrong\u003E\u003Cem\u003Ex\u003C/em\u003E\u003C/strong\u003E\u00A0is its ordinal position in the sorted version of the list. If we knew the values of \u003Cem\u003E\u003Cstrong\u003Em\u003C/strong\u003E\u003C/em\u003E and \u003Cstrong\u003E\u003Cem\u003Eb\u003C/em\u003E\u003C/strong\u003E, the constants in that equation, we wouldn\u0027t have to do ANY piecewise comparisons at all; we could just run through the list a single time, plug in the list value for\u00A0\u003Cstrong\u003E\u003Cem\u003Ey\u003C/em\u003E\u003C/strong\u003E, and solve for\u00A0\u003Cstrong\u003E\u003Cem\u003Ex\u003C/em\u003E\u003C/strong\u003E\u00A0to figure out where to put it on the list. We can guess at\u00A0\u003Cstrong\u003E\u003Cem\u003Em\u003C/em\u003E\u003C/strong\u003E\u00A0and\u00A0\u003Cstrong\u003E\u003Cem\u003Eb\u003C/em\u003E\u003C/strong\u003E\u00A0if we know the highest value, the lowest value, and the number of list items, so let\u0027s run through the list once (\u003Cem\u003E\u003Cstrong\u003E1*n\u003C/strong\u003E\u003C/em\u003E) and get those values. We run through the list and find that the lowest value is 6, the highest value is 41, and the number of list items is 10, so we take\u00A0\u003Cstrong\u003E\u003Cem\u003Eb\u00A0\u003C/em\u003E\u003C/strong\u003E= 6 and\u00A0\u003Cstrong\u003E\u003Cem\u003Em\u003C/em\u003E\u003C/strong\u003E\u00A0= (41 - 6) / 10 = 3.5. We define a new array and project a curve over that array using our equation \u003Cem\u003E\u003Cstrong\u003Ey = 3.5x \u002B 6\u003C/strong\u003E\u003C/em\u003E:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-1.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002257.71\u0022 height=\u0022307\u0022 width=\u0022532\u0022 src=\u0022https://i.postimg.cc/jqfDZY3n/slope-intercept-1.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nWe solve for\u00A0\u003Cstrong\u003E\u003Cem\u003Ex\u003C/em\u003E\u003C/strong\u003E\u00A0in our equation (giving us\u00A0\u003Cstrong\u003E\u003Cem\u003Ex = 2(y - 6)/7\u003C/em\u003E\u003C/strong\u003E), and then we go\u00A0back to the unsorted list and run through it a second time (now up to\u00A0\u003Cem\u003E\u003Cstrong\u003E2*n\u003C/strong\u003E\u003C/em\u003E), using our equation to place the list elements along this curve. For example, the first element is 14, which gives x = 2.29 which rounds to 2, so we move it to the second slot on our new array:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-first-pass.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002265.04\u0022 height=\u0022346\u0022 width=\u0022532\u0022 src=\u0022https://i.postimg.cc/L8hdH21x/slope-intercept-first-pass.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nWe repeat for each successive list item:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-first-pass-step-2.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002265.04\u0022 height=\u0022346\u0022 width=\u0022532\u0022 src=\u0022https://i.postimg.cc/rsPppgQb/slope-intercept-first-pass-step-2.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nIf our list had a perfectly linear distribution, each list item would land perfectly on the line in perfect order, and we\u0027d be done. But it\u0027s a random set, and so eventually we will run into a value that lands in a slot we already filled. Here, our sixth list element gives us \u003Cem\u003E\u003Cstrong\u003Ex(37) = 8.86 \u2248 9\u003C/strong\u003E\u003C/em\u003E, but we already put our second list item in that slot because \u003Cem\u003E\u003Cstrong\u003Ex(39) = 9.43 \u2248 9\u003C/strong\u003E\u003C/em\u003E, so we simply tack it onto the end of our second list item in a linked list (noting for future reference the length and position of our linked list):\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-first-pass-step-3.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002265.04\u0022 height=\u0022346\u0022 width=\u0022532\u0022 src=\u0022https://i.postimg.cc/4xTD9qmg/slope-intercept-first-pass-step-3.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nFinally, we continue until we complete our run through the list, still keeping track of where we are creating linked lists due to overflow (here still we are only at \u003Cstrong\u003E\u003Cem\u003E2*n\u003C/em\u003E\u003C/strong\u003E operations):\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-first-pass-complete.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002265.04\u0022 height=\u0022346\u0022 width=\u0022532\u0022 src=\u0022https://i.postimg.cc/gjmMJFgS/slope-intercept-first-pass-complete.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nIf the values had been perfectly linear, then they would have dropped perfectly into order and the number of new linked lists would have been zero, and the computer would know that the list was sorted. But of course this was a random list and there were three places where the algorithm dropped multiple values into the same slot. (We can visually see that each of those happen to be out of order, but the computer doesn\u0027t know that; it only knows that there was overflow in those spots.) We now essentially have done the following:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022slope-intercept-before-recursion.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002247.99\u0022 height=\u0022346\u0022 width=\u0022721\u0022 src=\u0022https://i.postimg.cc/cLmJzQnV/slope-intercept-before-recursion.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nThe computer knows that the green items are sorted, and it knows that the sequences of red items are unsorted sub-lists that are nevertheless sorted relative to the green items that bound them. Here, the algorithm can simply be called recursively on each unsorted sub-list until the entire list is sorted. (In operation, you would call a simpler comparison algorithm for a two-item unsorted list, but of course you would also have plenty of sub-lists that were longer than just two-items.)\n\u003C/p\u003E\n\u003Cp\u003E\nThis algorithm (I\u0027ll call it \u0022linear curvesort\u0022) has some conceptual commonalities with heapsort and smoothsort although it is not a comparison sort and it provides a stable sort (that is, if two items have the same value, they remain in the same order they started in). It is also adaptive, meaning that it will run much faster on a dataset that is almost entirely sorted than it will on a dataset that is more randomized. Determining best-case, worst-case, and average-case performance is beyond my ken.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Cstrong\u003EBeyond Linear\u003C/strong\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nLinear curvesort works great for lists with items that have a basically linear distribution, but what about a weighted or normal distribution? For example, the following set of randomized values, when sorted, is heavily weighted to the larger end:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022upper-weighted.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002268.72\u0022 height=\u0022380\u0022 width=\u0022553\u0022 src=\u0022https://i.postimg.cc/yxH236hc/upper-weighted.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nBecause the set is weighted toward larger values, running the linear version of this algorithm would end up having a bunch of successive recursions on the right-hand side of the data:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022too-many-recursions.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002230.75\u0022 height=\u0022350\u0022 width=\u00221200\u0022 src=\u0022https://i.postimg.cc/1XNCB8Pf/too-many-recursions.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nWith this small of a list, that\u0027s not so bad, but for larger lists you would end up reshuffling the top end of the list (or any areas where the list values were weighted) many many times (approaching \u003Cstrong\u003E\u003Cem\u003En\u003Csup\u003E2\u003C/sup\u003E\u003C/em\u003E\u003C/strong\u003E) to get it sorted. Conceptually, the linear curvesort predicts a line, then refines sections of that line which don\u0027t fit the data, then refines the remaining sections, over and over not unlike a monotonic moving average.\n\u003C/p\u003E\n\u003Cp\u003E\nCan we improve on this?\n\u003C/p\u003E\n\u003Cp\u003E\nI think so. When we do our initial linear sort, we set the size of the sorting array to some number that is much smaller than the original list. For example, in this version, there are thirty list items but I\u0027m setting my sorting array to a length of just 5. Because of this, all of the list items end up in a linked sub-list, but that is by design. As before, we keep track of the sizes and locations of each linked list, which creates a histogram of the dataset (note that you don\u0027t actually have to write the list items into new locations here; you can just keep track of how many\u00A0\u003Cem\u003Ewould\u003C/em\u003E\u00A0fit into each slot as you go through):\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022histogram.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002233.96\u0022 height=\u0022324\u0022 width=\u0022954\u0022 src=\u0022https://i.postimg.cc/XqpWFr3r/histogram.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nIn theory (depending on the size of my sorting array), this histogram will be able to tell me whether the dataset distribution is normal, inverted normal, weighted high, weighted low, or something else entirely, meaning that I can make predictions about the shape of the curve formed by the sorted set. This is clearly a normal distribution that is weighted high, so I can predict the curve and use the corresponding equation to shift my list items accordingly:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022curvy.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002226.50\u0022 height=\u0022299\u0022 width=\u00221200\u0022 src=\u0022https://i.postimg.cc/bwjZxsHB/curvy.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n\u003Cp\u003E\nAs shown, the new curve is generated by drawing through the average value within each sub-list (I did it in MS Paint but of course a computer would do it mathematically). I\u0027m sure that some empirical testing could yield a good measure of how many sub-lists to use based on the size of the dataset to minimize the amount of time it takes to reach a trivially sortable state; using 5 sub-lists for a 30-item list got this particular dataset there in one iteration.\n\u003C/p\u003E\n\u003Cp\u003E\nIt feels like this could be a very fast sorting algorithm for many real-world datasets. Unfortunately I don\u0027t quite have the math skills to figure out how fast it would be. Anybody here have ideas? Is this similar to other sorting algorithms I just don\u0027t know about?\n\u003C/p\u003E\n"},{"CreatedByName":"Kerbart","CreatedById":78303,"CreatedDateTime":"2023-10-05T19:46:41Z","Content":"\n\u003Cp\u003E\nSo... you look at the element value, see where it fals on the curve and then use the corresponding x-value to place it in a new array that uses\u00A0 floats as index values? Wouldn\u0027t you then have to sort \u003Cem\u003Ethat\u003C/em\u003E list as well to get things in the right order? Also, how expensive is it to do the reverse lookup on the curve?\n\u003C/p\u003E\n\u003Cp\u003E\nI\u0027m not saying you\u0027re wrong, but sorting is one field in computer science that has been exhaustively researched. It\u0027s also way beyond my capacity to understand, but my suspicion is that there\u0027s just a step with a \u0022hidden cost\u0022 involved that makes the algorithm in practice not as fast as you think.\n\u003C/p\u003E\n"},{"CreatedByName":"Terwin","CreatedById":139655,"CreatedDateTime":"2023-10-05T20:03:39Z","Content":"\n\u003Cp\u003E\nIt looks like you still have a best case of O(n) a worst case of O(n^2) and an average case of O(n*log(n)), just with different constants applied to those calculations.\n\u003C/p\u003E\n\u003Cp\u003E\nYou may have managed to trade a 2x cost on your best case for a smaller number on your average case(perhaps 0.5x), but the big O would be the same.\n\u003C/p\u003E\n"},{"CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-10-05T21:16:51Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224328347\u0022 data-ipsquote-contentid=\u0022219798\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221696535201\u0022 data-ipsquote-userid=\u002278303\u0022 data-ipsquote-username=\u0022Kerbart\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n27 minutes ago, Kerbart said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nSo... you look at the element value, see where it fals on the curve and then use the corresponding x-value to place it in a new array that uses\u00A0 floats as index values? Wouldn\u0027t you then have to sort \u003Cem\u003Ethat\u003C/em\u003E list as well to get things in the right order?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nFor linear curvesort, the new array doesn\u0027t need floats as index values; they would be ints and it would be the same length as your original list. And you don\u0027t need to sort it; the whole idea is that you are moving each element in your original list, one by one, directly into the index point on the new array that they SHOULD be when the list is sorted. Then you can just read the new array from start to finish to get the sorted list.\n\u003C/p\u003E\n\u003Cp\u003E\nA good real-world example would be if you had the pages to a book (let\u0027s say pages 25-35) and they were all jumbled up, and you wanted to arrange them properly. You could leaf through and sort them page by page (bubble sort), but the easier way to do it would be to eyeball 10 page-sized spaces on your table, put them all in the proper space, then pick them all up in sequence. For example, if the first page was 31, you\u0027d take 31-25 = 6 and put that page in the 6th space on the table. If the second page was 34, you\u0027d take 34-25 = 9 and put that page in the ninth space, and so forth.\n\u003C/p\u003E\n\u003Cp\u003E\nBut of course it\u0027s rare that you need to sort a series of ordinal numbers. Let\u0027s suppose it\u0027s more complicated: you need to put a stack of invoices in order from lowest billing to highest billing. If you\u0027re clever, you might flip through the stack quickly and note that (a) there are 10 total invoices, (b) the lowest invoice amount is $201, and (c) the highest invoice amount is $5,022. The difference between $5,022 and $201 is $4,821, and since there are 10 total invoices, you know that on average there will be $482.10 between individual invoices. Assuming you\u0027re REALLY good at mental math, you decide to divide each value by $482.10 in your head in order to space them out on the table by that amount. The first invoice is for $3,016, and 3016/482.10 = 6.3, which rounds to 6, so you move it to array_table[6]. The next invoice is for $3,715, and 3715/482.10 = 7.7, which rounds to 8, so you move it to array_table[8], and so forth:\n\u003C/p\u003E\n\u003Cp style=\u0022text-align:center;\u0022\u003E\n\u003Cimg alt=\u0022invoice-sort.png\u0022 class=\u0022ipsImage\u0022 data-ratio=\u002242.40\u0022 height=\u0022435\u0022 width=\u00221026\u0022 src=\u0022https://i.postimg.cc/FRVYvsh1/invoice-sort.png\u0022\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nWhen you encountered $2,659, the math told you to go to array_table[6], but when you got there, there was already something there. So you group them together and move on. Same with $4,086 going to array_table[8].\n\u003C/p\u003E\n\u003Cp\u003E\nThen you can simply scoop up the whole list, running across your table, skipping any empty spots, and sub-sorting any stacks you encounter. Nothing further needed.\n\u003C/p\u003E\n\u003Cp\u003E\n(Note that when I say \u0022table\u0022 here I mean a literal table. Probably should have said \u0022desk\u0022 instead.)\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224328347\u0022 data-ipsquote-contentid=\u0022219798\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221696535201\u0022 data-ipsquote-userid=\u002278303\u0022 data-ipsquote-username=\u0022Kerbart\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n27 minutes ago, Kerbart said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nAlso, how expensive is it to do the reverse lookup on the curve?\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nThat\u0027s a good question. The computational cost to define a straight line is negligible, but the computational cost to define a curve is not. I\u0027m not sure where the breaking point is, honestly.\n\u003C/p\u003E\n\u003Cp\u003E\nI wonder if it would be just as efficient (or nearly so) to make the first pass\u00A0\u003Cem\u003Ewithout\u003C/em\u003E\u00A0actually writing the values, merely keeping track of the running average of each sub-space with more than one value in it, and then define a series of straight lines linking those averaged points.\u00A0\n\u003C/p\u003E\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224328350\u0022 data-ipsquote-contentid=\u0022219798\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221696536219\u0022 data-ipsquote-userid=\u0022139655\u0022 data-ipsquote-username=\u0022Terwin\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\n1 hour ago, Terwin said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nIt looks like you still have a best case of O(n) a worst case of O(n^2) and an average case of O(n*log(n)), just with different constants applied to those calculations.\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nI\u00A0\u003Cem\u003Ethink\u003C/em\u003E\u00A0that I may have a worst case of O(n^1.5). Still not as good as a worse-case of O(n*log(n)) but better than O(n^2).\n\u003C/p\u003E\n"},{"CreatedByName":"monophonic","CreatedById":76902,"CreatedDateTime":"2023-10-06T05:58:20Z","Content":"\n\u003Cp\u003E\nA very interesting approach. It feels like your approach would share a lot of traits with radix sort. But the ability to tweak the curve to fit the expected data set might provide an advantage in some uses.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Ca href=\u0022https://en.wikipedia.org/wiki/Radix_sort\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://en.wikipedia.org/wiki/Radix_sort\u003C/a\u003E\n\u003C/p\u003E\n"},{"CreatedByName":"Terwin","CreatedById":139655,"CreatedDateTime":"2023-10-06T17:37:49Z","Content":"\n\u003Cp\u003E\nThe more I think about this sorting method, the more it looks like a hash sort.\n\u003C/p\u003E\n\u003Cp\u003E\nThe linear method might even be a standard hashing function.\n\u003C/p\u003E\n\u003Cp\u003E\nThe only real difference that I can see is that you are sorting into a smaller target array.\n\u003C/p\u003E\n\u003Cp\u003E\nI believe that hash sorts work better when your destination array is large enough to be sparsely populated in order to reduce collisions.\n\u003C/p\u003E\n\u003Cp\u003E\n\u00A0\n\u003C/p\u003E\n"},{"CreatedByName":"sevenperforce","CreatedById":157695,"CreatedDateTime":"2023-10-08T20:19:03Z","Content":"\n\u003Cblockquote class=\u0022ipsQuote\u0022 data-ipsquote=\u0022\u0022 data-ipsquote-contentapp=\u0022forums\u0022 data-ipsquote-contentclass=\u0022forums_Topic\u0022 data-ipsquote-contentcommentid=\u00224328448\u0022 data-ipsquote-contentid=\u0022219798\u0022 data-ipsquote-contenttype=\u0022forums\u0022 data-ipsquote-timestamp=\u00221696571900\u0022 data-ipsquote-userid=\u002276902\u0022 data-ipsquote-username=\u0022monophonic\u0022\u003E\n\u003Cdiv class=\u0022ipsQuote_citation\u0022\u003E\nOn 10/6/2023 at 1:58 AM, monophonic said:\n\u003C/div\u003E\n\u003Cdiv class=\u0022ipsQuote_contents\u0022\u003E\n\u003Cp\u003E\nA very interesting approach. It feels like your approach would share a lot of traits with radix sort. But the ability to tweak the curve to fit the expected data set might provide an advantage in some uses.\n\u003C/p\u003E\n\u003Cp\u003E\n\u003Ca href=\u0022https://en.wikipedia.org/wiki/Radix_sort\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://en.wikipedia.org/wiki/Radix_sort\u003C/a\u003E\n\u003C/p\u003E\n\u003C/div\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003E\nWell, looking at radix sort led me to \u003Ca href=\u0022https://en.wikipedia.org/wiki/Bucket_sort\u0022 rel=\u0022external nofollow\u0022\u003Ethe actual bucket sort\u003C/a\u003E which is essentially what I proposed, at least in the linear curvesort case.\n\u003C/p\u003E\n\u003Cp\u003E\nWhile unsourced, the Wikipedia notes in passing:\n\u003C/p\u003E\n\u003Cp style=\u0022margin-left:40px;\u0022\u003E\n\u003Cem\u003EIf the input distribution is known or can be estimated, buckets can often be chosen which contain constant density (rather than merely having constant size). This allows\u00A0\u003C/em\u003E\u003Cstrong\u003EO(n)\u003C/strong\u003E\u003Cem\u003E average time complexity even without uniformly distributed input.\u003C/em\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nThat seems very close to what I was proposing, where the initial low-\u003Cem\u003Ek\u003C/em\u003E\u00A0bucket sort is used to estimate the distribution. It\u0027s also reminiscent of a note concerning radix sort:\n\u003C/p\u003E\n\u003Cp style=\u0022margin-left:40px;\u0022\u003E\n\u003Cem\u003EComputerized radix sorts had previously been dismissed as impractical because of the perceived need for variable allocation of buckets of unknown size. Seward\u0027s innovation was to use a linear scan to determine the required bucket sizes and offsets beforehand, allowing for a single static allocation of auxiliary memory.\u003C/em\u003E\n\u003C/p\u003E\n\u003Cp\u003E\nSeems quite similar on the whole. Both appear to share a relationship to \u003Ca href=\u0022https://en.wikipedia.org/wiki/Counting_sort\u0022 rel=\u0022external nofollow\u0022\u003Ecounting sort\u003C/a\u003E.\n\u003C/p\u003E\n"}]}