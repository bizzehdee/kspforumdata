{"TopicId":107882,"ForumId":55,"TopicTitle":"SLI graphics","CreatedByName":"Alpkant","CreatedById":143242,"CreatedDateTime":"2015-05-06T13:26:37Z","PageNum":1,"Articles":[{"CreatedByName":"Alpkant","CreatedById":143242,"CreatedDateTime":"2015-05-06T13:26:37Z","Content":"\n\u003Cp\u003EHello I\u0027m using SLI graphic cards and if i\u0027m not wrong KSP not supporting SLI technology cause only 1 card working on KSP.Will KSP support the SLI?\u003C/p\u003E\n"},{"CreatedByName":"michaelsteele3","CreatedById":127113,"CreatedDateTime":"2015-05-06T14:46:45Z","Content":"\n\u003Cp\u003EWhat\u0027s SLI?\u003C/p\u003E\n"},{"CreatedByName":"KrazyKrl","CreatedById":65849,"CreatedDateTime":"2015-05-06T15:05:54Z","Content":"\n\u003Cp\u003EKSP is mostly bound to your CPU speed; as KSP isn\u0027t that graphically intensive. I believe that KSP will run with an SLI setup, but SLI will not increase your FPS. The biggest FPS gain in KSP would be a decent multi-core processor, with quick single cores (most of the FPS drop in KSP comes from the single-threaded physics calculations; and other game overhead that is difficult to multithread.)\u003C/p\u003E\u003Cp\u003ESLI/Crossfire is a method to pair two video cards to increase your FPS.\u003C/p\u003E\n"},{"CreatedByName":"Patrick Kerbivan","CreatedById":114804,"CreatedDateTime":"2015-05-06T17:06:05Z","Content":"\n\u003Cp\u003EI can confirm that KSP doesn\u0027t use SLI; when I look at my video cards\u0027 load monitor while running KSP there\u0027s only load on one card. This is while using the optimized profile for KSP as applied by the NVIDIA GeForce Experience application. Someone with a bit more advanced knowledge might know of some trick for forcing SLI, but I agree with KrazyKrl that KSP\u0027s performance is generally going to be limited by your CPU performance rather than your video card.\u003C/p\u003E\n"},{"CreatedByName":"Morashtak","CreatedById":88400,"CreatedDateTime":"2015-08-09T14:57:35Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Patrick Kerbivan\u0022 data-cite=\u0022Patrick Kerbivan\u0022\u003E\u003Cdiv\u003ESomeone with a bit more advanced knowledge might know of some trick for forcing SLI...\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EThe trick is to use FAFR2. \u003C/p\u003E\u003Cp\u003EFirst start by using the \u00223D Settings\u0022 drop down menu and turn on \u0022Show SLI Visual Indicator\u0022. Then fire up KSP and confirm that the indicator shows no hint of SLI usage (it shouldn\u0027t with typical settings).\u003C/p\u003E\u003Cp\u003EThen in your NVIDIA Control panel go to \u0022Manage 3D Settings\u0022, select KSP as the program to customize and try the following changes to force SLI;\u003C/p\u003E\u003Cp\u003E\u003C/p\u003E\u003Cul\u003E\u003Cli\u003E\u003Cspan style=\u0022text-decoration:underline;\u0022\u003E\u003Cspan style=\u0022font-size:12px;\u0022\u003E\u003Cspan style=\u0022font-family:\u0027Courier New\u0027;\u0022\u003EFeature\u003C/span\u003E\u003C/span\u003E\u003C/span\u003E....................................\u003Cspan style=\u0022text-decoration:underline;\u0022\u003ESettings\u003C/span\u003E\u003Cbr\u003E\u003C/li\u003E\u003Cli\u003EMulti-display/mixed-GPU acceleration.......\u003Cstrong\u003EMulti display performance mode\u003C/strong\u003E \u003Cspan style=\u0022font-size:10px;\u0022\u003E\u003Cem\u003E(choose this even if you only have one monitor)\u003C/em\u003E\u003C/span\u003E\u003Cbr\u003E\u003C/li\u003E\u003Cli\u003ESLI rendering mode.........................\u003Cstrong\u003EForce alternate frame rendering 2\u003C/strong\u003E\u003Cbr\u003E\u003C/li\u003E\u003Cli\u003EThreaded optimization......................\u003Cstrong\u003EOn\u003C/strong\u003E\u003C/li\u003E\u003C/ul\u003E\u003Cp\u003E\u003C/p\u003E\u003Cp\u003EFire up KSP and check that the SLI indicator is showing some SLI usage (it won\u0027t show much at all, sometimes virtually none which confirms it is advisable to upgrade your CPU vs your GPU for better performance). Turn off the SLI indicator and stop/start KSP to stop showing the indicator in game.\u003C/p\u003E\u003Cp\u003EBTW, the FPS increase is just about negligible on my rig tho\u0027 your mileage may vary.\u003C/p\u003E\u003Cp\u003EHope this helps.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-08-09T15:00:04Z\u0022 title=\u002208/09/2015 03:00  PM\u0022 data-short=\u00228 yr\u0022\u003EAugust 9, 2015\u003C/time\u003E by Morashtak\u003C/strong\u003E\n\u003Cbr\u003E\u0026quot;It passed the spell check\u0026quot;\n\u003C/span\u003E\n"},{"CreatedByName":"pincushionman","CreatedById":98495,"CreatedDateTime":"2015-08-09T15:11:28Z","Content":"\n\u003Cp\u003EAs others have said: FPS effect meager at best. Also, this would be a Unity feature, not KSP in particular.\u003C/p\u003E\n"},{"CreatedByName":"steve_v","CreatedById":69272,"CreatedDateTime":"2015-08-09T22:36:03Z","Content":"\n\u003Cp\u003EBetter idea: use second GPU as dedicated PhysX processor.\u003C/p\u003E\u003Cp\u003EOh, wait, KSP can\u0027t use GPU accelerated PhysX. What a shame.\u003C/p\u003E\u003Cp\u003EI guess we\u0027re stuck with lousy performance and that second GPU will just sit there twiddling its thumbs while your CPU melts.\u003C/p\u003E\u003Cp\u003EKSP, please utilise available hardware properly. I am tired of awful performance on high-end machines.\u003C/p\u003E\n"},{"CreatedByName":"Canberra Gaming","CreatedById":147308,"CreatedDateTime":"2015-08-10T00:22:29Z","Content":"\n\u003Cp\u003EHopefully when KSP moves to Unity 5 (1.1 I\u0027ve been hearing? needs clarification) it will fix a lot of the hardware problems. I\u0027m on a decently high end machine and I\u0027m still hitting a lag wall at about 300ish parts. My Specs are AMD Athlon X4 860K, GTX 960 4GB, and 8GB of RAM but I feel the pain. Again hopefully Unity 5 fixes these lag walls.\u003C/p\u003E\n"},{"CreatedByName":"steve_v","CreatedById":69272,"CreatedDateTime":"2015-08-10T00:53:54Z","Content":"\n\u003Cp\u003EMeh. As I understand it, U5 implements PhysX 3.3... which supports GPU acceleration for Nvidia cards on Windows. But it\u0027s only used for cloth simulation, which is no good to us anyway.\u003C/p\u003E\u003Cp\u003EPerformance across the board should improve, but it looks like it\u0027s still unable to utilise more than one thread for a group of connected rigidbodies... which means one thread per craft. Which means performance will still suck, but maybe not quite so hard.\u003C/p\u003E\u003Cp\u003EThere are cross-platform GPU accelerated physics engines available (bullet springs to mind) but \u003Cem\u003Esomeone\u003C/em\u003E appears to be intent on using only stock Unity functionality. \u003C/p\u003E\u003Cp\u003EPersonally, I\u0027m not expecting any huge performance boost. But we shall see.\u003C/p\u003E\n"}]}