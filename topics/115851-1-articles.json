{"TopicId":115851,"ForumId":44,"TopicTitle":"[Question] Why are spacecraft images always black and white?","CreatedByName":"quasarrgames","CreatedById":91692,"CreatedDateTime":"2015-07-14T01:28:30Z","PageNum":1,"Articles":[{"CreatedByName":"quasarrgames","CreatedById":91692,"CreatedDateTime":"2015-07-14T01:28:30Z","Content":"\n\u003Cp\u003EThat\u0027s the question! Black-and-white cameras are, like, very old now. Why do we still use them for missions that were launched only a decade ago (like rosetta and new horizons)? Also, why are the colour images always \u0022False\u0022 colour? Forgive me for being so utterly uninformed. I\u0027m sure there\u0027s a big difference between a space camera and a regular colourful digital camera, but i don\u0027t know what. \u003Cimg src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif\u0022 alt=\u0022:)\u0022\u003E\u003C/p\u003E\n"},{"CreatedByName":"windows_x_seven","CreatedById":112926,"CreatedDateTime":"2015-07-14T01:36:23Z","Content":"\n\u003Cp\u003EBlack and white images are easier to make and transmit, compared to true-color images.\u003C/p\u003E\n"},{"CreatedByName":"StrandedonEarth","CreatedById":89439,"CreatedDateTime":"2015-07-14T01:58:57Z","Content":"\n\u003Cp\u003EThey use filters to take \u0022black and white\u0022 pictures at different wavelengths, then combine them on the ground. Then they can assign whatever \u0022false\u0022 colors they want to the data.\u003C/p\u003E\n"},{"CreatedByName":"lajoswinkler","CreatedById":79159,"CreatedDateTime":"2015-07-14T02:05:12Z","Content":"\n\u003Cp\u003EBlack and white cameras are not old and obsolete. If I ever become loaded with money, I\u0027m gonna buy a monochrome camera.\u003C/p\u003E\u003Cp\u003EIt\u0027s because a monochrome camera gives much more sensor density. If you have a regular digital color camera, this is how its sensor works.\u003C/p\u003E\u003Cp\u003E\u003Cimg src=\u0022http://nofilmschool.com/sites/default/files/styles/article_wide/public/uploads/2012/11/Bayer-Pattern-Example.jpg?itok=MpSx6A-w\u0022 alt=\u0022Bayer-Pattern-Example.jpg?itok=MpSx6A-w\u0022\u003E\u003C/p\u003E\u003Cp\u003EBasically you have a third of your sensor used for every color channel. With monochrome, you can have very dense packed image data, and if you want color images, you put a filter over the camera that absorbs everything but the band you want (for example you put a 523 nm transmissive, and rest of the spectrum restrictive) and you get that channel.\u003C/p\u003E\u003Cp\u003EWith three channels like typical primary red, primary green and primary blue, a computer can synthetize a color RGB image and display it over a monitor for us to see.\u003C/p\u003E\u003Cp\u003ESometimes probes don\u0027t have R, G and B filters. They might have near infrared band pass filter, orange filter and soft ultraviolet one. Those can also be fed to the software, telling it to synthetize a RGB image using those channels. The resulting image does not have true colors. It has approximation of true color which is usually good enough.\u003C/p\u003E\u003Cp\u003EI\u0027ve done lots of such experiments over the years. Just recently I did two images using several filters for a RGB model.\u003C/p\u003E\u003Cp\u003EThis one used, if I recall correctly, near infrared, green and very deep and quite narrow blue band.\u003C/p\u003E\u003Cp\u003E\u003Cimg src=\u0022http://s16.postimg.org/qdh2jce7p/rgbsynth1.png\u0022 alt=\u0022rgbsynth1.png\u0022\u003E\u003C/p\u003E\u003Cp\u003EThis one used something close to primary colors. Still a bit off.\u003C/p\u003E\u003Cp\u003E\u003Cimg src=\u0022http://s3.postimg.org/i6r18lbsz/RGB_test1.png\u0022 alt=\u0022RGB_test1.png\u0022\u003E\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-07-14T02:07:37Z\u0022 title=\u002207/14/2015 02:07  AM\u0022 data-short=\u00228 yr\u0022\u003EJuly 14, 2015\u003C/time\u003E by lajoswinkler\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"YNM","CreatedById":98447,"CreatedDateTime":"2015-07-14T03:32:17Z","Content":"\n\u003Cp\u003EBecause any sensor CAN\u0027T differ between red, green, and blue photons. Only filters ensure that the light passed is only of either green, blue, red, or sometimes a range (ex. green to blue, red to green etc). Knowing which photons were recorded (via filters), you can color them (or, make their value into the RGB space). Then you get color images. This are true for your phone, DSLR, or even some reconnaisance sat camera.\u003C/p\u003E\u003Cp\u003EThree ways to deal with it :\u003C/p\u003E\u003Cp\u003E1. Take three consecutive images, one for each color. Of course a problem if you want to capture a very specific moment, like an explosion or someone jumping into a pool.\u003C/p\u003E\u003Cp\u003E2. Use three separate sensor, like lajoswinkler said. Suffers from less intensity (you need to divide it into three sensor, remember ?) and more costly, often offset by using CMOS sensors (astronomical observation prefers CCD for their sensitivity).\u003C/p\u003E\u003Cp\u003E3. Literally put a filter in front of each sensor pixel. Results in lower resolution (need to merge three sensor pixel into one image pixel) or the need for larger, more expensive sensor, which are often offset by using CMOS sensors.\u003C/p\u003E\u003Cp\u003E\u003Cimg src=\u0022https://upload.wikimedia.org/wikipedia/commons/3/37/Bayer_pattern_on_sensor.svg\u0022 alt=\u0022Bayer_pattern_on_sensor.svg\u0022\u003E\u003C/p\u003E\u003Cp\u003EThat\u0027s why scientific, astronomical observations prefer the first... It\u0027s not like the object is going to dissappear anyway, so a few more images won\u0027t hurt.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-07-14T03:38:06Z\u0022 title=\u002207/14/2015 03:38  AM\u0022 data-short=\u00228 yr\u0022\u003EJuly 14, 2015\u003C/time\u003E by YNM\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"PB666","CreatedById":107380,"CreatedDateTime":"2015-07-14T03:59:19Z","Content":"\n\u003Cp\u003EB\u0026amp;W is more gives better shading, Color gives better composition. You can extract point and shoot color with a spectrograph, which is more desirable anyway since digital cameras only pick up what is filtered and a spectrograph records the spectrum, you can\u0027t really verify flourescence with digitized color either, but you can with a spectrograph. Its more efficient to send the shading and spectrum information separately. \u003Ca href=\u0022https://en.wikipedia.org/wiki/Space_Telescope_Imaging_Spectrograph\u0022 rel=\u0022external nofollow\u0022\u003Ehttps://en.wikipedia.org/wiki/Space_Telescope_Imaging_Spectrograph\u003C/a\u003E (note image to the right)\u003C/p\u003E\u003Cp\u003EBTW the 67P comet appears to be largely grey tones, as with Ceres. I would rather have the spectrographic information since that informs on chemical composition.\u003C/p\u003E\n"},{"CreatedByName":"UmbralRaptor","CreatedById":2767,"CreatedDateTime":"2015-07-14T04:06:03Z","Content":"\n\u003Cp\u003ESince others have covered the basics, I just want to point out that the filter wheels carried by spacecraft cameras often result in *more* colors than the standard RGB. (Though chances are you\u0027ll only see a few at a time)\u003C/p\u003E\u003Cp\u003EIn the case of New Horizons, it\u0027s worth noting that we already have low resolution colors, but have not recieved the vast majority of the data. That will be arriving in lossily compressed form in coming weeks-months, with the full images, etc taking a year or more.\u003C/p\u003E\n"},{"CreatedByName":"PB666","CreatedById":107380,"CreatedDateTime":"2015-07-14T12:21:21Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022lajoswinkler\u0022 data-cite=\u0022lajoswinkler\u0022\u003E\u003Cdiv\u003EThey might have near infrared band pass filter, orange filter and soft ultraviolet one.\u003C/div\u003E\u003C/blockquote\u003E Depending on how the camera is lensed the best instruments use separate cameras for non-visible light, lensing distorts the light because different wavelength bend differently in a lens. Of course modern computers can transform filtered images correcting for the lensing effect. This is the reason that large telescopes use mirrors instead of lenses to focus the image. \u003Cp\u003E-------------------\u003C/p\u003E\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022\u003E\u003Cdiv\u003EBecause any sensor CAN\u0027T differ between red, green, and blue photons.\u003C/div\u003E\u003C/blockquote\u003E Photoelectric effect, sensors can do a binary sorting of photons, they either have sufficient energy to trigger the effect or not. Therefore sensors with thresholds in the visible spectrum can. \u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022\u003E\u003Cdiv\u003EThe photons of a light beam have a characteristic energy proportional to the frequency of the light. In the photoemission process, if an electron within some material absorbs the energy of one photon and acquires more energy than the \u003Ca href=\u0022https://en.wikipedia.org/wiki/Work_function\u0022 rel=\u0022external nofollow\u0022\u003Ework function\u003C/a\u003E (the electron binding energy) of the material, it is ejected. If the photon energy is too low, the electron is unable to escape the material.-WP-Photoelectric effect\u003C/div\u003E\u003C/blockquote\u003E\n"},{"CreatedByName":"YNM","CreatedById":98447,"CreatedDateTime":"2015-07-14T17:42:42Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022PB666\u0022 data-cite=\u0022PB666\u0022\u003E\u003Cdiv\u003E\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022YNM\u0022 data-cite=\u0022YNM\u0022\u003E\u003Cdiv\u003EBecause any sensor CAN\u0027T differ between red, green, and blue photons.\u003C/div\u003E\u003C/blockquote\u003EPhotoelectric effect, sensors can do a binary sorting of photons, they either have sufficient energy to trigger the effect or not. Therefore sensors with thresholds in the visible spectrum can.\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022\u003E\u003Cdiv\u003EThe photons of a light beam have a characteristic energy proportional to the frequency of the light. In the photoemission process, if an electron within some material absorbs the energy of one photon and acquires more energy than the work function (the electron binding energy) of the material, it is ejected. If the photon energy is too low, the electron is unable to escape the material.-WP-Photoelectric effect\u003C/div\u003E\u003C/blockquote\u003E\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EBut then... How would getting \u003Ca href=\u0022https://en.wikipedia.org/wiki/Color_index\u0022 rel=\u0022external nofollow\u0022\u003Ecolor index\u003C/a\u003E with CCDs only by the count works ? AFAIK if there\u0027s a difference it\u0027s too small (and so, somewhat neglected). After all, CCD materials are photosensitive semiconductors, they\u0027re not quite the photoelectric \u003Ca href=\u0022https://en.wikipedia.org/wiki/Excited_state\u0022 rel=\u0022external nofollow\u0022\u003Eas in this case\u003C/a\u003E - any photons would do (else you\u0027re not seeing STIS with 200 - 1030 nm wavelength range).\u003C/p\u003E\n"},{"CreatedByName":"ComradeWolfe","CreatedById":142254,"CreatedDateTime":"2015-07-15T12:21:31Z","Content":"\n\u003Cp\u003EIt\u0027s mostly because photo-shopping out motherships in color is a pain.\u003C/p\u003E\n"},{"CreatedByName":"Kryten","CreatedById":348,"CreatedDateTime":"2015-07-15T13:22:57Z","Content":"\n\u003Cp\u003EMany probes have imaging spectrographs, where you get readings for usually a few hundred wavelength channels per pixel. You can easily get true-colour images from that, but they usually aren\u0027t in the visual range and tend to have much lower resolution than the CCD sensors.\u003C/p\u003E\n"}]}