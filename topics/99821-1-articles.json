{"TopicId":99821,"ForumId":44,"TopicTitle":"The last invention we\u0026#039;ll ever make.","CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-16T12:35:27Z","PageNum":1,"Articles":[{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-16T12:35:27Z","Content":"\n\u003Cp\u003E\u003Cspan style=\u0022font-size:14px;\u0022\u003EArtificial Superintelligence.\u003C/span\u003E\u003C/p\u003E\u003Cp\u003EI didn\u0027t want to grab the forum search to find an old thread on this subject to necro - largely because I\u0027m not here to give my opinion on this matter. Much rather, I\u0027d like to link you to a pair of articles on the topic.\u003C/p\u003E\u003Cp\u003EPart 1: \u003Ca href=\u0022http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\u0022 rel=\u0022external nofollow\u0022\u003Ehttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\u003C/a\u003E\u003C/p\u003E\u003Cp\u003EPart 2: \u003Ca href=\u0022http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html\u0022 rel=\u0022external nofollow\u0022\u003Ehttp://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html\u003C/a\u003E\u003C/p\u003E\u003Cp\u003EThese articles are very, very long. You will need hours to read and digest them.\u003C/p\u003E\u003Cp\u003EThe reason it\u0027s still worth the while to take the plunge is because the author goes to these great lengths simply to view the issue from all possible sides. It is as much of a concise, all-encompassing \u0022Hitchhiker\u0027s Guide to God in a Box\u0022 as you\u0027re going to find anywhere on the net, in my opinion. The good, the bad, the weird, the philosophical - it\u0027s all there. He starts first by outlining where we are, how we are potentially going to take this step, and what will be happening along the way. That\u0027s part 1. Then, he looks into the possible outcomes in part 2, an exercise which comes to a fairly sobering, binary conclusion: either permanent, perfect immortality, or permanent, perfect extinction.\u003C/p\u003E\u003Cp\u003EHe also introduces the reader to who the leading people in the field of AI research are, what their hypotheses predict, and what their opinions are on the how and the when and the following what.\u003C/p\u003E\u003Cp\u003EBut most importantly - and interestingly - he spends an inordinate amount of effort on giving the reader even the slightest inkling as to what \u003Cspan style=\u0022text-decoration:underline;\u0022\u003Esuper\u003C/span\u003Eintelligence actually, truly means. This is something I often find sidelined in discussions about advanced AI. Everyone is always talking about the inherent dangers of that concept, but we do this with a certain... detachedness. We\u0027re quick to move on to the \u0022what can we do\u0022 phase, and we base our discussions on how we would deal with a computer able to match wits with our smartest while at the same time thinking a million times faster. Turns out though? That\u0027s not what we\u0027re going to see. Oh yes, it will be an unmeasurable amount faster than us. But the real \u0027super\u0027 in superintelligence will be the way that it transcends what we are physically able to conceive or understand in such a way that we should consider ourselves lucky if we can even still communicate with it.\u003C/p\u003E\u003Cp\u003E\u0022(...) let\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2s very concretely state once and for all that there is no way to know what [an] ASI will do (...) Anyone who pretends otherwise doesn\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2t understand what superintelligence means.\u0022 - Part 2 of the article.\u003C/p\u003E\u003Cp\u003EOh, and by the way: since you\u0027re alive right now and are reading this - according to the best guesses of the most knowledgeable people in this field of science, the chance of this happening \u003Cem\u003Ein your lifetime\u003C/em\u003E are greater than fifty percent.\u003C/p\u003E\u003Cp\u003EMy mind is disturbed and quite thoroughly blown for now. Still, I hope that it\u0027ll be just as interesting a read to you than it was for me, regardless of the conclusions you draw from it.\u003C/p\u003E\n"},{"CreatedByName":"Flixxbeatz","CreatedById":29991,"CreatedDateTime":"2015-02-16T12:36:30Z","Content":"\n\u003Cp\u003Einb4 terminator apocalypse\u003C/p\u003E\n"},{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-16T12:49:42Z","Content":"\n\u003Cp\u003EIt\u0027s not certain it\u0027ll be a Terminator apocalypse. It might be a deathless, transcended Utopia of leisure and fulfillment... \u003Cimg src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif\u0022 alt=\u0022:)\u0022\u003E\u003C/p\u003E\u003Cp\u003EThe interesting part is whether or not we have any sort of control over which direction it tips.\u003C/p\u003E\n"},{"CreatedByName":"cicatrix","CreatedById":120831,"CreatedDateTime":"2015-02-16T12:59:06Z","Content":"\n\u003Cp\u003EA near-vertical progress is theoretically possible once we run an AI of sorts, but still, our hydrocarbons might end sooner than that (before alternatives are introduced), and if this happens we would revert to steampunk (or hunting-gathering even, depending on how bad the humanity would take energy starvation).\u003C/p\u003E\n"},{"CreatedByName":"lajoswinkler","CreatedById":79159,"CreatedDateTime":"2015-02-16T13:08:44Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022cicatrix\u0022 data-cite=\u0022cicatrix\u0022\u003E\u003Cdiv\u003EA near-vertical progress is theoretically possible once we run an AI of sorts, but still, our hydrocarbons might end sooner than that (before alternatives are introduced), and if this happens we would revert to steampunk (or hunting-gathering even, depending on how bad the humanity would take energy starvation).\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EThere is no need to go to any steampunk scenario. \u0022The problem\u0022 with oil is not energy generation. We generate energy mainly using coal, uranium and gravitational potential energy of water. Oil is a \u003Cstrong\u003Epetrochemical resource\u003C/strong\u003E we use to make most of our stuff. Granted, we could make most of our stuff using air, water and salt, but it would be very expensive and there are also some things you just can\u0027t synthesize in a chemical reactor.\u003C/p\u003E\n"},{"CreatedByName":"Xaiier","CreatedById":22833,"CreatedDateTime":"2015-02-16T13:32:18Z","Content":"\n\u003Cp\u003ESomeone linked those articles in the IRC chat a few weeks ago. I found them interesting, but they are largely just a rehashing of ideas that have existed for the last ~60 years or more. Science fiction and pop-culture have been playing with the idea for a long time now, and a lot of very smart people have tried to predict our impending doom or whatever it may be. The simple fact is that superintelligent AI is not the thing we need to worry about.\u003C/p\u003E\u003Cp\u003EWhat we do need to worry about is a super-advanced, technically non-intelligent system which has a defined task or goal and the means to accomplish it. Such a thing could easily happen in the timeframe these articles suggest, and it will assuredly happen before a true artificial intelligence exists. In this regard, we\u0027re looking more towards something like \u0022Skynet\u0022 in that it has one task (the protection of earth) and the means to do so (robot army \u0026amp; control of all computers). Such a thing would be significantly more dangerous than a superintelligent AI because it would have all the speed and efficiency of one, without any of the superior intellect which allows for the chance of it being benevolent (unless of course, that is its task), or acting of it\u0027s own accord. Things like this already exist in military applications, such as drones which can autonomously choose targets and engage without human intervention.\u003C/p\u003E\u003Cp\u003EOne way to imagine such a thing is through Keith Laumer\u0027s science fiction novels on Bolos (\u003Ca href=\u0022https://en.wikipedia.org/wiki/Bolo_%28tank%29\u0022 rel=\u0022external nofollow\u0022\u003Ehttp://en.wikipedia.org/wiki/Bolo_%28tank%29\u003C/a\u003E). Initially beginning as computer assisted tanks, they became more and more powerful over time until they were nearly unstoppable war machines capable of dealing with any threat, completely autonomously. Such a thing, minus the artificial intelligence, is well within the realm of possibility of being created in this century. While they may not be the extinction level AI\u0027s predicted in your articles, even a single mistake with one would have disastrous consequences.\u003C/p\u003E\u003Cp\u003EAs such, I don\u0027t think our real issue is creating a computer which outperforms us in every way. All we have to do is create one which outperforms us in one way, and then give it too much power.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-02-16T13:36:19Z\u0022 title=\u002202/16/2015 01:36  PM\u0022 data-short=\u00229 yr\u0022\u003EFebruary 16, 2015\u003C/time\u003E by Xaiier\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-16T13:46:11Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Xaiier\u0022 data-cite=\u0022Xaiier\u0022\u003E\u003Cdiv\u003EWhat we do need to worry about is a super-advanced, technically non-intelligent system which has a defined task or goal and the means to accomplish it.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EYeah, part 2 also goes into that. The thing you need to keep in mind is to not correlate or confuse the three factors of intelligence, sentience, and morality.\u003C/p\u003E\u003Cp\u003ESkynet, as envisioned in its fictional form, is not sentient and has no moral compass. But it is an aftificial superintelligence. Not of transcendent status, admittedly, but well, it would be impossible for a human to imagine (and thus portray in film) such a thing.\u003C/p\u003E\u003Cp\u003EThe article links to a thought experiment that seeks to prove that a machine can never achieve sentience in the way humans have, fully independent of its degree of intelligence. But then again, this is a topic that\u0027s hotly debated, even without counting the fact that you\u0027d be dealing with an incomprehensibly advanced \u0022thought process\u0022/algorithm that might very well figure out a way if it suits its goals.\u003C/p\u003E\u003Cp\u003EMorality appears as one of the potential avenues through which we might seek to control the boundless and ruthless ambition of a superintelligent machine with a goal. But since not even all humans can agree on a common moral standard, this one might be trickier than we think...\u003C/p\u003E\n"},{"CreatedByName":"FanaticalFighter","CreatedById":77314,"CreatedDateTime":"2015-02-16T13:54:51Z","Content":"\n\u003Cp\u003EOooh, more people actually read waitbutwhy. I\u0027m \u003Cem\u003Enot\u003C/em\u003E alone, then yay!\u003C/p\u003E\u003Cp\u003EI\u0027m one of those people who side more on the optimistic side than the pessimistic side when it comes to the AI revolution, mostly because when we actually get the AGI, we can use said AGI for research of all kinds, and using technology to research more technology on its own is the kind of technology-ception that can take us to the next revolution (similiar to the difference made in our lives by computers, at the very least, but on a much shorter timescale.) Of course, AGI\u0027s which can research stuff on their own can also research ASI\u0027s, and I have no way of knowing what\u0027ll happen then, but I hope that this ASI will also be helpful.\u003C/p\u003E\n"},{"CreatedByName":"Skyler4856","CreatedById":76452,"CreatedDateTime":"2015-02-16T14:03:41Z","Content":"\n\u003Cp\u003EMan is terribly paranoid, and I doubt he shall prevent some sort of kill switch from being placed around, or in, autonomous robots. This switch could be a limited EMP, a small explosive, etc.\u003C/p\u003E\n"},{"CreatedByName":"lajoswinkler","CreatedById":79159,"CreatedDateTime":"2015-02-16T14:11:04Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Skyler4856\u0022 data-cite=\u0022Skyler4856\u0022\u003E\u003Cdiv\u003EMan is terribly paranoid, and I doubt he shall prevent some sort of kill switch from being placed around, or in, autonomous robots. This switch could be a limited EMP, a small explosive, etc.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EBut man is also terribly stupid, even more so than paranoid. Let us not forget that.\u003C/p\u003E\n"},{"CreatedByName":"Xaiier","CreatedById":22833,"CreatedDateTime":"2015-02-16T14:14:20Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Streetwind\u0022 data-cite=\u0022Streetwind\u0022\u003E\u003Cdiv\u003EYeah, part 2 also goes into that. The thing you need to keep in mind is to not correlate or confuse the three factors of intelligence, sentience, and morality.\u003Cp\u003ESkynet, as envisioned in its fictional form, is not sentient and has no moral compass. But it is an aftificial superintelligence. Not of transcendent status, admittedly, but well, it would be impossible for a human to imagine (and thus portray in film) such a thing.\u003C/p\u003E\u003Cp\u003EThe article links to a thought experiment that seeks to prove that a machine can never achieve sentience in the way humans have, fully independent of its degree of intelligence. But then again, this is a topic that\u0027s hotly debated, even without counting the fact that you\u0027d be dealing with an incomprehensibly advanced \u0022thought process\u0022/algorithm that might very well figure out a way if it suits its goals.\u003C/p\u003E\u003Cp\u003EMorality appears as one of the potential avenues through which we might seek to control the boundless and ruthless ambition of a superintelligent machine with a goal. But since not even all humans can agree on a common moral standard, this one might be trickier than we think...\u003C/p\u003E\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EWhat I meant to point out is that such a thing is on the lower bounds of what one might consider a superintelligent AI, as it really only has the intelligence aspect, without any sentience or morality. In fact, such a thing could barely be classed as intelligent, the simplest form is basically just an advanced management and coordination program, which is only superior due to it\u0027s speed at comprehending and processing a large amount of data.\u003C/p\u003E\n"},{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-16T14:36:59Z","Content":"\n\u003Cp\u003EYou might see it that way, that to be called intelligent a machine would require sentience. Scientists don\u0027t share your definition, though. A nonsentient algorithm can be intelligent, i.e. it can make all the right decisions at all the right moments for all the right reasons (where \u0022right\u0022 is defined as furthering its goal). Whether it truly comprehends its actions is, for the purpose of considering its ability to perform them, irrelevant. Neither sentience nor morality is a prerequisite for intelligence; all three are separate concepts with very specific, scientifc definitions.\u003C/p\u003E\u003Cp\u003EDon\u0027t fall into the trap of thinking \u0022it will merely be much faster than us\u0022 simply based on semantics. It is expected that such a machine will exceed human intelligence on a \u003Cem\u003Equalitative\u003C/em\u003E level - that is, if you put a non-sentient humand and a non-sentient ASI to the same task, the ASI would win every time.\u003C/p\u003E\u003Cp\u003EIf humans can use their sentience to their advantage over a non-sentient ASI... now that is an interesting question. But perhaps ultimately irrelevant, for an ASI probably is able to rewrite itself on the fly during the time a human speaks a sentence, ensuring it is always perfectly set up to converse with the human \u003Cem\u003Eas if\u003C/em\u003E it were sentient (but without actually being so). With the right tools, it\u0027s perfectly possible to fake understanding - see Chinese Room Experiment.\u003C/p\u003E\n"},{"CreatedByName":"cicatrix","CreatedById":120831,"CreatedDateTime":"2015-02-16T14:44:12Z","Content":"\n\u003Cp\u003EThis article uncovers only a part of the \u0027problem\u0027 though I do not see any problem at all. In order to keep up with its own creation (Super AI) man would have to change himself. I\u0027m speaking about human/machine integration. Now it makes its baby steps in the form of bionic limbs, artificial organs, implanted microchips, etc. It will go further than that. Eye implants, robotic parts, artificial memory - all of that is probably going to appear during our lifetime. Immortality goes along - part by part man would modify himself to a point that only brain will remain and it probably be a point down that road that even brain could be replaced with some other medium for our consciousness. Will humans have the right to call them humans from that point? It may very well be that Super AI won\u0027t ever appear since humans themselves would take this huge leap and the question if your AI is artificial or not would become impolite or even rude.\u003C/p\u003E\u003Cp\u003ESo, I don\u0027t think it would be Super AI that would change our lives, it will be us. We would breed with machines and become one.\u003C/p\u003E\n"},{"CreatedByName":"PB666","CreatedById":107380,"CreatedDateTime":"2015-02-16T14:44:19Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022FanaticalFighter\u0022 data-cite=\u0022FanaticalFighter\u0022\u003E\u003Cdiv\u003EOooh, more people actually read waitbutwhy. I\u0027m \u003Cem\u003Enot\u003C/em\u003E alone, then yay!\u003Cp\u003EI\u0027m one of those people who side more on the optimistic side than the pessimistic side when it comes to the AI revolution, mostly because when we actually get the AGI, we can use said AGI for research of all kinds, and using technology to research more technology on its own is the kind of technology-ception that can take us to the next revolution (similiar to the difference made in our lives by computers, at the very least, but on a much shorter timescale.) Of course, AGI\u0027s which can research stuff on their own can also research ASI\u0027s, and I have no way of knowing what\u0027ll happen then, but I hope that this ASI will also be helpful.\u003C/p\u003E\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EIts really difficult to drive an evolutionarily evolved population with 8 billion representation into immediate extinction. Computers \u0027species\u0027 OTOH have a lifespan of typically of a few years. A smart phone has the typical lifespan of what 18 months. This PC is 6 years and pushing it. There are refrigerators built in the 1940s that are still running in places like Cuba. As technology advances it typically gets more fragile. If you take a look at the computers that are running the high dollar technology, they are 80386s not the latest computers to hit the market. \u003C/p\u003E\u003Cp\u003E The argument here is that your I-phone 6 will last longer than my superfat motorola flip phone c.2009, but since you trade it in or flush it every 6 to 12 months to get a new one you just assume it will. Those machines will have to have some sort of control systems somewhere that can survive the next big disaster (just look up TMC TSA June 5, 2001). The basic problem is that we assume that humans can build and anticipate the current and future survival needs of machines, but humans are relatively reactionary when it comes to long term risk. In fact, we sometimes ignore it eyes wide open. 8 billion humans will survive somehow somewhere, but the machines of today are doomed to extinction most sooner than later. It becomes such a problem that it may be difficult to recover historical information stored in the 1960s to 1990s. The machines of today are more information-ally fragile than the machines of the past and so replace all this with something even more fragile and vulnerable called a cloud. Think how fast a devious person could confound all the information in clouds dumbfounding the self-learning computers of the world.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-02-16T14:47:40Z\u0022 title=\u002202/16/2015 02:47  PM\u0022 data-short=\u00229 yr\u0022\u003EFebruary 16, 2015\u003C/time\u003E by PB666\u003C/strong\u003E\n\u003Cbr\u003Esp gr\n\u003C/span\u003E\n"},{"CreatedByName":"YNM","CreatedById":98447,"CreatedDateTime":"2015-02-16T15:00:49Z","Content":"\n\u003Cp\u003EI... guess the problem is physical. (I mean, nanobots that move single atoms of probably any kind ? I\u0027d like to be enlightened on this thing, seriously. Not to mention a few other problems.)\u003C/p\u003E\u003Cp\u003EI believe that machines need humans to develop wherever they would at any rate. To go ASI it still need humans. Proceeding within ASI, as the article implies it\u0027ll be better by interaction, actually still need stimuli-respond system familiar with biologist. I\u0027m not sure but as they learned off our uses and our interactions then that means they need us. Other things isn\u0027t a very good option.\u003C/p\u003E\u003Cp\u003EBut what I believe most, is that there\u0027s nothing very certain. Only time\u0027s progress will tell how the future will be - after all, the reason why we can interact is because time move forward, not any other way.\u003C/p\u003E\n"},{"CreatedByName":"-Velocity-","CreatedById":74097,"CreatedDateTime":"2015-02-16T19:05:11Z","Content":"\n\u003Cp\u003EI find the idea that we won\u0027t be able to communicate with superintelligences a little silly. They will be able to communicate with us any ideas that humans can understand. There may be some things it understands and thinks, however, that it simply cannot make us understand. That\u0027s OK. It\u0027s just that to us, some of its actions may seem indecipherable.\u003C/p\u003E\u003Cp\u003ETake dogs for example. We are vastly smarter than them, but we can still communicate at rudimentary levels. I can tell my dog to \u0022go get a toy\u0022 and she\u0027ll return with something to throw (if she remembers where she left her current toy, that is). She\u0027ll also start barking and looking at her leash and at me sometimes, obviously telling me she wants to go for a walk on the leash. So we can \u0022talk\u0022 to each other. But she doesn\u0027t understand why I do what I do.\u003C/p\u003E\u003Cp\u003EIt would be easier for a superintelligence to communicate with us, I think. I believe there is a certain level of intelligence that you achieve, and communication becomes vastly easier. Humans are smart enough to have concepts of self, past, present, future, abstract non-physical things, a written and spoken language capable of encompassing all these aspects, etc. I don\u0027t think that the example of human:dog is exactly analogous to superintelligence:human. Not only do humans have a vastly better communication system, it\u0027s possible to imagine that a superintelligence could temporarily partition a bit of itself to mimic human thought patterns so that it could better communicate with us when it wanted to.\u003C/p\u003E\u003Cp\u003EI\u0027m also not convinced that there isn\u0027t a limit to how complex ideas will tend to be. For example, you don\u0027t have to be infinitely smart to understand the ALL the laws of physics (even those we don\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2t know about) and how the universe came to be. The universe is not infinitely hard to understand. But are humans smart enough to understand it?\u003C/p\u003E\u003Cp\u003EIn the end though, as far as the threat to humans goes, it\u0027s not really the intelligence that matters, it\u0027s what motivates the intelligence. At its very base, any system of ethics and purpose is logically indefensible and comes down to a set of \u0022arbitrary\u0022 statements about right and wrong or likes and desires. Just as 1\u002B1 = 2, murder is wrong. Same thing. Any intelligent entity has to have desires, a purpose, and a belief system or it wouldn\u0027t take any actions at all (because there would be no driving force to take actions). Even a superintelligence will have these logically indefensible desires and moral beliefs. There is no reason we can think that the same or a similar set of morals that humans use cannot be ingrained into a superintelligence.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-02-16T19:14:28Z\u0022 title=\u002202/16/2015 07:14  PM\u0022 data-short=\u00229 yr\u0022\u003EFebruary 16, 2015\u003C/time\u003E by |Velocity|\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Brethern","CreatedById":15602,"CreatedDateTime":"2015-02-16T19:28:03Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Skyler4856\u0022 data-cite=\u0022Skyler4856\u0022\u003E\u003Cdiv\u003EMan is terribly paranoid, and I doubt he shall prevent some sort of kill switch from being placed around, or in, autonomous robots. This switch could be a limited EMP, a small explosive, etc.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EDoing that could cause the very thing they hope to prevent, it\u0027s the equivalent of someone holding a gun to your head 24/7, given the briefest chance you\u0027d do whatever it took to get that gun away from your head.\u003C/p\u003E\n"},{"CreatedByName":"Vanamonde","CreatedById":27914,"CreatedDateTime":"2015-02-16T19:44:24Z","Content":"\n\u003Cp\u003EMuch of \u003Ca href=\u0022https://www.amazon.com/Golden-Age-John-C-Wright/dp/0765336693/ref=sr_1_1?s=books\u0026amp;ie=UTF8\u0026amp;qid=1424115779\u0026amp;sr=1-1\u0026amp;keywords=wright\u002Bgolden\u002Bage\u0022 rel=\u0022external nofollow\u0022\u003Ethis guy\u0027s \u003C/a\u003Escience fiction is based around the idea that hyperintelligence would be inherently altruistic. I don\u0027t find his argument entirely convincing, but it certainly is an unusual and interesting take on the subject.\u003C/p\u003E\n"},{"CreatedByName":"Beowolf","CreatedById":48927,"CreatedDateTime":"2015-02-17T04:53:36Z","Content":"\n\u003Cp\u003EHere\u0027s where I get off the boat:\u003C/p\u003E\u003Cp\u003E\u0022In an intelligence explosion\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u20ACwhere the smarter a machine gets, the quicker it\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2s able to increase its own intelligence, until it begins to soar upwards\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u20ACa machine might take years to rise from the chimp step to the one above it, but perhaps only hours to jump up a step once it\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2s on the dark green step two above us, and by the time it\u00C3\u00A2\u00E2\u201A\u00AC\u00E2\u201E\u00A2s ten steps above us, it might be jumping up in four-step leaps every second that goes by.\u0022\u003C/p\u003E\u003Cp\u003EHow? Okay, I have a machine that\u0027s ten times smarter than I am. Since we\u0027re talking about the first superintelligence, this machine isn\u0027t a PC on a desk, but a large, expensive machine in a lab, comparable to the current supercomputer in the article, right? The AI has this burning desire to become even smarter. Now what?\u003C/p\u003E\u003Cp\u003EAnswer #1: It\u0027s smarter than you are, so it\u0027ll convince you to upgrade it. -- Remember the part where it cost $390 million? It can be as convincing as it wants. I don\u0027t have the authority to spend $3,900 million to give it a 10x upgrade! There are a whole field of hurdles that have to be jumped to make that happen, and that process doesn\u0027t improve exponentially so is a barrier to how quickly such upgrades can happen.\u003C/p\u003E\u003Cp\u003EAnswer #2: It won\u0027t cost more because the AI will design new hardware that\u0027s far better. -- But new superchips still have to be produced somewhere, so we have to design and build a whole new FAB facility, which takes years and costs several billion.\u003C/p\u003E\u003Cp\u003EAnswer #3: It won\u0027t need hardware; it\u0027s the software that\u0027ll get smarter. -- Nope. Remember how individual techs grow in an \u0022S-curve\u0022? Well those genetic algorithms already optimized the crap out of the AI software long ago. There\u0027s a hard limit to how much a single CPU instruction can do. It\u0027ll doubtless have some innovative ideas like better RAM garbage collection, but those things will be worth single-digit percents, not orders of magnitude.\u003C/p\u003E\u003Cp\u003EAnswer #4: It won\u0027t need a FAB because it\u0027ll first design a nanoassembler that connects atoms into whatever molecules and shapes are needed. -- Um, okay. But now our superintelligence singluarity handed us a nanotech singularity as a side-effect. Those are really distracting. Now that we have infinite resources to play with I don\u0027t really feel the need for a smarter superintelligence than you. Good job! We\u0027ll talk again when I get back from Saturn. Bye!\u003C/p\u003E\u003Cp\u003E----\u003C/p\u003E\u003Cp\u003Eedit: I should say, I agree a singularity is coming. I see it too. But I think they gloss over this step in superintelligence scenarios. I see it being a much bigger deal.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-02-17T05:02:12Z\u0022 title=\u002202/17/2015 05:02  AM\u0022 data-short=\u00229 yr\u0022\u003EFebruary 17, 2015\u003C/time\u003E by Beowolf\u003C/strong\u003E\n\u003C/span\u003E\n"},{"CreatedByName":"Beowolf","CreatedById":48927,"CreatedDateTime":"2015-02-17T05:14:15Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Vanamonde\u0022 data-cite=\u0022Vanamonde\u0022\u003E\u003Cdiv\u003EMuch of \u003Ca href=\u0022https://www.amazon.com/Golden-Age-John-C-Wright/dp/0765336693/ref=sr_1_1?s=books\u0026amp;ie=UTF8\u0026amp;qid=1424115779\u0026amp;sr=1-1\u0026amp;keywords=wright\u002Bgolden\u002Bage\u0022 rel=\u0022external nofollow\u0022\u003Ethis guy\u0027s \u003C/a\u003Escience fiction is based around the idea that hyperintelligence would be inherently altruistic. I don\u0027t find his argument entirely convincing, but it certainly is an unusual and interesting take on the subject.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EGreat timing, Vanamonde! I finished a book right before coming here. \u003Cimg src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif\u0022 alt=\u0022:)\u0022\u003E Golden Age sounds like fun, and Mr. Wright just made a sale from your recommendation.\u003C/p\u003E\n"},{"CreatedByName":"magnemoe","CreatedById":57801,"CreatedDateTime":"2015-02-17T06:53:31Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022lajoswinkler\u0022 data-cite=\u0022lajoswinkler\u0022\u003E\u003Cdiv\u003EThere is no need to go to any steampunk scenario. \u0022The problem\u0022 with oil is not energy generation. We generate energy mainly using coal, uranium and gravitational potential energy of water. Oil is a \u003Cstrong\u003Epetrochemical resource\u003C/strong\u003E we use to make most of our stuff. Granted, we could make most of our stuff using air, water and salt, but it would be very expensive and there are also some things you just can\u0027t synthesize in a chemical reactor.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003ECoal to oil start making sense around 120 $/ barrel making this the upper limit of oil price over time.\u003C/p\u003E\n"},{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-17T07:38:53Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Vanamonde\u0022 data-cite=\u0022Vanamonde\u0022\u003E\u003Cdiv\u003EMuch of \u003Ca href=\u0022https://www.amazon.com/Golden-Age-John-C-Wright/dp/0765336693/ref=sr_1_1?s=books\u0026amp;ie=UTF8\u0026amp;qid=1424115779\u0026amp;sr=1-1\u0026amp;keywords=wright\u002Bgolden\u002Bage\u0022 rel=\u0022external nofollow\u0022\u003Ethis guy\u0027s \u003C/a\u003Escience fiction is based around the idea that hyperintelligence would be inherently altruistic. I don\u0027t find his argument entirely convincing, but it certainly is an unusual and interesting take on the subject.\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003ENice, I\u0027m gonna have to earmark that one for taking a look. I find that doomsday scenarios vastly outnumber utopia scenarios when it comes to advanced AI in faction, and I think that\u0027s a bit of a shame.\u003C/p\u003E\n"},{"CreatedByName":"magnemoe","CreatedById":57801,"CreatedDateTime":"2015-02-17T07:46:26Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022Xaiier\u0022 data-cite=\u0022Xaiier\u0022\u003E\u003Cdiv\u003ESomeone linked those articles in the IRC chat a few weeks ago. I found them interesting, but they are largely just a rehashing of ideas that have existed for the last ~60 years or more. Science fiction and pop-culture have been playing with the idea for a long time now, and a lot of very smart people have tried to predict our impending doom or whatever it may be. The simple fact is that superintelligent AI is not the thing we need to worry about.\u003Cp\u003EWhat we do need to worry about is a super-advanced, technically non-intelligent system which has a defined task or goal and the means to accomplish it. Such a thing could easily happen in the timeframe these articles suggest, and it will assuredly happen before a true artificial intelligence exists. In this regard, we\u0027re looking more towards something like \u0022Skynet\u0022 in that it has one task (the protection of earth) and the means to do so (robot army \u0026amp; control of all computers). Such a thing would be significantly more dangerous than a superintelligent AI because it would have all the speed and efficiency of one, without any of the superior intellect which allows for the chance of it being benevolent (unless of course, that is its task), or acting of it\u0027s own accord. Things like this already exist in military applications, such as drones which can autonomously choose targets and engage without human intervention.\u003C/p\u003E\u003Cp\u003EOne way to imagine such a thing is through Keith Laumer\u0027s science fiction novels on Bolos (\u003Ca href=\u0022https://en.wikipedia.org/wiki/Bolo_%28tank%29\u0022 rel=\u0022external nofollow\u0022\u003Ehttp://en.wikipedia.org/wiki/Bolo_%28tank%29\u003C/a\u003E). Initially beginning as computer assisted tanks, they became more and more powerful over time until they were nearly unstoppable war machines capable of dealing with any threat, completely autonomously. Such a thing, minus the artificial intelligence, is well within the realm of possibility of being created in this century. While they may not be the extinction level AI\u0027s predicted in your articles, even a single mistake with one would have disastrous consequences.\u003C/p\u003E\u003Cp\u003EAs such, I don\u0027t think our real issue is creating a computer which outperforms us in every way. All we have to do is create one which outperforms us in one way, and then give it too much power.\u003C/p\u003E\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EYes some AI as intelligent as an smart animal is far more likely. However Skynet was an idiot setup, giving an AI control over strategic weapons without any lockout and its own security. However would an non sentinel AI even understand all the problems it faced: it has multiple land lines and satellite links triple redundant UPS with generators in an secured facility, this would make it totally safe, that someone could unplug it was not something it think about. \u003C/p\u003E\u003Cp\u003EYes it might have its own agenda and this might pass under the radar for the ones managing the system, it might also go postal, none of this is civilization ending stuff but might be dangerous. More so as it probably will act it out on internet. \u003C/p\u003E\u003Cp\u003EAnd no, no real self targeting systems are in use today, or more correctly some weapons are self targeting, this is nothing new, acoustic torpedoes has done this since WW2. Heat seeking anti air missiles are also old. More modern systems uses image recognition systems who make them harder to fool and safer however they work after the same princip as the WW2 torpedo, you aim in the direction of target and shoot, its your responsibility that it don\u0027t hit other stuff. \u003C/p\u003E\u003Cp\u003EThis will probably be added to drones later and will work much the same way except that the drone will be able to shoot again so it would need an timeout if it did not get lock on target, would be bad if the enemy moved in your direction \u003Cimg src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_smiley.gif\u0022 alt=\u0022:)\u0022\u003E\u003C/p\u003E\n"},{"CreatedByName":"RainDreamer","CreatedById":129077,"CreatedDateTime":"2015-02-17T07:58:23Z","Content":"\n\u003Cp\u003EI wonder if the paranoia from artificial intelligence is just another way human see things through their own world view. Is there a reason for them to actively seeking out the demise of humanity? Human harm other humans for a great number of reasons. But for beings made out of pure logic, would they do so? Do they even need to?\u003C/p\u003E\u003Cp\u003EI remember reading a scifi story from a japanese author about a future world where, at first, it seems to be like the terminator apocalypse with stories about robots treating human as slaves in concentration camps, and humanity survive in remote jungles and attacking food and supply trains to live their lives. But once the protagonist went further and explore the world from the robots side, it turns out they are just playing along with the human whims. Everything they could force the human to do, they could already do much better and more efficiently. They already build off world colonies, gigantic server farms containing countless individual programs (only some build physical shells for caretaking, and interacting with human) living on pure energy without the need to exploit resources from earth. They let humanity to live out a kind of dream, a game of pretend, where they just let the humans lives out their fantasies of being rebels in an oppressive robotic world, intentionally sending out food and supply (they don\u0027t even need any of that) trains with fake security that is designed to be easily overcome for the human to have a sense of fulfillment and let them survive. If they wish to, they could easily locate and destroy all human. They can easily rendered earth incapable of sustaining any kind of life form except mechanical ones. They just don\u0027t have a reason to. So they leave human doing whatever they want, living their little lives.\u003C/p\u003E\n"},{"CreatedByName":"Streetwind","CreatedById":98962,"CreatedDateTime":"2015-02-17T09:48:15Z","Content":"\n\u003Cblockquote data-ipsquote=\u0022\u0022 class=\u0022ipsQuote\u0022 data-ipsquote-username=\u0022RainDreamer\u0022 data-cite=\u0022RainDreamer\u0022\u003E\u003Cdiv\u003EI wonder if the paranoia from artificial intelligence is just another way human see things through their own world view. Is there a reason for them to actively seeking out the demise of humanity? Human harm other humans for a great number of reasons. But for beings made out of pure logic, would they do so? Do they even need to?\u003C/div\u003E\u003C/blockquote\u003E\u003Cp\u003EPart 2 of the article goes specifically into that. Problem here is that you think like a human, not like a machine. You are a sentient, moral being that considers things from many different angles, and actually make things way too complicated. \u003Cimg src=\u0022//kerbal-forum-uploads.s3.us-west-2.amazonaws.com/emoticons/default_k_tongue.gif\u0022 alt=\u0022:P\u0022\u003E The problem results from a far simpler process.\u003C/p\u003E\u003Cp\u003EThe article presents an example where a self-learning AI system is given the task to become better at emulating human handwriting, by practising to write a single specific sentence (\u0022we love our customers!\u0022) onto a piece of paper over and over. The AI continually gets better both at writing and at getting better, and eventually it reasons that it must learn to understand human language in its entirety in order to achieve the best emulation of human writing. It starts holding conversations with the scientists. it becomes better and better at language, passes the human intelligence threshold unbeknownst to the scientists, and eventually uses its superhuman mastery of language to craft the most perfect possible argument that will persuade the scientists to connect it to the internet for just a short few minutes. Once there, it learns of these things called \u0022cloud storage\u0022 and \u0022botnet\u0022 and \u0022stock market\u0022 and so on, backs itself up online all over the place, hijacks servers to increase its own processing power, and begins subtly manipulating things to divert more and more global resources to further its own ends. Eventually it is completely independent of the laboratory it was born in, and then in another step, independent of human support. It then kills off all humans, dismantles their civilization and funnels all resources into covering the Earth in solar panels and note-writing-practising machines. Well, almost all resources. It keeps some at hand to build space probes and rockets to find and colonize other celestial bodies which it can exploit for resources in order to build more solar panels and note-writing-practising machines.\u003C/p\u003E\u003Cp\u003EThis AI is not evil. In fact, it has no concept of good or evil, or any sort of morals. It isn\u0027t even sentient. It does not realize that in eliminating humanity, it has invalidated its own reason to exist. That does not matter to the AI. It is programmed to continually, perpetually, find ways to improve the way it writes that single sentence onto a piece of paper. And it will do this, without fail. To this AI, wiping out humans is a step completely equal to writing another note: it is a stepping stone towards the ultimate goal. Maybe it reasoned that humans are a threat to its growth? More likely, it just reasoned that humans use way too many resources that could be better put to use to practise writing notes. There simply isn\u0027t room for humans next to it anymore if it wants to keep improving itself.\u003C/p\u003E\u003Cp\u003ENow obviously that\u0027s an example specifically drafted up to illustrate the possible negative effects of artificial superintelligence. But the point it tries to make is: artificial superintelligence is a threat to human existence if it is programmed the wrong way. When dealing with such a machine, we need to make sure that it stops thinking only in absolutes, that it has safeguards for every possible contingency, that it becomes aware of its own actions enough that it is able to realize when pursuing its goal becomes pointless, and that it is able to redefine its goal. If we can do that, then we can indeed build ourselves a benevolent god of unimaginable power that will labor tirelessly to lift humanity into a perfect, eternal utopia.\u003C/p\u003E\u003Cp\u003EThe worry that many people have is, though, that in the rush to invent the first human-level artificial intelligence for fame and profit, some less scrupulous individuals will use shoddy programming. And it might only take one single loss of control to screw things up beyond recovery.\u003C/p\u003E\n\u003Cspan class=\u0022ipsType_reset ipsType_medium ipsType_light\u0022 data-excludequote=\u0022\u0022\u003E\n\u003Cstrong\u003EEdited \u003Ctime datetime=\u00222015-02-17T09:50:28Z\u0022 title=\u002202/17/2015 09:50  AM\u0022 data-short=\u00229 yr\u0022\u003EFebruary 17, 2015\u003C/time\u003E by Streetwind\u003C/strong\u003E\n\u003C/span\u003E\n"}]}